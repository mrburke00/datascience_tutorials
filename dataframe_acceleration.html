<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Accelerating Dataframe workflows</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="dataframe_acceleration_files/libs/clipboard/clipboard.min.js"></script>
<script src="dataframe_acceleration_files/libs/quarto-html/quarto.js"></script>
<script src="dataframe_acceleration_files/libs/quarto-html/popper.min.js"></script>
<script src="dataframe_acceleration_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="dataframe_acceleration_files/libs/quarto-html/anchor.min.js"></script>
<link href="dataframe_acceleration_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="dataframe_acceleration_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="dataframe_acceleration_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="dataframe_acceleration_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="dataframe_acceleration_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Accelerating Dataframe workflows</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="devin-burke" class="level3">
<h3 class="anchored" data-anchor-id="devin-burke">Devin Burke</h3>
<div id="S01BsA8Y0g9H" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>env LC_ALL<span class="op">=</span>C.UTF<span class="op">-</span><span class="dv">8</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>env LANG<span class="op">=</span>C.UTF<span class="op">-</span><span class="dv">8</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="table-of-contents" class="level1">
<h1>Table of Contents</h1>
<ul>
<li><a href="#primer-sparsity-density-and-csr">1. Primer: Sparsity, Density, and CSR</a>
<ul>
<li><a href="#primer-sparsity-density-and-csr">Sparsity vs Density</a></li>
<li><a href="#dense-vs.-sparse-in-memory">Dense vs Sparse Memory Intuition</a></li>
<li><a href="#what-csr-actually-looks-like">What CSR Actually Looks Like</a></li>
<li><a href="#why-csr-is-fast-for-row-operations">Why CSR is Fast for Rows but Slower for Columns</a></li>
<li><a href="#common-pitfalls">Common Pitfalls &amp; Safer Alternatives</a></li>
</ul></li>
<li><a href="#tools-to-find-bottlenecks---measuring-before-optimizing">2. Tools to Find Bottlenecks</a>
<ul>
<li><a href="#tools-to-find-bottlenecks---measuring-before-optimizing">Inspecting Matrix Memory Footprint</a></li>
<li><a href="#tools-to-find-bottlenecks---measuring-before-optimizing">Timing QC Metrics</a></li>
<li><a href="#tools-to-find-bottlenecks---measuring-before-optimizing">Row-wise Scaling Example (Mito Fractions)</a></li>
<li><a href="#tools-to-find-bottlenecks---measuring-before-optimizing">The Dangers of Densification</a></li>
<li><a href="#tools-to-find-bottlenecks---measuring-before-optimizing">Takeaways from Bottleneck Profiling</a></li>
</ul></li>
<li><a href="#vectorization">3. Vectorization</a>
<ul>
<li><a href="#vectorization">Primer: What is Vectorization</a></li>
<li><a href="#vectorization">Loop vs Vectorized Reductions</a></li>
<li><a href="#vectorization">Elementwise Ops on Sparse <code>.data</code></a></li>
<li><a href="#vectorization">Broadcasting Safely for Normalization</a></li>
<li><a href="#vectorization">Fuse Passes to Reuse Intermediates</a></li>
<li><a href="#vectorization">Scaling Curve: Vectorized vs Loop</a></li>
</ul></li>
<li><a href="#sparsity-savvy-ops">4. Sparsity-Savvy Ops</a>
<ul>
<li><a href="#sparsity-savvy-ops">Counting Genes per Cell: getnnz vs Boolean</a></li>
<li><a href="#sparsit--savvy-ops">Column Sums: CSR vs CSC</a></li>
<li><a href="#sparsity-savvy-ops">Batching Column Ops</a></li>
<li><a href="#sparsity-savvy-ops">Anti-Pattern: Densification via NumPy</a></li>
</ul></li>
<li><a href="#cpu-micro-optimizations">5. CPU Micro-Optimizations</a>
<ul>
<li><a href="#cpu-micro-optimizations">Dtype Discipline</a></li>
<li><a href="#cpu-micro-optimizations">Row Normalization: Diag vs In-Place</a></li>
<li><a href="#cpu-micro-optimizations">Numba for Custom Transforms</a></li>
<li><a href="#cpu-micro-optimizations">Block/Tile Processing</a></li>
</ul></li>
<li><a href="#when-and-how-to-use-the-gpu">6. When (and How) to Use the GPU</a>
<ul>
<li><a href="#when-and-how-to-use-the-gpu">CuPy Basics</a></li>
<li><a href="#when-and-how-to-use-the-gpu">CuPy Sparse CSR</a></li>
<li><a href="#when-and-how-to-use-the-gpu">RAPIDS cuDF / cuML</a></li>
<li><a href="#when-and-how-to-use-the-gpu">RAPIDS Single-Cell End-to-End Pipeline</a></li>
<li><a href="#when-and-how-to-use-the-gpu">CPU vs GPU Timing Comparison</a></li>
</ul></li>
<li><a href="#hybrid-pipelines">7. Hybrid Pipelines</a>
<ul>
<li><a href="#hybrid-pipelines">CPU for Preprocessing</a></li>
<li><a href="#hybrid-pipelines">GPU for Linear Algebra &amp; Graphs</a></li>
<li><a href="#hybrid-pipelines">Split-Pipeline Pattern</a></li>
</ul></li>
<li><a href="#emerging-tools">8. Emerging Runtimes &amp; Tools</a>
<ul>
<li><a href="#emerging-tools">GPU-Native Frameworks</a></li>
<li><a href="#emerging-tools">Scalable Array &amp; Storage Backends</a></li>
<li><a href="#emerging-tools">CPU Accelerators</a></li>
<li><a href="#emerging-tools">Cluster &amp; Distributed Execution</a></li>
<li><a href="#emerging-tools">Where Things Are Heading</a></li>
</ul></li>
</ul>
<div id="UAXNK3Dd0jpy" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Uninstall only the conflicting RAPIDS bits (keep it minimal)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip uninstall <span class="op">-</span>y cugraph cugraph<span class="op">-</span>cu12 cuml cuml<span class="op">-</span>cu12 rmm rmm<span class="op">-</span>cu12 pylibraft<span class="op">-</span>cu12 pylibcugraph<span class="op">-</span>cu12</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Reinstall a coherent RAPIDS 24.10 set that your Colab already uses</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">\</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">"rmm-cu12==24.10.*"</span> \</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">"pylibraft-cu12==24.10.*"</span> \</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">"pylibcugraph-cu12==24.10.*"</span> \</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="st">"cuml-cu12==24.10.*"</span> \</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="st">"cugraph-cu12==24.10.*"</span> \</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="op">--</span>extra<span class="op">-</span>index<span class="op">-</span>url<span class="op">=</span>https:<span class="op">//</span>pypi.nvidia.com <span class="op">-</span>U <span class="op">--</span>quiet</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Arrow must be &lt;20 for these RAPIDS wheels</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="st">"pyarrow&lt;20"</span> <span class="op">--</span>upgrade <span class="op">--</span>force<span class="op">-</span>reinstall <span class="op">--</span>quiet</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Print what we actually have now (versions)</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys, subprocess</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Python:"</span>, sys.version)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pkg <span class="kw">in</span> [</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"cudf-cu12"</span>,<span class="st">"cuml-cu12"</span>,<span class="st">"cugraph-cu12"</span>,</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"rmm-cu12"</span>,<span class="st">"pylibraft-cu12"</span>,<span class="st">"pylibcugraph-cu12"</span>,</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"pyarrow"</span>,<span class="st">"cupy-cuda12x"</span>,<span class="st">"cuda-python"</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>]:</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> subprocess.check_output([sys.executable,<span class="st">"-m"</span>,<span class="st">"pip"</span>,<span class="st">"show"</span>,pkg], text<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        ver <span class="op">=</span> <span class="bu">next</span>((l.split(<span class="st">":"</span>,<span class="dv">1</span>)[<span class="dv">1</span>].strip() <span class="cf">for</span> l <span class="kw">in</span> out.splitlines() <span class="cf">if</span> l.startswith(<span class="st">"Version:"</span>)), <span class="st">"unknown"</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>pkg<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>ver<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> subprocess.CalledProcessError:</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>pkg<span class="sc">}</span><span class="ss">: not installed"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="8uTHinLX0os2" class="cell" data-outputid="8de5e1c0-89d8-4597-8556-bc7f555b99ee">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> rmm, cudf, cuml, cugraph</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> pyarrow <span class="im">as</span> pa</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Imports OK:"</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>          <span class="bu">getattr</span>(cudf, <span class="st">"__version__"</span>, <span class="st">"?"</span>),</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>          <span class="bu">getattr</span>(cuml, <span class="st">"__version__"</span>, <span class="st">"?"</span>),</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>          <span class="bu">getattr</span>(cugraph, <span class="st">"__version__"</span>, <span class="st">"?"</span>),</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>          pa.__version__)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> traceback, sys</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Import failed:"</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    traceback.print_exc(limit<span class="op">=</span><span class="dv">2</span>, <span class="bu">file</span><span class="op">=</span>sys.stdout)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Imports OK: 24.10.01 24.10.00 24.10.00 19.0.1</code></pre>
</div>
</div>
<div id="Pc31d8xHnjzv" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="st">"rapids-singlecell==0.13.1"</span> <span class="st">"scanpy&gt;=1.10,&lt;1.12"</span> <span class="st">"anndata&gt;=0.10,&lt;0.12"</span> <span class="st">"igraph"</span> <span class="st">"leidenalg"</span> <span class="st">"pyarrow&lt;20"</span> <span class="op">--</span>upgrade<span class="op">-</span>strategy only<span class="op">-</span><span class="cf">if</span><span class="op">-</span>needed <span class="op">-</span>q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="57QqqsM0tJ4D" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scanpy <span class="im">as</span> sc, anndata, pandas <span class="im">as</span> pd, numpy <span class="im">as</span> np, scipy <span class="im">as</span> sp</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rmm, cudf, cuml, cugraph, rapids_singlecell <span class="im">as</span> rsc</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cupy <span class="im">as</span> cp</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RAPIDS:"</span>, <span class="bu">getattr</span>(cudf, <span class="st">"__version__"</span>, <span class="st">"?"</span>), <span class="bu">getattr</span>(cuml, <span class="st">"__version__"</span>, <span class="st">"?"</span>), <span class="bu">getattr</span>(cugraph, <span class="st">"__version__"</span>, <span class="st">"?"</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"rsc:"</span>, rsc.__version__)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"scanpy:"</span>, sc.__version__)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"anndata:"</span>, anndata.__version__)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"pandas/numpy/scipy:"</span>, pd.__version__, np.__version__, sp.__version__)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"cupy:"</span>, cp.__version__)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scanpy <span class="im">as</span> sc</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time, warnings</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rapids_singlecell <span class="im">as</span> rsc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="umwqXP8zcsy4" class="cell" data-outputid="bc288635-87c2-4a42-ca6a-aa5db4303cf9">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget https:<span class="op">//</span>cf<span class="fl">.10</span><span class="er">xgenomics</span>.com<span class="op">/</span>samples<span class="op">/</span>cell<span class="op">-</span>exp<span class="op">/</span><span class="fl">9.0.0</span><span class="op">/</span><span class="dv">320</span><span class="er">k_Human_PBMCs_Sub_Pool_16</span><span class="op">-</span>plex_GEM<span class="op">-</span>X_FLEX_PBMC_BC1<span class="op">-</span><span class="dv">16</span><span class="op">/</span><span class="dv">320</span><span class="er">k_Human_PBMCs_Sub_Pool_16</span><span class="op">-</span>plex_GEM<span class="op">-</span>X_FLEX_PBMC_BC1<span class="op">-</span><span class="dv">16</span><span class="er">_count_sample_filtered_feature_bc_matrix</span>.h5</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--2025-08-20 19:56:49--  https://cf.10xgenomics.com/samples/cell-exp/9.0.0/320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16/320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16_count_sample_filtered_feature_bc_matrix.h5
Resolving cf.10xgenomics.com (cf.10xgenomics.com)... 104.18.1.173, 104.18.0.173, 2606:4700::6812:1ad, ...
Connecting to cf.10xgenomics.com (cf.10xgenomics.com)|104.18.1.173|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1187383220 (1.1G) [application/x-hdf5]
Saving to: ‘320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16_count_sample_filtered_feature_bc_matrix.h5’

320k_Human_PBMCs_Su 100%[===================&gt;]   1.11G  20.2MB/s    in 49s     

2025-08-20 19:57:39 (23.2 MB/s) - ‘320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16_count_sample_filtered_feature_bc_matrix.h5’ saved [1187383220/1187383220]
</code></pre>
</div>
</div>
</section>
<section id="primer-sparsity-density-and-csr" class="level1">
<h1>1. Primer: Sparsity, Density, and CSR</h1>
<section id="sparsity-vs.-density" class="level2">
<h2 class="anchored" data-anchor-id="sparsity-vs.-density">Sparsity vs.&nbsp;Density</h2>
<p>Density = fraction of non-zeros</p>
<p>Sparsity = fraction of zeros</p>
<pre><code>density = nnz / (n_rows × n_cols)
sparsity = 1 – density</code></pre>
<p>Where: * Dense algorithms (e.g.&nbsp;BLAS, numpy) treat every entry equally, doing work even on zeros.</p>
<ul>
<li>Sparse algorithms (e.g.&nbsp;CSR, CSC) skip over zeros and only process the non-zeros.</li>
</ul>
<p>When a dataset is very sparse, this saves both memory and compute. But as density rises (20–30% or more), the cost of bookkeeping (extra index arrays) can outweigh the savings, and dense storage or dense kernels may actually win.</p>
</section>
<section id="sparsity-vs.-density-in-memory" class="level2">
<h2 class="anchored" data-anchor-id="sparsity-vs.-density-in-memory">Sparsity vs.&nbsp;Density in memory</h2>
<p>###Dense storage</p>
<p>If you store every entry as a float32 (4 bytes), the cost is:</p>
<p><code>bytes_dense = n_rows × n_cols × 4</code></p>
<p>No matter how many zeros, you pay for them all. CSR storage</p>
<p><strong>Compressed Sparse Row (CSR</strong>) avoids storing zeros. Instead, it uses three arrays:</p>
<ol type="1">
<li><strong>data</strong></li>
</ol>
<ul>
<li><p>Length = <code>nnz</code></p></li>
<li><p>Stores the actual non-zero values (floats, 4 bytes each)</p></li>
</ul>
<ol start="2" type="1">
<li><strong>indices</strong></li>
</ol>
<ul>
<li><p>Length = <code>nnz</code></p></li>
<li><p>Stores the column index for each non-zero (ints, 4 bytes each)</p></li>
</ul>
<ol start="3" type="1">
<li><strong>indptr</strong></li>
</ol>
<ul>
<li><p>Length = <code>n_rows + 1</code></p></li>
<li><p>A “row pointer” array: tells you where each row starts and ends inside <code>data/indices</code>.</p></li>
<li><p>Think of it as the “table of contents” for rows.</p></li>
<li><p>Each entry is also an int (4 bytes).</p></li>
</ul>
<pre><code>bytes_CSR = 4 × nnz (data)
        + 4 × nnz (indices)
        + 4 × (n_rows + 1) (indptr)</code></pre>
<section id="k-pbmc-dataset" class="level3">
<h3 class="anchored" data-anchor-id="k-pbmc-dataset">320k PBMC dataset</h3>
<p>Matrix: 289,331 × 18,129 with nnz ≈ 935,809,081</p>
<p>Dense: <code>289,331 × 18,129 × 4 ≈ 21.0 GB</code> CSR:</p>
<pre><code>(4 × 935,809,081) [data]
+ (4 × 935,809,081) [indices]
+ (4 × 289,332)    [indptr]
≈ 7.49 GB</code></pre>
</section>
<section id="rule-of-thumb" class="level3">
<h3 class="anchored" data-anchor-id="rule-of-thumb">Rule of Thumb</h3>
<p>CSR is worth it when sparsity is high (lots of zeros).</p>
<p>As density increases, the overhead of storing indices + row pointers eats into savings. Around 20–30% density, dense storage can be competitive or even faster.</p>
</section>
<section id="what-csr-actually-looks-like" class="level3">
<h3 class="anchored" data-anchor-id="what-csr-actually-looks-like">What CSR Actually Looks Like</h3>
<pre><code>Row 0:  .   3   .   .   .   1
Row 1:  2   .   .   .   .   .
Row 2:  .   .   .   7   .   .
Row 3:  .   4   5   .   .   .</code></pre>
<p>Coordinates of non-zeros</p>
<pre><code>(0,1,3), (0,5,1), (1,0,2), (2,3,7), (3,1,4), (3,2,5)</code></pre>
<p>CSR Arrays</p>
<pre><code>data    = [3, 1, 2, 7, 4, 5]       (the values, row by row)
indices = [1, 5, 0, 3, 1, 2]       (the column index of each value)
indptr  = [0, 2, 3, 4, 6]          (row boundaries in data/indices)</code></pre>
</section>
<section id="why-csr-is-fast-for-row-operations" class="level3">
<h3 class="anchored" data-anchor-id="why-csr-is-fast-for-row-operations">Why CSR is Fast for Row Operations</h3>
<p>Many common tasks in single-cell analysis are per-row (per-cell):</p>
<ul>
<li><p>total UMI counts per cell</p></li>
<li><p>mitochondrial fractions</p></li>
<li><p>number of detected genes per cell</p></li>
</ul>
<p>In CSR, each row’s values are stored contiguously in data, with start/end marked by indptr. That means row operations are just <code>O(nnz)</code>, with good cache locality.</p>
</section>
<section id="why-csr-is-slower-for-column-operations" class="level3">
<h3 class="anchored" data-anchor-id="why-csr-is-slower-for-column-operations">Why CSR is Slower for Column Operations</h3>
<p>Column entries are scattered across rows, not contiguous. So column sums require “hunting” through many rows, which is slower. If you care about per-column operations, you often convert to CSC format.</p>
</section>
<section id="csc-the-mirror-image" class="level3">
<h3 class="anchored" data-anchor-id="csc-the-mirror-image">CSC: The Mirror Image</h3>
<p>Compressed Sparse Column (CSC) flips the idea:</p>
<ul>
<li><p>data/indices are grouped by column.</p></li>
<li><p>indptr marks column boundaries.</p></li>
</ul>
<p>CSC is ideal for per-column (per-gene) work. That’s why R’s Matrix package uses CSC (dgCMatrix) by default. In CSC: * per-gene ops are “free”</p>
<ul>
<li>per-cell ops require either transposing or special functions</li>
</ul>
</section>
</section>
<section id="common-pitfalls" class="level2">
<h2 class="anchored" data-anchor-id="common-pitfalls">Common Pitfalls</h2>
<p>Sparse matrices save huge amounts of memory, but only if you stay within the sparse world. Many “convenient” operations in numpy/pandas land are secretly dense, and if you aren’t careful, you can blow up memory without realizing it.</p>
<ol type="1">
<li>Boolean masks and densification</li>
</ol>
<p>Pretend we are counting detected genes with something like</p>
<pre><code>genes_per_cell = (X &gt; 0).sum(axis=1).A1</code></pre>
<p>Here’s what happens</p>
<ol type="1">
<li><code>(X &gt; 0)</code> creates a new sparse matrix of the same <code>nnz</code>pattern, but with dtype <code>bool</code>.</li>
</ol>
<pre><code>* This doubles your memory use temporarily (since both `X` and (X &gt; 0)` exist).</code></pre>
<ol start="2" type="1">
<li><p><code>.sum(axis=1)</code> computes row sums, but the result is still a matrix object.</p></li>
<li><p><code>.A1</code> (or <code>.toarray()</code>) forces dense materialization.</p></li>
</ol>
<pre><code>* At this point, you’ve asked Python to allocate `n_rows × n_cols` floats in memory.

* For large single-cell datasets, this can be tens of gigabytes, even if the original sparse matrix fit comfortably in RAM.</code></pre>
<hr>
<ol start="2" type="1">
<li>Implicit Conversions to Dense Many of the default function calls in popular packages (numpy, sklearn, pandas) don’t support sparse arrays. If you pass a sparse matrix to them, they will silently convert to dense behind the senses. Meaning:</li>
</ol>
<ul>
<li>a 21 GB sparse matrix might suddenly try to allocate 100GB+</li>
</ul>
<p>Examples:</p>
<ul>
<li><code>.mean(axis=0)</code> on a SciPy sparse matrix → densifies</li>
<li><code>Using</code>np.log1p(X)` directly on sparse → densifies.</li>
<li>Converting sparse → pandas DataFrame (<code>pd.DataFrame(X)</code>) → densifies</li>
</ul>
<p>Safer alternatives:</p>
<ul>
<li><p>Use sparse-aware functions: scipy.sparse has its own <code>.sum</code>, <code>.getnnz</code>, etc.</p></li>
<li><p>For elementwise transforms, use specialized functions like <code>sparsefuncs.log1p</code> from <code>sklearn.utils.extmath</code> or libraries like <code>scanpy/anndata</code> that wrap these safely.</p></li>
</ul>
<hr>
<ol start="3" type="1">
<li><p>Copy-on-write behavior Sparse matrices don’t always behave like numpy arrays with views. Many slicing operations return new copies of arrays (data, indices, indptr). If you repeatedly slice in a loop, you may be silently creating many copies of large structures. —</p></li>
<li><p>Awareness of dtype</p></li>
</ol>
<p>Your values (data) are often float32 (4 bytes).</p>
<p>But if you accidentally convert to float64, memory doubles instantly.</p>
<p>Similarly, some sparse constructors default to int64 indices instead of int32, doubling index storage.</p>
<p><strong>Packages like SciPy’s functions will often solve all of these issues for you, but it’s still useful to understand when you wander off path</strong></p>
</section>
</section>
<section id="tools-to-find-bottlenecks---measuring-before-optimizing" class="level1">
<h1>2. Tools to find bottlenecks - Measuring before optimizing</h1>
<p>Our goal in this section is not to optimize, rather to measure and understand. * What the matrix looks like in memory * How common tasks scale * What densification does to memory and runtime</p>
<p>Once we can name bottlenecks with evidence, targeted optimizations are much more obvious.</p>
<section id="inspect-the-matrix-in-memory-shape-density-bytes" class="level2">
<h2 class="anchored" data-anchor-id="inspect-the-matrix-in-memory-shape-density-bytes">2.1 Inspect the matrix in memory (shape, density, bytes)</h2>
<p>how big is the matrix, how sparse is it, and what would it cost if someone accidentally made it dense?</p>
<p>density tells you whether sparse is still paying off; the dense vs CSR bytes show your safety margin.</p>
<div id="h2Ov5iO6EIRX" class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>adata <span class="op">=</span> sc.read_10x_h5(<span class="st">"/content/320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16_count_sample_filtered_feature_bc_matrix.h5"</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>adata.var_names_make_unique()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="93gw3OImv3IX" class="cell" data-outputid="7358540c-8789-44f6-fa9b-2fe67eeca37a">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ——— Download (only once) ———</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !wget -O bone_marrow.h5 \</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">#   https://cf.10xgenomics.com/samples/cell-vdj/6.1.0/10k_BMMNC_5pv2_nextgem_10k_BMMNC_5pv2_nextgem/10k_BMMNC_5pv2_nextgem_10k_BMMNC_5pv2_nextgem_count_sample_feature_bc_matrix.h5</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scanpy <span class="im">as</span> sc</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time, statistics <span class="im">as</span> stats, psutil, os, gc</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># If not already loaded:</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co">#adata = sc.read_10x_h5("/content/320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16_count_sample_filtered_feature_bc_matrix.h5")</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co">#adata.var_names_make_unique()</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rss_mb():</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> psutil.Process(os.getpid()).memory_info().rss <span class="op">/</span> <span class="fl">1e6</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Stopwatch:</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, label): <span class="va">self</span>.label <span class="op">=</span> label</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__enter__</span>(<span class="va">self</span>):</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        gc.collect()</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._rss0 <span class="op">=</span> rss_mb()</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__exit__</span>(<span class="va">self</span>, <span class="op">*</span>exc):</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        dt <span class="op">=</span> time.perf_counter() <span class="op">-</span> <span class="va">self</span>._t0</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        drss <span class="op">=</span> rss_mb() <span class="op">-</span> <span class="va">self</span>._rss0</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>label<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>dt<span class="sc">:.3f}</span><span class="ss">s | RSS Δ </span><span class="sc">{</span>drss<span class="sc">:+.1f}</span><span class="ss"> MB (now </span><span class="sc">{</span>rss_mb()<span class="sc">:.1f}</span><span class="ss"> MB)"</span>)</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench(fn, <span class="op">*</span>args, repeats<span class="op">=</span><span class="dv">3</span>, warmup<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs):</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(warmup): fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    times <span class="op">=</span> []</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(repeats):</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        times.append(time.perf_counter() <span class="op">-</span> t0)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(median_s<span class="op">=</span>stats.median(times), mean_s<span class="op">=</span><span class="bu">float</span>(np.mean(times)),</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>                min_s<span class="op">=</span><span class="bu">min</span>(times), max_s<span class="op">=</span><span class="bu">max</span>(times), n<span class="op">=</span>repeats)</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matrix_stats(X, name<span class="op">=</span><span class="st">"X"</span>):</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> sparse.issparse(X):</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>        m, n <span class="op">=</span> X.shape</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>        nnz <span class="op">=</span> X.nnz</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>        density <span class="op">=</span> nnz<span class="op">/</span>(m<span class="op">*</span>n)</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>        dense_bytes <span class="op">=</span> m<span class="op">*</span>n<span class="op">*</span><span class="dv">4</span>  <span class="co"># float32</span></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>        csr_bytes <span class="op">=</span> nnz<span class="op">*</span>(<span class="dv">4</span><span class="op">+</span><span class="dv">4</span>) <span class="op">+</span> (m<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span><span class="dv">4</span>  <span class="co"># data(float32)+indices(int32)+indptr(int32)</span></span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: sparse </span><span class="sc">{</span>X<span class="sc">.</span><span class="bu">format</span><span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>X<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, nnz=</span><span class="sc">{</span>nnz<span class="sc">:,}</span><span class="ss"> (density=</span><span class="sc">{</span>density<span class="sc">:.4%}</span><span class="ss">), dtype=</span><span class="sc">{</span>X<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  ~Dense(float32) if materialized: </span><span class="sc">{</span>dense_bytes<span class="op">/</span><span class="fl">1e9</span><span class="sc">:.2f}</span><span class="ss"> GB"</span>)</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  ~CSR payload (data+idx+indptr): </span><span class="sc">{</span>csr_bytes<span class="op">/</span><span class="fl">1e6</span><span class="sc">:.1f}</span><span class="ss"> MB  (excl. Python overhead)"</span>)</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: dense </span><span class="sc">{</span>X<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, dtype=</span><span class="sc">{</span><span class="bu">getattr</span>(X,<span class="st">'dtype'</span>,<span class="va">None</span>)<span class="sc">}</span><span class="ss">, nbytes=</span><span class="sc">{</span><span class="bu">getattr</span>(X,<span class="st">'nbytes'</span>,<span class="dv">0</span>)<span class="op">/</span><span class="fl">1e9</span><span class="sc">:.2f}</span><span class="ss"> GB"</span>)</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse <span class="im">as</span> sp</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ensure_csr_float32(X):</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> sp.isspmatrix_csr(X):</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X.astype(np.float32, copy<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X.tocsr().astype(np.float32, copy<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"AnnData: n_obs=</span><span class="sc">{</span>adata<span class="sc">.</span>n_obs<span class="sc">:,}</span><span class="ss">, n_var=</span><span class="sc">{</span>adata<span class="sc">.</span>n_vars<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>adata.X <span class="op">=</span> ensure_csr_float32(adata.X)</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>matrix_stats(adata.X, <span class="st">"adata.X (CSR/float32)"</span>)</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>AnnData: n_obs=289,331, n_var=18,129
adata.X (CSR/float32): sparse csr (289331, 18129), nnz=935,809,081 (density=17.8410%), dtype=float32
  ~Dense(float32) if materialized: 20.98 GB
  ~CSR payload (data+idx+indptr): 7487.6 MB  (excl. Python overhead)</code></pre>
</div>
</div>
</section>
<section id="what-the-matrix-looks-like" class="level2">
<h2 class="anchored" data-anchor-id="what-the-matrix-looks-like">What the matrix looks like</h2>
<p>Our dataset has 289,331 cells × 18,129 genes, with ~935.8M non-zero entries.</p>
<ul>
<li><p>Density ≈ 17.84% → still sparse enough that CSR is beneficial.</p></li>
<li><p>Dense float32 footprint ≈ 21.0 GB.</p></li>
<li><p>CSR payload ≈ 7.5 GB (data + indices + indptr, excluding Python overhead).</p></li>
</ul>
<p>At this density, CSR saves ~2.8× memory, though we’re close to the “edge zone” where index overhead becomes more noticeable.</p>
</section>
<section id="time-scanpy-qc-to-spot-fixed-overheads-vs-row-wise-work" class="level2">
<h2 class="anchored" data-anchor-id="time-scanpy-qc-to-spot-fixed-overheads-vs-row-wise-work">2.2 Time Scanpy QC to spot fixed overheads vs row-wise work</h2>
<p>Now we measure a realistic multi-pass routine. Expect some fixed overhead and several row-wise passes; the full run vs a subset gives a sense of scaling and where time is spent.</p>
<div id="xzYWTSs4A3cx" class="cell" data-outputid="c3138548-8dd3-4e76-e084-24fd9e5f06b8">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> qc_metrics(adata_):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    sc.pp.calculate_qc_metrics(adata_, percent_top<span class="op">=</span><span class="va">None</span>, log1p<span class="op">=</span><span class="va">False</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> Stopwatch(<span class="st">"QC metrics (full copy)"</span>):</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    qc_metrics(adata.copy())</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>n_demo <span class="op">=</span> <span class="bu">min</span>(<span class="dv">80_000</span>, adata.n_obs)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> Stopwatch(<span class="ss">f"QC metrics (subset </span><span class="sc">{</span>n_demo<span class="sc">:,}</span><span class="ss">)"</span>):</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    qc_metrics(adata[:n_demo,:].copy())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>QC metrics (full copy): 9.783s | RSS Δ +0.1 MB (now 10625.0 MB)
QC metrics (subset 80,000): 5.394s | RSS Δ +0.3 MB (now 10625.3 MB)</code></pre>
</div>
</div>
</section>
<section id="qc-metrics-multi-pass-overhead" class="level2">
<h2 class="anchored" data-anchor-id="qc-metrics-multi-pass-overhead">QC Metrics: multi-pass overhead</h2>
<p>Running Scanpy’s calculate_qc_metrics:</p>
<ul>
<li><p>Full copy (289k cells): ~9.78s</p></li>
<li><p>Subset (80k cells): ~5.39s</p></li>
</ul>
<p>This is not perfectly linear with n, because the function does several passes and bookkeeping tasks. Still, the cost is dominated by row-wise work, which matches what CSR is good at.</p>
</section>
<section id="measure-a-pure-row-wise-kernel-across-sizes" class="level2">
<h2 class="anchored" data-anchor-id="measure-a-pure-row-wise-kernel-across-sizes">2.3 Measure a pure row-wise kernel across sizes</h2>
<p>We benchmark mitochondrial fraction, a straightforward row reduction. If CSR fits the workload, you should see smooth, near-linear scaling with n.</p>
<div id="R3PeyYjPA5v2" class="cell" data-outputid="ac939de1-5ed4-4d33-90d2-576e07f14b50">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>mito_mask <span class="op">=</span> np.fromiter((g.startswith(<span class="st">"MT-"</span>) <span class="cf">for</span> g <span class="kw">in</span> adata.var_names), dtype<span class="op">=</span><span class="bu">bool</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mito_fraction_vec(X, mito_mask):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> sp.issparse(X):</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        num <span class="op">=</span> X[:, mito_mask].<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        den <span class="op">=</span> X.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        num <span class="op">=</span> X[:, mito_mask].<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        den <span class="op">=</span> X.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> np.divide(num, den, out<span class="op">=</span>np.zeros_like(num, dtype<span class="op">=</span>np.float32), where<span class="op">=</span>den<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> [<span class="dv">20_000</span>, <span class="dv">40_000</span>, <span class="dv">80_000</span>, <span class="dv">160_000</span>, <span class="bu">min</span>(<span class="dv">320_000</span>, adata.n_obs)]</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>cpu_curve <span class="op">=</span> []</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> sizes:</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    subX <span class="op">=</span> adata.X[:n, :]</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    stats_n <span class="op">=</span> bench(<span class="kw">lambda</span>: mito_fraction_vec(subX, mito_mask), repeats<span class="op">=</span><span class="dv">3</span>, warmup<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>    cpu_curve.append((n, stats_n[<span class="st">"median_s"</span>]))</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"mito_fraction_vec n=</span><span class="sc">{</span>n<span class="sc">:&gt;6}</span><span class="ss">: </span><span class="sc">{</span>stats_n<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>mito_fraction_vec n= 20000: {'median_s': 0.15986045399949944, 'mean_s': 0.16111667266644267, 'min_s': 0.15954597699965234, 'max_s': 0.1639435870001762, 'n': 3}
mito_fraction_vec n= 40000: {'median_s': 0.3227266129997588, 'mean_s': 0.3246616669991151, 'min_s': 0.3221335949983768, 'max_s': 0.3291247929992096, 'n': 3}
mito_fraction_vec n= 80000: {'median_s': 0.6394602919990575, 'mean_s': 0.6403345810000852, 'min_s': 0.6314820660008991, 'max_s': 0.6500613850002992, 'n': 3}
mito_fraction_vec n=160000: {'median_s': 1.297852285000772, 'mean_s': 1.2947686590002074, 'min_s': 1.2806189630009612, 'max_s': 1.3058347289988887, 'n': 3}
mito_fraction_vec n=289331: {'median_s': 2.322633675999896, 'mean_s': 2.32081454766679, 'min_s': 2.313078230999963, 'max_s': 2.326731736000511, 'n': 3}</code></pre>
</div>
</div>
</section>
<section id="scaling-row-wise-mito-fraction" class="level2">
<h2 class="anchored" data-anchor-id="scaling-row-wise-mito-fraction">Scaling: row-wise mito fraction</h2>
<p>We measured per-cell mitochondrial fraction (a row-wise reduction) across increasing numbers of cells:</p>
<ul>
<li><p>20k → 0.16s</p></li>
<li><p>40k → 0.32s</p></li>
<li><p>80k → 0.64s</p></li>
<li><p>160k → 1.30s</p></li>
<li><p>289k → 2.32s</p></li>
</ul>
<p>This is almost perfectly linear scaling. Throughput stays stable at ~120–130k cells/sec.&nbsp;That’s exactly what we’d expect: CSR stores each row contiguously, so row reductions are <code>O(nnz)</code> with good cache locality.</p>
<p>Takeaway: CSR delivers efficient row-wise ops — the backbone of per-cell QC and summary metrics.</p>
</section>
<section id="dangers-of-densification" class="level2">
<h2 class="anchored" data-anchor-id="dangers-of-densification">2.4 Dangers of densification</h2>
<p>Accidental densification is the most common way pipelines blow up. Here we deliberately convert a modest slice to dense and hold it to capture the real memory impact.</p>
<div id="9Pn6VwYfA7JU" class="cell" data-outputid="c453fb9b-31f7-4d51-b150-ad8e3536ea66">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse <span class="im">as</span> sp</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> force_dense_and_hold(X):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> sp.issparse(X):</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        D <span class="op">=</span> X.toarray()  <span class="co"># allocate dense (n_rows x n_cols)</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        D <span class="op">=</span> np.asarray(X, order<span class="op">=</span><span class="st">"C"</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Do a tiny op to ensure it's touched</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> D.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> D, out</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>n_safe <span class="op">=</span> <span class="bu">min</span>(<span class="dv">25_000</span>, adata.n_obs)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> adata.X[:n_safe, :]</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Before:"</span>, rss_mb(), <span class="st">"MB"</span>)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> Stopwatch(<span class="st">"force_dense_and_hold"</span>):</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    D_held, _out <span class="op">=</span> force_dense_and_hold(subX)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After holding dense:"</span>, rss_mb(), <span class="st">"MB"</span>)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Keep D_held alive for a moment (optional: inspect top)</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a><span class="co"># time.sleep(2)</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Cleanup</span></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> D_held, _out<span class="op">;</span> gc.collect()</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After cleanup:"</span>, rss_mb(), <span class="st">"MB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Before: 11275.620352 MB
force_dense_and_hold: 2.270s | RSS Δ +1812.9 MB (now 12439.1 MB)
After holding dense: 12439.097344 MB
After cleanup: 10626.224128 MB</code></pre>
</div>
</div>
</section>
<section id="densification-the-real-hazard" class="level2">
<h2 class="anchored" data-anchor-id="densification-the-real-hazard">Densification: the real hazard</h2>
<p>We tested the “what if I accidentally densify” scenario with a 25k × 18,129 slice:</p>
<p>CSR payload ≈ 650 MB</p>
<p>Dense float32 equivalent ≈ 1.81 GB</p>
<p>Observed RAM jump: +1.81 GB while the dense copy was held</p>
<p>This validates the caution: densifying isn’t a rounding error — even a modest slice can balloon memory by gigabytes. In larger pipelines, those dense temporaries often stick around (via chained expressions or DataFrame conversions), which can silently push you into OOM territory.</p>
</section>
<section id="takeaways" class="level2">
<h2 class="anchored" data-anchor-id="takeaways">Takeaways</h2>
<ul>
<li><p>The matrix is sparse enough for CSR to pay off; row-wise ops scale linearly and fast.</p></li>
<li><p>QC has multi-pass overhead but remains row-dominated, aligning with CSR strengths.</p></li>
<li><p>Densification is a gigabyte-scale event even for modest slices; avoid it unless it’s a small, deliberate block.</p></li>
</ul>
</section>
</section>
<section id="vectorization" class="level1">
<h1>3. Vectorization</h1>
<section id="primer---what-is-vectorization-and-why-it-matters" class="level2">
<h2 class="anchored" data-anchor-id="primer---what-is-vectorization-and-why-it-matters">3.1 Primer - What is vectorization and why it matters</h2>
<p>Non-vectorized (Python loop): You write an explicit for loop in Python and operate on one element at a time. Every iteration goes through the Python interpreter → slow overhead, poor cache usage.</p>
<p>Vectorized (array operation): You express the work as a whole-array operation (sum, multiply, log, etc.). The heavy lifting happens inside optimized C/CUDA kernels, touching millions of elements in a tight, compiled loop.</p>
<p>Example:</p>
<p>Imagine we want to multiply two arrays <code>a</code> and <code>b</code>.</p>
<p><strong>Scalar</strong></p>
<pre><code>a = [1, 2, 3, 4]
b = [5, 6, 7, 8]</code></pre>
<pre><code>take a[0], b[0] → 1*5 = 5 → store
take a[1], b[1] → 2*6 = 12 → store
take a[2], b[2] → 3*7 = 21 → store
take a[3], b[3] → 4*8 = 32 → store</code></pre>
<pre><code> [1]   [2]   [3]   [4]
   ×     ×     ×     ×      (each step separate)
 [5]   [6]   [7]   [8]

 → [5] [12] [21] [32]</code></pre>
<p>Each multiplication is its own job — the “vector” is not visible as a whole, just a sequence of scalars.</p>
<p><strong>Vectorized (treat arrays as one object)</strong></p>
<p>Now, same math (dot product), but expressed as a whole operation:</p>
<pre><code>c = a · b</code></pre>
<pre><code>     [1   2   3   4]
   ⨉ [5   6   7   8]
   -------------------
     [5  12  21  32]
</code></pre>
</section>
<section id="why-its-faster" class="level2">
<h2 class="anchored" data-anchor-id="why-its-faster">Why it’s faster</h2>
<ul>
<li><p>Interpreter overhead: Python loops pay a cost per iteration; vectorized ops pay one cost per whole array.</p></li>
<li><p>Cache and memory locality: compiled kernels access memory contiguously and predictably, while Python loops jump around object references.</p></li>
<li><p>Parallelism under the hood: libraries can use SIMD (vector CPU instructions), multithreading, or GPU CUDA kernels automatically.</p></li>
<li><p>Fewer passes: combining operations reduces memory bandwidth usage.</p></li>
</ul>
<section id="sparse-csr-per-cell-totals-loop-vs-vectorized" class="level3">
<h3 class="anchored" data-anchor-id="sparse-csr-per-cell-totals-loop-vs-vectorized">3.2 sparse CSR: per-cell totals, loop vs vectorized</h3>
<p>We compare a Python row loop (<code>getrow().sum()</code>) to the vectorized sparse reduction</p>
<div id="L4LuAVVYEq__" class="cell" data-outputid="c027bd6a-edbf-4a3c-bd03-370114f7a285">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse <span class="im">as</span> sp</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, time, statistics <span class="im">as</span> stats</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> adata.X  <span class="co"># CSR float32 from Part 2</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loop_counts_per_cell(X):</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> np.empty(m, dtype<span class="op">=</span>np.float32)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        out[i] <span class="op">=</span> X.getrow(i).<span class="bu">sum</span>()</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vect_counts_per_cell(X):</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench(fn, <span class="op">*</span>args, repeats<span class="op">=</span><span class="dv">3</span>, warmup<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs):</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(warmup): fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>    ts <span class="op">=</span> []</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(repeats):</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>        fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>        ts.append(time.perf_counter() <span class="op">-</span> t0)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(median_s<span class="op">=</span>stats.median(ts), mean_s<span class="op">=</span><span class="bu">float</span>(np.mean(ts)),</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>                min_s<span class="op">=</span><span class="bu">min</span>(ts), max_s<span class="op">=</span><span class="bu">max</span>(ts), n<span class="op">=</span>repeats)</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>n_demo <span class="op">=</span> <span class="bu">min</span>(<span class="dv">40_000</span>, X.shape[<span class="dv">0</span>])</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"loop_counts_per_cell:"</span>, bench(loop_counts_per_cell, X[:n_demo,:]))</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"vect_counts_per_cell:"</span>, bench(vect_counts_per_cell, X[:n_demo,:]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>loop_counts_per_cell: {'median_s': 3.9722902630001045, 'mean_s': 3.970004134333673, 'min_s': 3.962057611999626, 'max_s': 3.975664528001289, 'n': 3}
vect_counts_per_cell: {'median_s': 0.05190052400030254, 'mean_s': 0.05413572200025859, 'min_s': 0.04996352800117165, 'max_s': 0.06054311399930157, 'n': 3}</code></pre>
</div>
</div>
</section>
<section id="loop-vs-vectorizd-on-csr" class="level3">
<h3 class="anchored" data-anchor-id="loop-vs-vectorizd-on-csr">Loop vs Vectorizd on CSR</h3>
<p>Per-cell totals (40k rows)</p>
<ul>
<li><p>Loop: 3.972 s</p></li>
<li><p>Vectorized: 0.0519 s</p></li>
<li><p>≈ 76× faster</p></li>
</ul>
<p>Why: the vectorized reduction treats the whole matrix as one object and walks contiguous row segments; the loop does 40k tiny jobs and pays per-iteration overhead.</p>
</section>
</section>
<section id="elementwise-transform-on-sparse-touch-.data-vectorized" class="level2">
<h2 class="anchored" data-anchor-id="elementwise-transform-on-sparse-touch-.data-vectorized">3.3 Elementwise transform on sparse: touch .data (vectorized)</h2>
<p>We scale-and-shift the non-zeros in one shot by operating on the CSR data buffer. Then we compare to a scalar loop over that buffer.</p>
<div id="dtzfcf11dNJR" class="cell" data-outputid="c3b8c929-aed1-407a-a07a-38a89b9ee269">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse <span class="im">as</span> sp</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time, statistics <span class="im">as</span> stats</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> scale_shift_sparse(X, a<span class="op">=</span><span class="fl">1.2</span>, b<span class="op">=</span><span class="fl">0.7</span>):</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> X.copy()</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    Y.data <span class="op">=</span> Y.data <span class="op">*</span> a <span class="op">+</span> b</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> scale_shift_loop(X, a<span class="op">=</span><span class="fl">1.2</span>, b<span class="op">=</span><span class="fl">0.7</span>):</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> X.copy()</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> Y.data</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(d.size):</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        d[i] <span class="op">=</span> d[i] <span class="op">*</span> a <span class="op">+</span> b</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench(fn, <span class="op">*</span>args, repeats<span class="op">=</span><span class="dv">3</span>, warmup<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs):</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(warmup): fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    ts <span class="op">=</span> []</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(repeats):</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>        fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>        ts.append(time.perf_counter() <span class="op">-</span> t0)</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(median_s<span class="op">=</span>stats.median(ts), mean_s<span class="op">=</span><span class="bu">float</span>(np.mean(ts)),</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>                min_s<span class="op">=</span><span class="bu">min</span>(ts), max_s<span class="op">=</span><span class="bu">max</span>(ts), n<span class="op">=</span>repeats)</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> X[:<span class="dv">1000</span>,:]</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"scale_shift_loop:"</span>, bench(scale_shift_loop, subX))</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"scale_shift_sparse:"</span>, bench(scale_shift_sparse, subX))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>scale_shift_loop: {'median_s': 5.757489456000258, 'mean_s': 5.747247039000285, 'min_s': 5.724614021000889, 'max_s': 5.75963763999971, 'n': 3}
scale_shift_sparse: {'median_s': 0.0071631329992669635, 'mean_s': 0.007237320999289902, 'min_s': 0.007122330998754478, 'max_s': 0.007426498999848263, 'n': 3}</code></pre>
</div>
</div>
</section>
<section id="elementwise-on-sparse-operate-on-.data" class="level2">
<h2 class="anchored" data-anchor-id="elementwise-on-sparse-operate-on-.data">Elementwise on sparse: operate on .data</h2>
<p>Scale-and-shift nonzeros (50k rows slice)</p>
<ul>
<li><p>Loop over nonzeros: 5.757 s</p></li>
<li><p>Vectorized on X.data: 0.00716 s</p></li>
<li><p>≈ 800× faster</p></li>
</ul>
<p>Lesson: when you can express the operation as “apply function to all nonzeros,” touch CSR.data directly. You skip zeros entirely and do exactly one pass.</p>
<section id="broadcast-safely-per-cell-normalization-without-densifying" class="level3">
<h3 class="anchored" data-anchor-id="broadcast-safely-per-cell-normalization-without-densifying">3.4 Broadcast safely: per-cell normalization without densifying</h3>
<p>Row-normalize using a diagonal multiply (sparse-safe). This is the canonical way to avoid broadcasting traps that create dense temporaries.</p>
<p>Sidenote: Broadcasting is NumPy/Scipy’s way of automatically expanding arrays of different shapes so you can combine them without looping.</p>
<p>With dense matrices, Numpy broadcasting is cheap but with Scipy sparse you can often experience unintended densification because sparse matrices don’t fully support Numpy’s broadcasting rules. So below we are showing one way to ensure Scipy doesn’t silently densify.</p>
<div id="hWJnQKrAdVbp" class="cell" data-outputid="8a6de01f-4f7e-460f-95fb-a3a10ebd0728">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse <span class="im">as</span> sp</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time, statistics <span class="im">as</span> stats</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> row_normalize_csr(X):</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> X.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> np.divide(<span class="fl">1.0</span>, counts, out<span class="op">=</span>np.zeros_like(counts, dtype<span class="op">=</span>np.float32), where<span class="op">=</span>counts<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> sp.diags(scale, offsets<span class="op">=</span><span class="dv">0</span>, dtype<span class="op">=</span>np.float32, <span class="bu">format</span><span class="op">=</span><span class="st">"csr"</span>)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> D <span class="op">@</span> X</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench(fn, <span class="op">*</span>args, repeats<span class="op">=</span><span class="dv">3</span>, warmup<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs):</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(warmup): fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    ts <span class="op">=</span> []</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(repeats):</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>        fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>        ts.append(time.perf_counter() <span class="op">-</span> t0)</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(median_s<span class="op">=</span>stats.median(ts), mean_s<span class="op">=</span><span class="bu">float</span>(np.mean(ts)),</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>                min_s<span class="op">=</span><span class="bu">min</span>(ts), max_s<span class="op">=</span><span class="bu">max</span>(ts), n<span class="op">=</span>repeats)</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"row_normalize_csr:"</span>, bench(row_normalize_csr, X[:<span class="dv">80_000</span>,:]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>row_normalize_csr: {'median_s': 2.2612861880006676, 'mean_s': 2.2661139726666684, 'min_s': 2.260578056000668, 'max_s': 2.27647767399867, 'n': 3}</code></pre>
</div>
</div>
</section>
<section id="fuse-passes-reuse-intermediates-mito-fraction-example" class="level3">
<h3 class="anchored" data-anchor-id="fuse-passes-reuse-intermediates-mito-fraction-example">3.5 Fuse passes: reuse intermediates (mito fraction example)</h3>
<p>We compute mitochondrial fraction two ways: naïve (two sums) vs fused (reuse precomputed row totals).</p>
<div id="wCFzEDlBdgnj" class="cell" data-outputid="4b3693de-982b-4dc7-fe71-8f448702b8be">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse <span class="im">as</span> sp</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time, statistics <span class="im">as</span> stats</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>mito_mask <span class="op">=</span> np.fromiter((g.startswith(<span class="st">"MT-"</span>) <span class="cf">for</span> g <span class="kw">in</span> adata.var_names), dtype<span class="op">=</span><span class="bu">bool</span>)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mito_fraction_naive(X, mito_mask):</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    num <span class="op">=</span> X[:, mito_mask].<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>    den <span class="op">=</span> X.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.divide(num, den, out<span class="op">=</span>np.zeros_like(num, dtype<span class="op">=</span>np.float32), where<span class="op">=</span>den<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mito_fraction_fused(X, mito_mask, row_counts<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> row_counts <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>        row_counts <span class="op">=</span> X.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    num <span class="op">=</span> X[:, mito_mask].<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.divide(num, row_counts, out<span class="op">=</span>np.zeros_like(num, dtype<span class="op">=</span>np.float32), where<span class="op">=</span>row_counts<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench(fn, <span class="op">*</span>args, repeats<span class="op">=</span><span class="dv">3</span>, warmup<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs):</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(warmup): fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    ts <span class="op">=</span> []</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(repeats):</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>        fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>        ts.append(time.perf_counter() <span class="op">-</span> t0)</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(median_s<span class="op">=</span>stats.median(ts), mean_s<span class="op">=</span><span class="bu">float</span>(np.mean(ts)),</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>                min_s<span class="op">=</span><span class="bu">min</span>(ts), max_s<span class="op">=</span><span class="bu">max</span>(ts), n<span class="op">=</span>repeats)</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> X[:<span class="dv">160_000</span>,:]</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>row_counts <span class="op">=</span> subX.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"naive:"</span>, bench(mito_fraction_naive, subX, mito_mask))</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"fused (cached counts):"</span>, bench(mito_fraction_fused, subX, mito_mask, row_counts<span class="op">=</span>row_counts))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>naive: {'median_s': 1.2879675139993196, 'mean_s': 1.2942474799995882, 'min_s': 1.276605152999764, 'max_s': 1.3181697729996813, 'n': 3}
fused (cached counts): {'median_s': 1.0758264319993032, 'mean_s': 1.0748436906663603, 'min_s': 1.0687177370000427, 'max_s': 1.0799869029997353, 'n': 3}</code></pre>
</div>
</div>
</section>
</section>
<section id="broadcast-safely-row-normalization-without-traps" class="level2">
<h2 class="anchored" data-anchor-id="broadcast-safely-row-normalization-without-traps">Broadcast safely: row normalization without traps</h2>
<p>Row-normalize (80k rows slice)</p>
<ul>
<li>Diagonal-multiply approach: 2.261 s</li>
</ul>
<p>This is already vectorized and sparse-safe (no densification), but the cost is dominated by building the diagonal + a sparse matmul. Keep this result in mind—we’ll beat it in Part 5 with a row-scaling kernel that multiplies each row’s data segment in place. Same math, fewer moving parts.</p>
<p>Fuse passes when you can</p>
<p>Mito fraction (160k rows)</p>
<ul>
<li><p>Naïve (two sums): 1.288 s</p></li>
<li><p>Fused (reuse row counts): 1.076 s</p></li>
<li><p>~20% faster</p></li>
</ul>
<p>Even vectorized code can burn bandwidth if it revisits the matrix. Cache shared intermediates (row totals, masks) to avoid extra passes.</p>
<section id="vectorized-scaling-curve-with-a-small-loop-baseline" class="level3">
<h3 class="anchored" data-anchor-id="vectorized-scaling-curve-with-a-small-loop-baseline">3.6 Vectorized scaling curve (with a small loop baseline)</h3>
<p>We replicate the Part-2 style scaling for the vectorized mito calculation and overlay a tiny loop baseline so the structural benefit is visible.</p>
<div id="KhI-OLKHdvkX" class="cell" data-outputid="b61a5ee3-2ff0-46fe-dd8e-7ab68a705b34">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, time, statistics <span class="im">as</span> stats</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse <span class="im">as</span> sp</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench_vect_mito(X, mito_mask, n_list):</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> []</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> n_list:</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        sub <span class="op">=</span> X[:n,:]</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> bench(<span class="kw">lambda</span>: (sub[:, mito_mask].<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1, sub.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1), repeats<span class="op">=</span><span class="dv">3</span>)[<span class="st">'median_s'</span>]</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        out.append((n, t))</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"n=</span><span class="sc">{</span>n<span class="sc">:&gt;6}</span><span class="ss"> vectorized mito: </span><span class="sc">{</span>t<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench_loop_counts(X, n_list):</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> []</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> n_list:</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>        sub <span class="op">=</span> X[:n,:]</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> bench(<span class="kw">lambda</span>: [sub.getrow(i).<span class="bu">sum</span>() <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)], repeats<span class="op">=</span><span class="dv">1</span>, warmup<span class="op">=</span><span class="dv">0</span>)[<span class="st">'median_s'</span>]</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>        out.append((n, t))</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"n=</span><span class="sc">{</span>n<span class="sc">:&gt;6}</span><span class="ss"> loop row sums:   </span><span class="sc">{</span>t<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>n_list_small <span class="op">=</span> [<span class="dv">5_000</span>, <span class="dv">10_000</span>, <span class="dv">20_000</span>, <span class="dv">40_000</span>]</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>rows_vect <span class="op">=</span> bench_vect_mito(X, mito_mask, n_list_small)</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>rows_loop <span class="op">=</span> bench_loop_counts(X, n_list_small)</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>ns_v <span class="op">=</span> np.array([r[<span class="dv">0</span>] <span class="cf">for</span> r <span class="kw">in</span> rows_vect])<span class="op">;</span> tv <span class="op">=</span> np.array([r[<span class="dv">1</span>] <span class="cf">for</span> r <span class="kw">in</span> rows_vect])</span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>ns_l <span class="op">=</span> np.array([r[<span class="dv">0</span>] <span class="cf">for</span> r <span class="kw">in</span> rows_loop])<span class="op">;</span> tl <span class="op">=</span> np.array([r[<span class="dv">1</span>] <span class="cf">for</span> r <span class="kw">in</span> rows_loop])</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>plt.plot(ns_v, tv, marker<span class="op">=</span><span class="st">"o"</span>, label<span class="op">=</span><span class="st">"vectorized mito (CSR)"</span>)</span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>plt.plot(ns_l, tl, marker<span class="op">=</span><span class="st">"o"</span>, label<span class="op">=</span><span class="st">"loop row sums (baseline)"</span>)</span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"cells (n)"</span>)</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"seconds"</span>)</span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Vectorized vs Loop scaling"</span>)</span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>n=  5000 vectorized mito: 0.040s
n= 10000 vectorized mito: 0.082s
n= 20000 vectorized mito: 0.164s
n= 40000 vectorized mito: 0.320s
n=  5000 loop row sums:   0.504s
n= 10000 loop row sums:   0.991s
n= 20000 loop row sums:   1.985s
n= 40000 loop row sums:   3.944s</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="dataframe_acceleration_files/figure-html/cell-17-output-2.png" width="315" height="318" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="sparsity-savvy-ops" class="level2">
<h2 class="anchored" data-anchor-id="sparsity-savvy-ops">4. Sparsity-Savvy Ops</h2>
<p>Not all sparse operations are equal</p>
<section id="row-counts-boolean-temp-vs-getnnz" class="level3">
<h3 class="anchored" data-anchor-id="row-counts-boolean-temp-vs-getnnz">4.1 Row counts: boolean temp vs getnnz</h3>
<p>Counting nonzeros per row is a classic pitfall. Boolean masks look vectorized but create new sparse structures; getnnz is cheaper and avoids densification. Let’s measure both.</p>
<div id="8Tv07231d1qr" class="cell" data-outputid="87ba5a97-eb21-462b-b5a2-70d0ebeb897e">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse <span class="im">as</span> sp</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, statistics <span class="im">as</span> stats, time</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench(fn, <span class="op">*</span>args, repeats<span class="op">=</span><span class="dv">3</span>, warmup<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs):</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(warmup): fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    ts <span class="op">=</span> []</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(repeats):</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>        fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>        ts.append(time.perf_counter() <span class="op">-</span> t0)</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(median_s<span class="op">=</span>stats.median(ts), mean_s<span class="op">=</span><span class="bu">float</span>(np.mean(ts)),</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>                min_s<span class="op">=</span><span class="bu">min</span>(ts), max_s<span class="op">=</span><span class="bu">max</span>(ts), n<span class="op">=</span>repeats)</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> genes_per_cell_getnnz(X):</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X.getnnz(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> genes_per_cell_boolean(X):</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.asarray((X <span class="op">&gt;</span> <span class="dv">0</span>).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)).ravel()</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> adata.X[:<span class="dv">40_000</span>,:]</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"getnnz:"</span>, bench(genes_per_cell_getnnz, subX))</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"boolean:"</span>, bench(genes_per_cell_boolean, subX))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>getnnz: {'median_s': 1.3445009244605899e-05, 'mean_s': 1.557865956177314e-05, 'min_s': 1.2559990864247084e-05, 'max_s': 2.073097857646644e-05, 'n': 3}
boolean: {'median_s': 0.7257259640027769, 'mean_s': 0.7260426096715188, 'min_s': 0.7230394780053757, 'max_s': 0.7293623870064039, 'n': 3}</code></pre>
</div>
</div>
<p>Counting genes per cell: <code>getnnz</code> vs boolean temp</p>
<ul>
<li><p><code>getnnz</code>: 0.000013 s</p></li>
<li><p>Boolean mask: 0.726 s</p></li>
</ul>
<p>That’s a ~50,000× speedup. Why?</p>
<ul>
<li><p><code>getnnz</code> directly counts nonzeros row by row using the CSR index structure.</p></li>
<li><p><code>(X &gt; 0)</code> builds a whole new sparse boolean matrix and then sums it.</p></li>
</ul>
<p>Takeaway: for detection counts, always prefer <code>getnnz</code>. It avoids memory waste and runs in microseconds.</p>
<hr>
</section>
<section id="column-sums-csr-vs-csc" class="level3">
<h3 class="anchored" data-anchor-id="column-sums-csr-vs-csc">4.2 Column sums: CSR vs CSC</h3>
<p>CSR is row-friendly, CSC is column-friendly. We’ll compare column sums done directly in CSR vs after a one-time CSC conversion.</p>
<div id="tffMVYRgxT-G" class="cell" data-outputid="f25a7cb5-bd3b-4418-a764-3736c233682d">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> col_sums_csr(X):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.asarray(X.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)).ravel()</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> col_sums_csc(X):</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    Xc <span class="op">=</span> X.tocsc()</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.asarray(Xc.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)).ravel()</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> adata.X[:<span class="dv">40_000</span>,:]</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"col_sums_csr:"</span>, bench(col_sums_csr, subX))</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"col_sums_csc:"</span>, bench(col_sums_csc, subX))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>col_sums_csr: {'median_s': 0.14801824200549163, 'mean_s': 0.14785454766630815, 'min_s': 0.14687072299420834, 'max_s': 0.1486746779992245, 'n': 3}
col_sums_csc: {'median_s': 1.8990398200112395, 'mean_s': 1.8993665896705352, 'min_s': 1.8879537030006759, 'max_s': 1.9111062459996901, 'n': 3}</code></pre>
</div>
</div>
<section id="column-sums-csr-vs-csc-1" class="level4">
<h4 class="anchored" data-anchor-id="column-sums-csr-vs-csc-1">Column sums: CSR vs CSC</h4>
<ul>
<li><p>CSR: 0.148 s</p></li>
<li><p>CSC: 1.899 s</p></li>
</ul>
<p>This shows the classic tradeoff:</p>
<p>CSR is row-optimized. Doing column work directly in CSR is cheaper than converting for one op.</p>
<p>CSC pays the one-time conversion, but shines if you batch multiple column ops.</p>
<hr>
</section>
</section>
<section id="batch-multiple-column-ops-after-one-conversion" class="level3">
<h3 class="anchored" data-anchor-id="batch-multiple-column-ops-after-one-conversion">4.3 Batch multiple column ops after one conversion</h3>
<p>The best pattern is: convert once, do many column operations, then (optionally) convert back. This avoids paying the conversion cost repeatedly.</p>
<div id="dH-bO_wxxWxq" class="cell" data-outputid="e1f86a9c-ca71-4bf4-bbce-cab1240611f4">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> col_ops_batched(X):</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    Xc <span class="op">=</span> X.tocsc()</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>    s1 <span class="op">=</span> np.asarray(Xc.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)).ravel()</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    s2 <span class="op">=</span> Xc.getnnz(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    s3 <span class="op">=</span> np.asarray(Xc.power(<span class="dv">2</span>).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)).ravel()</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> s1, s2, s3</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> adata.X[:<span class="dv">40_000</span>,:]</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"batched column ops:"</span>, bench(col_ops_batched, subX))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>batched column ops: {'median_s': 2.405151458020555, 'mean_s': 2.4064764560122662, 'min_s': 2.404300422000233, 'max_s': 2.409977488016011, 'n': 3}</code></pre>
</div>
</div>
<section id="batched-column-ops-csc" class="level4">
<h4 class="anchored" data-anchor-id="batched-column-ops-csc">Batched column ops (CSC)</h4>
<ul>
<li><p>Doing three column ops after one conversion: 2.405 s</p></li>
<li><p>That’s only slightly slower than the conversion itself, because the ops become “free” once in CSC.</p></li>
</ul>
<p>Pattern:</p>
<ul>
<li><p>If you only need one column op → stay in CSR.</p></li>
<li><p>If you need many → convert once to CSC, batch them, and optionally convert back.</p></li>
</ul>
<hr>
</section>
</section>
<section id="anti-pattern-densification-through-numpy-apis" class="level3">
<h3 class="anchored" data-anchor-id="anti-pattern-densification-through-numpy-apis">4.4 Anti-pattern: densification through NumPy APIs</h3>
<p>Some NumPy functions don’t understand sparse matrices. Let’s show how np.mean on a SciPy sparse matrix forces a dense conversion.</p>
<div id="_FwF1OpXxYCB" class="cell" data-outputid="a447239b-073b-4343-fad7-fe3062d7a05d">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dense_mean(X):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(X)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> adata.X[:<span class="dv">5000</span>,:]  <span class="co"># keep safe size</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"np.mean on sparse (forces dense!):"</span>)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> Stopwatch(<span class="st">"np.mean(sparse)"</span>):</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> dense_mean(subX)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>np.mean on sparse (forces dense!):
np.mean(sparse): 0.200s | RSS Δ +0.0 MB (now 12744.8 MB)</code></pre>
</div>
</div>
<section id="anti-pattern-numpy-apis-on-sparse" class="level4">
<h4 class="anchored" data-anchor-id="anti-pattern-numpy-apis-on-sparse">Anti-pattern: NumPy APIs on sparse</h4>
<p><code>np.mean(X)</code> on a sparse slice ran in 0.200 s and (here) didn’t spike RSS much, but only because you used a small slice. Under the hood, this forces a dense array. On a real slice with 25k × 18k cells, you’d see multi-GB blowups.</p>
<p>Rule: if you see plain NumPy functions on a sparse matrix, assume it’s dangerous unless explicitly sparse-aware. Or at least test it using the above tools</p>
<hr>
</section>
</section>
</section>
<section id="cpu-micro-optimizations" class="level2">
<h2 class="anchored" data-anchor-id="cpu-micro-optimizations">5. CPU Micro-Optimizations</h2>
<p>Vectorization gets us 10×–100× wins. Micro-optimizations give the last 2×–3× by reducing overhead, improving cache locality, and avoiding waste. This section covers:</p>
<ul>
<li><p>Dtype discipline: <code>float32</code> / <code>int32</code> only.</p></li>
<li><p>Row-scaling in place: avoid <code>D @ X</code>.</p></li>
<li><p>Block processing: keep data in cache.</p></li>
<li><p>Numba kernels: for custom transforms on <code>.data</code>.</p></li>
</ul>
<section id="dtype-inflation-doubles-memory" class="level3">
<h3 class="anchored" data-anchor-id="dtype-inflation-doubles-memory">5.1 Dtype inflation doubles memory</h3>
<p>Show how <code>float64</code> silently doubles payload vs <code>float32</code>.</p>
<div id="6tzXI7RvxZw2" class="cell" data-outputid="8cde61bd-2334-42f8-c71b-86b755e37108">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mem_bytes_csr(X):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X.data.nbytes <span class="op">+</span> X.indices.nbytes <span class="op">+</span> X.indptr.nbytes</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>X32 <span class="op">=</span> adata.X.astype(np.float32, copy<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>X64 <span class="op">=</span> adata.X.astype(np.float64, copy<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"float32 bytes:"</span>, mem_bytes_csr(X32))</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"float64 bytes:"</span>, mem_bytes_csr(X64))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>float32 bytes: 7487629976
float64 bytes: 11230866300</code></pre>
</div>
</div>
<section id="dtype-discipline" class="level4">
<h4 class="anchored" data-anchor-id="dtype-discipline">Dtype discipline</h4>
<ul>
<li><p>CSR <code>float32</code>: ~7.49 GB</p></li>
<li><p>CSR <code>float64</code>: ~11.23 GB</p></li>
</ul>
<p>That’s 50% more memory for zero extra information. Same for <code>int64</code> indices. Always store values as <code>float32</code> and indices as <code>int32</code>.</p>
<hr>
</section>
</section>
<section id="row-normalization-diag-multiply-vs-in-place-row-scale" class="level3">
<h3 class="anchored" data-anchor-id="row-normalization-diag-multiply-vs-in-place-row-scale">5.2 Row-normalization: diag multiply vs in-place row scale</h3>
<p>Compare the “safe but heavy” diagonal approach from Part 3 to a lean in-place row scaling kernel that touches each row’s data segment once.</p>
<div id="gUFZzFKQxbMF" class="cell" data-outputid="4ed0c639-9006-45a0-88ed-96fc0ac5c5c9">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> row_normalize_diag(X):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> X.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> np.divide(<span class="fl">1.0</span>, counts, out<span class="op">=</span>np.zeros_like(counts, dtype<span class="op">=</span>np.float32), where<span class="op">=</span>counts<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> sp.diags(scale, offsets<span class="op">=</span><span class="dv">0</span>, dtype<span class="op">=</span>np.float32, <span class="bu">format</span><span class="op">=</span><span class="st">"csr"</span>)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> D <span class="op">@</span> X</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> row_normalize_inplace(X):</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X.copy()</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> X.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> np.divide(<span class="fl">1.0</span>, counts, out<span class="op">=</span>np.zeros_like(counts, dtype<span class="op">=</span>np.float32), where<span class="op">=</span>counts<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">0</span>]):</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>        start, end <span class="op">=</span> X.indptr[i], X.indptr[i<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scale[i] <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>            X.data[start:end] <span class="op">*=</span> scale[i]</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> adata.X[:<span class="dv">80_000</span>,:]</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"row_normalize_diag:"</span>, bench(row_normalize_diag, subX))</span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"row_normalize_inplace:"</span>, bench(row_normalize_inplace, subX))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>row_normalize_diag: {'median_s': 2.2259483500092756, 'mean_s': 2.225888760003727, 'min_s': 2.2219033279980067, 'max_s': 2.2298146020038985, 'n': 3}
row_normalize_inplace: {'median_s': 1.2840500500169583, 'mean_s': 1.286525678326143, 'min_s': 1.28124573698733, 'max_s': 1.2942812479741406, 'n': 3}</code></pre>
</div>
</div>
</section>
<section id="advanced-5.3-numba-for-custom-elementwise-transforms" class="level3">
<h3 class="anchored" data-anchor-id="advanced-5.3-numba-for-custom-elementwise-transforms">Advanced: 5.3 Numba for custom elementwise transforms</h3>
<p>Numba is a just in time (JIT) compiler for python focused on numerical arrays. You decorate a Python function with <code>@numba.njit</code>, and Numba compiles it to fast machine code the first time you call it.</p>
<p>Caveats:</p>
<ul>
<li><p>Compilation has overhead on the first call; use warmup runs before timing.</p></li>
<li><p>Works best on numeric arrays with simple loops; struggles with Python objects.</p></li>
<li><p>Debugging JIT’d code is trickier (can’t step through like normal Python).</p></li>
</ul>
<div id="MuyRRk--xc-q" class="cell" data-outputid="802ae34c-6b0e-4e67-82c3-e7a5fcc26f2e">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse <span class="im">as</span> sp</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time, statistics <span class="im">as</span> stats</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numba <span class="im">as</span> nb</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench(fn, <span class="op">*</span>args, repeats<span class="op">=</span><span class="dv">3</span>, warmup<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs):</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(warmup): fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>    ts <span class="op">=</span> []</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(repeats):</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>        fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>        ts.append(time.perf_counter() <span class="op">-</span> t0)</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(median_s<span class="op">=</span>stats.median(ts), mean_s<span class="op">=</span><span class="bu">float</span>(np.mean(ts)),</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>                min_s<span class="op">=</span><span class="bu">min</span>(ts), max_s<span class="op">=</span><span class="bu">max</span>(ts), n<span class="op">=</span>repeats)</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Piecewise parameters</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> <span class="fl">5.0</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1) Python loop baseline</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> piecewise_python(X, t1<span class="op">=</span>t1, t2<span class="op">=</span>t2, alpha<span class="op">=</span>alpha):</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> X.copy()</span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> Y.data</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(d.size):</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> d[i]</span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x <span class="op">&lt;</span> t1:</span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>            d[i] <span class="op">=</span> alpha <span class="op">*</span> x</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> x <span class="op">&lt;</span> t2:</span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>            d[i] <span class="op">=</span> np.log1p(x)</span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>            d[i] <span class="op">=</span> np.sqrt(x)</span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y</span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 2) NumPy masking (vectorized with multiple passes + masks)</span></span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> piecewise_numpy_masks(X, t1<span class="op">=</span>t1, t2<span class="op">=</span>t2, alpha<span class="op">=</span>alpha):</span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> X.copy()</span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> Y.data</span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a>    m1 <span class="op">=</span> d <span class="op">&lt;</span> t1</span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a>    m2 <span class="op">=</span> (<span class="op">~</span>m1) <span class="op">&amp;</span> (d <span class="op">&lt;</span> t2)</span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a>    m3 <span class="op">=</span> <span class="op">~</span>(m1 <span class="op">|</span> m2)</span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a>    d[m1] <span class="op">=</span> alpha <span class="op">*</span> d[m1]</span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a>    d[m2] <span class="op">=</span> np.log1p(d[m2])</span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a>    d[m3] <span class="op">=</span> np.sqrt(d[m3])</span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y</span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: np.select variant (also allocates full-size temporaries)</span></span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> piecewise_numpy_select(X, t1<span class="op">=</span>t1, t2<span class="op">=</span>t2, alpha<span class="op">=</span>alpha):</span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> X.copy()</span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> Y.data</span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a>    Y.data <span class="op">=</span> np.select(</span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a>        [d <span class="op">&lt;</span> t1, d <span class="op">&lt;</span> t2],</span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a>        [alpha <span class="op">*</span> d, np.log1p(d)],</span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a>        default<span class="op">=</span>np.sqrt(d)</span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a>    ).astype(d.dtype, copy<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y</span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3) Numba single-pass, branchy kernel (no masks)</span></span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a><span class="at">@nb.njit</span>(cache<span class="op">=</span><span class="va">True</span>, fastmath<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _piecewise_kernel(d, t1, t2, alpha):</span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(d.size):</span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> d[i]</span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x <span class="op">&lt;</span> t1:</span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a>            d[i] <span class="op">=</span> alpha <span class="op">*</span> x</span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> x <span class="op">&lt;</span> t2:</span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a>            d[i] <span class="op">=</span> np.log1p(x)</span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a>            d[i] <span class="op">=</span> np.sqrt(x)</span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> piecewise_numba(X, t1<span class="op">=</span>t1, t2<span class="op">=</span>t2, alpha<span class="op">=</span>alpha):</span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> X.copy()</span>
<span id="cb54-72"><a href="#cb54-72" aria-hidden="true" tabindex="-1"></a>    _piecewise_kernel(Y.data, t1, t2, alpha)</span>
<span id="cb54-73"><a href="#cb54-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y</span>
<span id="cb54-74"><a href="#cb54-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-75"><a href="#cb54-75" aria-hidden="true" tabindex="-1"></a><span class="co"># ---- Benchmark on a realistic slice</span></span>
<span id="cb54-76"><a href="#cb54-76" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> adata.X[:<span class="dv">100</span>, :]  <span class="co"># increase/decrease as your machine allows</span></span>
<span id="cb54-77"><a href="#cb54-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-78"><a href="#cb54-78" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Warm up Numba…"</span>)</span>
<span id="cb54-79"><a href="#cb54-79" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> piecewise_numba(subX)</span>
<span id="cb54-80"><a href="#cb54-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-81"><a href="#cb54-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"python loop:"</span>, bench(piecewise_python, subX))</span>
<span id="cb54-82"><a href="#cb54-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"numpy masks:"</span>, bench(piecewise_numpy_masks, subX))</span>
<span id="cb54-83"><a href="#cb54-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"numpy select:"</span>, bench(piecewise_numpy_select, subX))</span>
<span id="cb54-84"><a href="#cb54-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"numba single-pass:"</span>, bench(piecewise_numba, subX))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Warm up Numba…
python loop: {'median_s': 1.30168483700254, 'mean_s': 1.2997610680079863, 'min_s': 1.293608893000055, 'max_s': 1.303989474021364, 'n': 3}
numpy masks: {'median_s': 0.004838541004573926, 'mean_s': 0.004852356670501952, 'min_s': 0.004779270006110892, 'max_s': 0.004939259000821039, 'n': 3}
numpy select: {'median_s': 0.002205534983659163, 'mean_s': 0.002208151990392556, 'min_s': 0.0021943429892417043, 'max_s': 0.0022245779982768, 'n': 3}
numba single-pass: {'median_s': 0.004716848983662203, 'mean_s': 0.004716882326950629, 'min_s': 0.00470307600335218, 'max_s': 0.004730721993837506, 'n': 3}</code></pre>
</div>
</div>
</section>
<section id="blocktile-demo-optional" class="level3">
<h3 class="anchored" data-anchor-id="blocktile-demo-optional">5.4 Block/tile demo (optional)</h3>
<p>Operating on all genes at once can overflow caches. Here we tile the columns into smaller blocks to keep working sets in L2/L3.</p>
<div id="ROHS9hxdxffx" class="cell" data-outputid="1fe6e445-9e78-4d9d-dac4-e2ff9cea3dcf">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> hvg_blocked(X, block_size<span class="op">=</span><span class="dv">2000</span>):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">vars</span> <span class="op">=</span> []</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> start <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n, block_size):</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(start<span class="op">+</span>block_size, n)</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>        A <span class="op">=</span> X[:, start:end].toarray().astype(np.float32, copy<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">vars</span>.append(A.var(axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.concatenate(<span class="bu">vars</span>)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> adata.X[:<span class="dv">40_000</span>,:]</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"hvg_blocked:"</span>, bench(hvg_blocked, subX))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>hvg_blocked: {'median_s': 8.301487449003616, 'mean_s': 8.295305196671203, 'min_s': 8.278520805004518, 'max_s': 8.305907336005475, 'n': 3}</code></pre>
</div>
</div>
</section>
<section id="blocktile-processing" class="level3">
<h3 class="anchored" data-anchor-id="blocktile-processing">Block/tile processing</h3>
<p>Blocked HVG proxy: 8.30 s for 40k cells.</p>
<p>This method keeps working sets cache-friendly by slicing columns into blocks. It’s slower than lightweight row ops, but essential for expensive column-wise statistics (variance, correlations) when matrices get very wide.</p>
</section>
</section>
<section id="when-and-how-to-use-the-gpu" class="level2">
<h2 class="anchored" data-anchor-id="when-and-how-to-use-the-gpu">6. When and How to Use the GPU</h2>
<p>GPU acceleration can feel like a silver bullet, but it only wins under the right conditions. The key is to know when transfer costs dominate vs when workloads are big enough to amortize them.</p>
<p>Heuristics</p>
<ul>
<li><p>Yes: large matrix operations (PCA, KNN graph, UMAP, batched elementwise ops).</p></li>
<li><p>No: tiny slices, heavy Python control flow, or memory-bound row sums on small datasets.</p></li>
<li><p>Critical rule: once you move data to GPU, keep it there for as much of the pipeline as possible. Avoid CPU↔︎GPU ping-pong.</p></li>
</ul>
<hr>
<section id="cupy-basics-a-drop-in-numpy" class="level3">
<h3 class="anchored" data-anchor-id="cupy-basics-a-drop-in-numpy">6.1 CuPy basics: a drop-in NumPy</h3>
<p>We’ll start with a dense demo. CuPy arrays behave like NumPy arrays, but live on the GPU. Expect timings to look worse on tiny arrays (transfer cost dominates), but better on big ones.</p>
<div id="Kt28mfSE2Kae" class="cell" data-outputid="0ee3f612-91e2-4500-fad6-6217525911b2">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cupy <span class="im">as</span> cp</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, time</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench(fn, <span class="op">*</span>args, repeats<span class="op">=</span><span class="dv">3</span>, warmup<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs):</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(warmup): fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    ts <span class="op">=</span> []</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(repeats):</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>        fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>        ts.append(time.perf_counter() <span class="op">-</span> t0)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(median_s<span class="op">=</span>np.median(ts), mean_s<span class="op">=</span><span class="bu">float</span>(np.mean(ts)),</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>                min_s<span class="op">=</span><span class="bu">min</span>(ts), max_s<span class="op">=</span><span class="bu">max</span>(ts), n<span class="op">=</span>repeats)</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">5_000_000</span></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>x_cpu <span class="op">=</span> np.random.rand(n).astype(np.float32)</span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> cp.asarray(x_cpu)</span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"CPU sum:"</span>, bench(<span class="kw">lambda</span>: x_cpu.<span class="bu">sum</span>()))</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GPU sum (resident):"</span>, bench(<span class="kw">lambda</span>: x_gpu.<span class="bu">sum</span>()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU sum: {'median_s': 0.0019603009859565645, 'mean_s': 0.001969521321977178, 'min_s': 0.0019528029952198267, 'max_s': 0.0019954599847551435, 'n': 3}
GPU sum (resident): {'median_s': 0.0003212369920220226, 'mean_s': 0.0003279416705481708, 'min_s': 0.000268882024101913, 'max_s': 0.00039370599552057683, 'n': 3}</code></pre>
</div>
</div>
</section>
<section id="dense-hello-world-summing-5m-floats" class="level3">
<h3 class="anchored" data-anchor-id="dense-hello-world-summing-5m-floats">Dense “hello world”: summing 5M floats</h3>
<ul>
<li><p>CPU sum: 0.00196 s</p></li>
<li><p>GPU sum (already on device): 0.000321 s</p></li>
</ul>
<p>~6× faster on GPU when the array is already resident.</p>
<p>Moral: once data is on the GPU, even simple reductions get a consistent speedup. But the transfer has to be amortized</p>
<hr>
</section>
<section id="sparse-matrices-with-cupy" class="level3">
<h3 class="anchored" data-anchor-id="sparse-matrices-with-cupy">6.2 Sparse matrices with CuPy</h3>
<p>CuPy has <code>cupyx.scipy.sparse</code> which mirrors SciPy’s sparse formats. Let’s move our AnnData matrix to GPU as CSR and benchmark a row sum.</p>
<div id="djuJIP_d5iV1" class="cell" data-outputid="dfbd46b7-6a12-4bc8-a770-dd4ad2830c1b">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cupyx.scipy <span class="im">import</span> sparse <span class="im">as</span> cpx_sp</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_cupy_csr(X):</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> scipy <span class="im">import</span> sparse <span class="im">as</span> sp</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> sp.isspmatrix_csr(X):</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> cpx_sp.csr_matrix((cp.asarray(X.data),</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>                                  cp.asarray(X.indices),</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>                                  cp.asarray(X.indptr)),</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>                                 shape<span class="op">=</span>X.shape)</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cpx_sp.csr_matrix(X.tocsr())</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>Xg <span class="op">=</span> to_cupy_csr(adata.X)</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gpu_row_sums(Xg):</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Xg.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).get().ravel()</span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> adata.X[:<span class="dv">40_000</span>,:]</span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>subXg <span class="op">=</span> to_cupy_csr(subX)</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"CPU row sums:"</span>, bench(<span class="kw">lambda</span>: subX.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1))</span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GPU row sums:"</span>, bench(<span class="kw">lambda</span>: gpu_row_sums(subXg)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU row sums: {'median_s': 0.05175546099781059, 'mean_s': 0.051829414325766265, 'min_s': 0.05166531799477525, 'max_s': 0.05206746398471296, 'n': 3}
GPU row sums: {'median_s': 0.004674978990806267, 'mean_s': 0.004725466996508961, 'min_s': 0.004659167985664681, 'max_s': 0.0048422540130559355, 'n': 3}</code></pre>
</div>
</div>
</section>
<section id="sparse-csr-row-sums-40k-18k-slice" class="level3">
<h3 class="anchored" data-anchor-id="sparse-csr-row-sums-40k-18k-slice">Sparse CSR row sums (40k × 18k slice)</h3>
<ul>
<li><p>CPU row sums: 0.0518 s</p></li>
<li><p>GPU row sums (resident): 0.00467 s</p></li>
<li><p>~11× faster on GPU for the same operation.</p></li>
</ul>
<p>Transfer cost (host → device)</p>
<ul>
<li>40k-row CSR to GPU: ~0.248 s</li>
</ul>
<p>Put together:</p>
<ul>
<li><p>If you transfer once and run many ops (PCA → KNN → UMAP → clustering), the ~0.25 s “toll” fades into the noise.</p></li>
<li><p>If you bounce back and forth, you’ll erase the speedup. Keep the pipeline GPU-resident after the first hop.</p></li>
</ul>
<hr>
</section>
<section id="rapids-cudf-gpu-dataframes" class="level3">
<h3 class="anchored" data-anchor-id="rapids-cudf-gpu-dataframes">6.3 RAPIDS cuDF: GPU DataFrames</h3>
<p>For obs/var DataFrame operations (groupbys, joins), cuDF provides a GPU-accelerated replacement for pandas. Let’s demo a simple groupby.</p>
<div id="pmzfbe_n520i" class="cell" data-outputid="f2a07b15-0205-42e8-ec26-26679057f60a">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cudf</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> adata.X.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1.astype(np.float32)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>gdf <span class="op">=</span> cudf.DataFrame({<span class="st">"counts"</span>: cp.asarray(counts)})</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>gdf[<span class="st">"bin"</span>] <span class="op">=</span> (gdf[<span class="st">"counts"</span>] <span class="op">&gt;</span> <span class="dv">1000</span>).astype(<span class="st">"int8"</span>)</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GPU groupby mean:"</span>)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(gdf.groupby(<span class="st">"bin"</span>).counts.mean().to_pandas())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GPU groupby mean:
bin
0     733.258743
1    9076.485380
Name: counts, dtype: float64</code></pre>
</div>
</div>
</section>
</section>
<section id="hybrid-pipelines" class="level2">
<h2 class="anchored" data-anchor-id="hybrid-pipelines">7. Hybrid Pipelines</h2>
<p>The sweet spot is often split workflows: use the CPU for cheap preprocessing, then move to GPU for expensive linear algebra.</p>
<hr>
<section id="cpu-preprocessing-qc" class="level3">
<h3 class="anchored" data-anchor-id="cpu-preprocessing-qc">7.1 CPU preprocessing (QC)</h3>
<p>We’ll run QC metrics on CPU, as before.</p>
<div id="3MEghcKK5_FP" class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scanpy <span class="im">as</span> sc</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> qc_metrics(adata_):</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    sc.pp.calculate_qc_metrics(adata_, percent_top<span class="op">=</span><span class="va">None</span>, log1p<span class="op">=</span><span class="va">False</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>qc_metrics(adata[:<span class="dv">40_000</span>,:].copy())  <span class="co"># CPU slice for demo</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="transfer-once-to-gpu" class="level3">
<h3 class="anchored" data-anchor-id="transfer-once-to-gpu">7.2 Transfer once to GPU</h3>
<p>Now move the processed matrix to GPU. Measure the transfer cost separately so you can see when it’s worth it.</p>
<div id="ESI6Y5N36Gw_" class="cell" data-outputid="40b6211e-5b8c-4cdb-a1e8-7e0806880d23">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>subX <span class="op">=</span> adata.X[:<span class="dv">40_000</span>,:]</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transfer_to_gpu(X):</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> to_cupy_csr(X)</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Transfer 40k slice to GPU:"</span>)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bench(transfer_to_gpu, subX))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Transfer 40k slice to GPU:
{'median_s': 0.24823900000774302, 'mean_s': 0.24861315032467246, 'min_s': 0.24495537197799422, 'max_s': 0.25264507898828015, 'n': 3}</code></pre>
</div>
</div>
</section>
<section id="gpu-linear-algebra-pca-neighbors-clustering" class="level3">
<h3 class="anchored" data-anchor-id="gpu-linear-algebra-pca-neighbors-clustering">7.3 GPU linear algebra (PCA → neighbors → clustering)</h3>
<p>RAPIDS cuml provides GPU PCA and KNN. The pattern is: keep everything on GPU until you have small results to bring back.</p>
<div id="xn9vbxDC6LaP" class="cell" data-outputid="81ad485f-98c9-48e6-fc8b-5f96780f3e75">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scanpy <span class="im">as</span> sc</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cupy <span class="im">as</span> cp</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cuml.decomposition <span class="im">import</span> PCA <span class="im">as</span> cuPCA</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cuml.neighbors <span class="im">import</span> NearestNeighbors</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) HVGs on CPU</span></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>adata_tmp <span class="op">=</span> adata[:<span class="dv">40_000</span>, :].copy()          <span class="co"># demo subset; scale up as you like</span></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>sc.pp.highly_variable_genes(adata_tmp, n_top_genes<span class="op">=</span><span class="dv">2000</span>, flavor<span class="op">=</span><span class="st">"seurat_v3"</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>hvg_mask <span class="op">=</span> adata_tmp.var[<span class="st">"highly_variable"</span>].values</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> hvg_mask.<span class="bu">sum</span>() <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Slice to HVGs (still CSR on CPU)</span></span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>X_hvg <span class="op">=</span> adata_tmp.X[:, hvg_mask]</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Move once to GPU as DENSE (width=~2k → safe)</span></span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a>Xg <span class="op">=</span> cp.asarray(X_hvg.toarray(), dtype<span class="op">=</span>cp.float32, order<span class="op">=</span><span class="st">"C"</span>)  <span class="co"># deliberate densify after HVG</span></span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) PCA on GPU</span></span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> cuPCA(n_components<span class="op">=</span><span class="dv">50</span>, svd_solver<span class="op">=</span><span class="st">"jacobi"</span>, output_type<span class="op">=</span><span class="st">"cupy"</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>Yg <span class="op">=</span> pca.fit_transform(Xg)                           <span class="co"># (n_cells, 50) on GPU</span></span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 5) Neighbors on GPU</span></span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> NearestNeighbors(n_neighbors<span class="op">=</span><span class="dv">15</span>, algorithm<span class="op">=</span><span class="st">"brute"</span>)</span>
<span id="cb67-25"><a href="#cb67-25" aria-hidden="true" tabindex="-1"></a>knn.fit(Yg)</span>
<span id="cb67-26"><a href="#cb67-26" aria-hidden="true" tabindex="-1"></a>distances, indices <span class="op">=</span> knn.kneighbors(Yg)</span>
<span id="cb67-27"><a href="#cb67-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-28"><a href="#cb67-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GPU pipeline OK:"</span>, Xg.shape, Yg.shape, indices.shape, <span class="bu">type</span>(Yg), <span class="bu">type</span>(indices))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>extracting highly variable genes
--&gt; added
    'highly_variable', boolean vector (adata.var)
    'highly_variable_rank', float vector (adata.var)
    'means', float vector (adata.var)
    'variances', float vector (adata.var)
    'variances_norm', float vector (adata.var)
GPU pipeline OK: (40000, 2000) (40000, 50) (40000, 15) &lt;class 'cupy.ndarray'&gt; &lt;class 'cupy.ndarray'&gt;</code></pre>
</div>
</div>
</section>
</section>
<section id="rapids-single-cell" class="level2">
<h2 class="anchored" data-anchor-id="rapids-single-cell">7.4 RAPIDS Single-Cell</h2>
<p>What is RAPIDS Single-Cell (rsc)?</p>
<p>RSC is a GPU-accelerated, Scanpy-compatible toolkit. It aims to be a near drop-in replacement: you keep using <code>AnnData</code>, but call <code>rsc.pp.*</code> and <code>rsc.tl.*</code> for GPU versions of familiar steps (normalize, log1p, HVGs, PCA/TSVD, neighbors, UMAP, clustering). Under the hood it leans on CuPy and RAPIDS cuML/cuGraph. In many workflows you’ll see order-of-magnitude speedups while keeping the Scanpy mental model.</p>
<p>Tips:</p>
<p>It works with a standard <code>AnnData</code>. For best performance, keep data <code>float32</code>, and try to keep matrices GPU-resident once you move them there to avoid PCIe ping-pong.</p>
<p>You can start with CPU-backed AnnData; rsc will move data as needed. If you want full control, you can put a CuPy dense or CuPy-sparse array into <code>adata.X</code> before calling rsc</p>
<p>We’ll run the standard preprocessing and analysis steps on GPU via rsc: total-count normalization → log1p → HVGs (2k) → PCA (50 PCs) → neighbors (k=15) → UMAP. This mirrors the manual pipeline you just ran (HVG → GPU PCA → KNN), but with fewer lines and tight Scanpy compatibility.</p>
<div id="wMLbotpd6Qnl" class="cell" data-outputid="5ee0b367-51ef-44e5-cb87-99c540489188">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time, statistics <span class="im">as</span> stats</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cupy <span class="im">as</span> cp</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rapids_singlecell <span class="im">as</span> rsc</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scanpy <span class="im">as</span> sc</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cupyx.scipy <span class="im">import</span> sparse <span class="im">as</span> cpx_sp</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse <span class="im">as</span> sp</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gpu_sync():</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ensure all queued GPU work finishes before we stop timers</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>        cp.cuda.Stream.null.synchronize()</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench_once(fn, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>    t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>    gpu_sync()</span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out, (time.perf_counter() <span class="op">-</span> t0)</span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-22"><a href="#cb69-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bench_step(label, fn, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb69-23"><a href="#cb69-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># one warmup + 3 measured runs (use median)</span></span>
<span id="cb69-24"><a href="#cb69-24" aria-hidden="true" tabindex="-1"></a>    _, _ <span class="op">=</span> bench_once(fn, <span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb69-25"><a href="#cb69-25" aria-hidden="true" tabindex="-1"></a>    ts <span class="op">=</span> []</span>
<span id="cb69-26"><a href="#cb69-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb69-27"><a href="#cb69-27" aria-hidden="true" tabindex="-1"></a>        _, dt <span class="op">=</span> bench_once(fn, <span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb69-28"><a href="#cb69-28" aria-hidden="true" tabindex="-1"></a>        ts.append(dt)</span>
<span id="cb69-29"><a href="#cb69-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">float</span>(np.median(ts))</span>
<span id="cb69-30"><a href="#cb69-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-31"><a href="#cb69-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_gpu_csr(X):</span>
<span id="cb69-32"><a href="#cb69-32" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X.tocsr().astype(<span class="st">'float32'</span>, copy<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb69-33"><a href="#cb69-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cpx_sp.csr_matrix(</span>
<span id="cb69-34"><a href="#cb69-34" aria-hidden="true" tabindex="-1"></a>        (cp.asarray(X.data), cp.asarray(X.indices), cp.asarray(X.indptr)),</span>
<span id="cb69-35"><a href="#cb69-35" aria-hidden="true" tabindex="-1"></a>        shape<span class="op">=</span>X.shape)</span>
<span id="cb69-36"><a href="#cb69-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-37"><a href="#cb69-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gpu_rsc_pipeline(adata_src, n_top_genes<span class="op">=</span><span class="dv">2000</span>, n_pcs<span class="op">=</span><span class="dv">50</span>, k<span class="op">=</span><span class="dv">15</span>):</span>
<span id="cb69-38"><a href="#cb69-38" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> adata_src.copy()</span>
<span id="cb69-39"><a href="#cb69-39" aria-hidden="true" tabindex="-1"></a>    A.X <span class="op">=</span> to_gpu_csr(A.X)  <span class="co"># ensure GPU CSR</span></span>
<span id="cb69-40"><a href="#cb69-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-41"><a href="#cb69-41" aria-hidden="true" tabindex="-1"></a>    times <span class="op">=</span> {}</span>
<span id="cb69-42"><a href="#cb69-42" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'normalize_total'</span>] <span class="op">=</span> bench_step(<span class="st">"norm"</span>, rsc.pp.normalize_total, A, target_sum<span class="op">=</span><span class="fl">1e4</span>)</span>
<span id="cb69-43"><a href="#cb69-43" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'log1p'</span>]           <span class="op">=</span> bench_step(<span class="st">"log1p"</span>, rsc.pp.log1p, A)</span>
<span id="cb69-44"><a href="#cb69-44" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'hvg'</span>]             <span class="op">=</span> bench_step(<span class="st">"hvg"</span>, rsc.pp.highly_variable_genes, A, n_top_genes<span class="op">=</span>n_top_genes)</span>
<span id="cb69-45"><a href="#cb69-45" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'pca'</span>]             <span class="op">=</span> bench_step(<span class="st">"pca"</span>, rsc.pp.pca, A, n_comps<span class="op">=</span>n_pcs)</span>
<span id="cb69-46"><a href="#cb69-46" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'neighbors'</span>]       <span class="op">=</span> bench_step(<span class="st">"neighbors"</span>, rsc.pp.neighbors, A, n_neighbors<span class="op">=</span>k)</span>
<span id="cb69-47"><a href="#cb69-47" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'umap'</span>]            <span class="op">=</span> bench_step(<span class="st">"umap"</span>, rsc.tl.umap, A)</span>
<span id="cb69-48"><a href="#cb69-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-49"><a href="#cb69-49" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="bu">sum</span>(times.values())</span>
<span id="cb69-50"><a href="#cb69-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A, times, total</span>
<span id="cb69-51"><a href="#cb69-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-52"><a href="#cb69-52" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cpu_scanpy_pipeline(adata_src, n_top_genes<span class="op">=</span><span class="dv">2000</span>, n_pcs<span class="op">=</span><span class="dv">50</span>, k<span class="op">=</span><span class="dv">15</span>):</span>
<span id="cb69-53"><a href="#cb69-53" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> adata_src.copy()</span>
<span id="cb69-54"><a href="#cb69-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-55"><a href="#cb69-55" aria-hidden="true" tabindex="-1"></a>    times <span class="op">=</span> {}</span>
<span id="cb69-56"><a href="#cb69-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># normalize_total in Scanpy 1.11 still supports target_sum; mutates in place</span></span>
<span id="cb69-57"><a href="#cb69-57" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'normalize_total'</span>] <span class="op">=</span> bench_step(<span class="st">"norm"</span>, sc.pp.normalize_total, A, target_sum<span class="op">=</span><span class="fl">1e4</span>)</span>
<span id="cb69-58"><a href="#cb69-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-59"><a href="#cb69-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># log1p in Scanpy 1.11 mutates in place by default; DO NOT pass inplace</span></span>
<span id="cb69-60"><a href="#cb69-60" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'log1p'</span>]           <span class="op">=</span> bench_step(<span class="st">"log1p"</span>, sc.pp.log1p, A)</span>
<span id="cb69-61"><a href="#cb69-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-62"><a href="#cb69-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># HVGs (CPU), inplace by default in this version</span></span>
<span id="cb69-63"><a href="#cb69-63" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'hvg'</span>]             <span class="op">=</span> bench_step(</span>
<span id="cb69-64"><a href="#cb69-64" aria-hidden="true" tabindex="-1"></a>        <span class="st">"hvg"</span>, sc.pp.highly_variable_genes, A,</span>
<span id="cb69-65"><a href="#cb69-65" aria-hidden="true" tabindex="-1"></a>        n_top_genes<span class="op">=</span>n_top_genes, flavor<span class="op">=</span><span class="st">"seurat_v3"</span></span>
<span id="cb69-66"><a href="#cb69-66" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb69-67"><a href="#cb69-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-68"><a href="#cb69-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># subset to HVGs before PCA to keep the dense step bounded</span></span>
<span id="cb69-69"><a href="#cb69-69" aria-hidden="true" tabindex="-1"></a>    hvg_mask <span class="op">=</span> A.var[<span class="st">'highly_variable'</span>].values</span>
<span id="cb69-70"><a href="#cb69-70" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> A[:, hvg_mask].copy()</span>
<span id="cb69-71"><a href="#cb69-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-72"><a href="#cb69-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PCA (CPU; densifies X[:, HVG])</span></span>
<span id="cb69-73"><a href="#cb69-73" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'pca'</span>]             <span class="op">=</span> bench_step(<span class="st">"pca"</span>, sc.pp.pca, A, n_comps<span class="op">=</span>n_pcs, svd_solver<span class="op">=</span><span class="st">"arpack"</span>)</span>
<span id="cb69-74"><a href="#cb69-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-75"><a href="#cb69-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># neighbors + umap (CPU)</span></span>
<span id="cb69-76"><a href="#cb69-76" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'neighbors'</span>]       <span class="op">=</span> bench_step(<span class="st">"neighbors"</span>, sc.pp.neighbors, A, n_neighbors<span class="op">=</span>k)</span>
<span id="cb69-77"><a href="#cb69-77" aria-hidden="true" tabindex="-1"></a>    times[<span class="st">'umap'</span>]            <span class="op">=</span> bench_step(<span class="st">"umap"</span>, sc.tl.umap, A)</span>
<span id="cb69-78"><a href="#cb69-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-79"><a href="#cb69-79" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="bu">sum</span>(times.values())</span>
<span id="cb69-80"><a href="#cb69-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A, times, total</span>
<span id="cb69-81"><a href="#cb69-81" aria-hidden="true" tabindex="-1"></a><span class="co"># use the same 40k slice you benchmarked earlier</span></span>
<span id="cb69-82"><a href="#cb69-82" aria-hidden="true" tabindex="-1"></a>base <span class="op">=</span> adata[:<span class="dv">40_000</span>, :]</span>
<span id="cb69-83"><a href="#cb69-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-84"><a href="#cb69-84" aria-hidden="true" tabindex="-1"></a>gpu_A, gpu_times, gpu_total <span class="op">=</span> gpu_rsc_pipeline(base, n_top_genes<span class="op">=</span><span class="dv">2000</span>, n_pcs<span class="op">=</span><span class="dv">50</span>, k<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb69-85"><a href="#cb69-85" aria-hidden="true" tabindex="-1"></a>cpu_A, cpu_times, cpu_total <span class="op">=</span> cpu_scanpy_pipeline(base, n_top_genes<span class="op">=</span><span class="dv">2000</span>, n_pcs<span class="op">=</span><span class="dv">50</span>, k<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb69-86"><a href="#cb69-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-87"><a href="#cb69-87" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pretty(d):</span>
<span id="cb69-88"><a href="#cb69-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">", "</span>.join(<span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">=</span><span class="sc">{</span>v<span class="sc">:.3f}</span><span class="ss">s"</span> <span class="cf">for</span> k,v <span class="kw">in</span> d.items())</span>
<span id="cb69-89"><a href="#cb69-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-90"><a href="#cb69-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GPU (rsc) times:"</span>)</span>
<span id="cb69-91"><a href="#cb69-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pretty(gpu_times), <span class="ss">f"| TOTAL=</span><span class="sc">{</span>gpu_total<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb69-92"><a href="#cb69-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GPU shapes:"</span>, gpu_A.obsm[<span class="st">'X_pca'</span>].shape, gpu_A.obsm[<span class="st">'X_umap'</span>].shape)</span>
<span id="cb69-93"><a href="#cb69-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-94"><a href="#cb69-94" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">CPU (scanpy) times:"</span>)</span>
<span id="cb69-95"><a href="#cb69-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pretty(cpu_times), <span class="ss">f"| TOTAL=</span><span class="sc">{</span>cpu_total<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb69-96"><a href="#cb69-96" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"CPU shapes:"</span>, cpu_A.obsm[<span class="st">'X_pca'</span>].shape, cpu_A.obsm[<span class="st">'X_umap'</span>].shape)</span>
<span id="cb69-97"><a href="#cb69-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-98"><a href="#cb69-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Speedups (CPU/GPU):"</span>)</span>
<span id="cb69-99"><a href="#cb69-99" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> gpu_times:</span>
<span id="cb69-100"><a href="#cb69-100" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> k <span class="kw">in</span> cpu_times:</span>
<span id="cb69-101"><a href="#cb69-101" aria-hidden="true" tabindex="-1"></a>        spd <span class="op">=</span> cpu_times[k] <span class="op">/</span> <span class="bu">max</span>(<span class="fl">1e-9</span>, gpu_times[k])</span>
<span id="cb69-102"><a href="#cb69-102" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>k<span class="sc">:&gt;12}</span><span class="ss">: ×</span><span class="sc">{</span>spd<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb69-103"><a href="#cb69-103" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'TOTAL'</span><span class="sc">:&gt;12}</span><span class="ss">: ×</span><span class="sc">{</span>cpu_total<span class="op">/</span><span class="bu">max</span>(<span class="fl">1e-9</span>,gpu_total)<span class="sc">:.1f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>normalizing counts per cell
    finished (0:00:00)
normalizing counts per cell
    finished (0:00:00)
normalizing counts per cell
    finished (0:00:00)
normalizing counts per cell
    finished (0:00:00)
WARNING: adata.X seems to be already log-transformed.
WARNING: adata.X seems to be already log-transformed.
WARNING: adata.X seems to be already log-transformed.
extracting highly variable genes</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/legacy_api_wrap/__init__.py:82: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.
  return fn(*args_all, **kw)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>--&gt; added
    'highly_variable', boolean vector (adata.var)
    'highly_variable_rank', float vector (adata.var)
    'means', float vector (adata.var)
    'variances', float vector (adata.var)
    'variances_norm', float vector (adata.var)
extracting highly variable genes</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/legacy_api_wrap/__init__.py:82: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.
  return fn(*args_all, **kw)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>--&gt; added
    'highly_variable', boolean vector (adata.var)
    'highly_variable_rank', float vector (adata.var)
    'means', float vector (adata.var)
    'variances', float vector (adata.var)
    'variances_norm', float vector (adata.var)
extracting highly variable genes</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/legacy_api_wrap/__init__.py:82: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.
  return fn(*args_all, **kw)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>--&gt; added
    'highly_variable', boolean vector (adata.var)
    'highly_variable_rank', float vector (adata.var)
    'means', float vector (adata.var)
    'variances', float vector (adata.var)
    'variances_norm', float vector (adata.var)
extracting highly variable genes</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/legacy_api_wrap/__init__.py:82: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.
  return fn(*args_all, **kw)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>--&gt; added
    'highly_variable', boolean vector (adata.var)
    'highly_variable_rank', float vector (adata.var)
    'means', float vector (adata.var)
    'variances', float vector (adata.var)
    'variances_norm', float vector (adata.var)
computing PCA
    with n_comps=50
    finished (0:00:07)
computing PCA
    with n_comps=50
    finished (0:00:07)
computing PCA
    with n_comps=50
    finished (0:00:07)
computing PCA
    with n_comps=50
    finished (0:00:07)
computing neighbors
    using 'X_pca' with n_pcs = 50</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
<span class="ansi-red-fg">AttributeError</span>: 'MessageFactory' object has no attribute 'GetPrototype'</pre>
</div>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
<span class="ansi-red-fg">AttributeError</span>: 'MessageFactory' object has no attribute 'GetPrototype'</pre>
</div>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
<span class="ansi-red-fg">AttributeError</span>: 'MessageFactory' object has no attribute 'GetPrototype'</pre>
</div>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
<span class="ansi-red-fg">AttributeError</span>: 'MessageFactory' object has no attribute 'GetPrototype'</pre>
</div>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
<span class="ansi-red-fg">AttributeError</span>: 'MessageFactory' object has no attribute 'GetPrototype'</pre>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>    finished: added to `.uns['neighbors']`
    `.obsp['distances']`, distances for each pair of neighbors
    `.obsp['connectivities']`, weighted adjacency matrix (0:01:08)
computing neighbors
    using 'X_pca' with n_pcs = 50
    finished: added to `.uns['neighbors']`
    `.obsp['distances']`, distances for each pair of neighbors
    `.obsp['connectivities']`, weighted adjacency matrix (0:00:06)
computing neighbors
    using 'X_pca' with n_pcs = 50
    finished: added to `.uns['neighbors']`
    `.obsp['distances']`, distances for each pair of neighbors
    `.obsp['connectivities']`, weighted adjacency matrix (0:00:06)
computing neighbors
    using 'X_pca' with n_pcs = 50
    finished: added to `.uns['neighbors']`
    `.obsp['distances']`, distances for each pair of neighbors
    `.obsp['connectivities']`, weighted adjacency matrix (0:00:06)
computing UMAP
    finished: added
    'X_umap', UMAP coordinates (adata.obsm)
    'umap', UMAP parameters (adata.uns) (0:00:31)
computing UMAP
    finished: added
    'X_umap', UMAP coordinates (adata.obsm)
    'umap', UMAP parameters (adata.uns) (0:00:29)
computing UMAP
    finished: added
    'X_umap', UMAP coordinates (adata.obsm)
    'umap', UMAP parameters (adata.uns) (0:00:29)
computing UMAP
    finished: added
    'X_umap', UMAP coordinates (adata.obsm)
    'umap', UMAP parameters (adata.uns) (0:00:29)

GPU (rsc) times:
normalize_total=0.016s, log1p=0.026s, hvg=0.049s, pca=0.183s, neighbors=0.118s, umap=0.107s | TOTAL=0.499s
GPU shapes: (40000, 50) (40000, 2)

CPU (scanpy) times:
normalize_total=0.337s, log1p=0.174s, hvg=2.033s, pca=7.307s, neighbors=6.432s, umap=29.808s | TOTAL=46.090s
CPU shapes: (40000, 50) (40000, 2)

Speedups (CPU/GPU):
normalize_total: ×21.1
       log1p: ×6.6
         hvg: ×41.2
         pca: ×39.9
   neighbors: ×54.6
        umap: ×279.0
       TOTAL: ×92.3</code></pre>
</div>
</div>
</section>
<section id="emerging-tools" class="level2">
<h2 class="anchored" data-anchor-id="emerging-tools">8. Emerging Tools</h2>
<p>Single-cell analysis has historically revolved around Scanpy (Python/CPU) and Seurat (R/CPU). Over the last 2–3 years, the performance landscape has expanded dramatically. Here’s a guided sweep of the tools you should be aware of, even if you don’t use them immediately.</p>
<section id="gpu-native-runtimes" class="level3">
<h3 class="anchored" data-anchor-id="gpu-native-runtimes">8.1 GPU-native runtimes</h3>
<ul>
<li><p>RAPIDS Single-Cell (rapids_singlecell)</p>
<ul>
<li><p>Already demoed. Scanpy-like API, accelerated PCA/UMAP/KNN.</p></li>
<li><p>Good entry point if you’re comfortable in Python and want fast wins on 100k–1M cell data.</p></li>
</ul></li>
<li><p>PyTorch Sparse / JAX Sparse</p>
<ul>
<li><p>Experimental support for sparse tensors in deep-learning frameworks.</p></li>
<li><p>Useful if you’re moving from QC into deep generative models (scVI, VAEs, diffusion models).</p></li>
<li><p>These frameworks can directly interop with GPUs, TPUs, or clusters.</p></li>
</ul></li>
<li><p>scvi-tools / PyTorch Geometric</p>
<ul>
<li><p>scvi-tools brings state-of-the-art generative models to single-cell (e.g.&nbsp;batch correction, multimodal integration).</p></li>
<li><p>PyTorch Geometric supports graph neural networks — potentially useful for cell-cell interaction models.</p></li>
</ul></li>
</ul>
</section>
<section id="scalable-array-storage-backends" class="level3">
<h3 class="anchored" data-anchor-id="scalable-array-storage-backends">8.2 Scalable array &amp; storage backends</h3>
<ul>
<li><p>Zarr / TileDB</p>
<ul>
<li><p>Both store n-dimensional arrays on disk in chunked formats.</p></li>
<li><p>Let you stream slices (cells × genes) into memory without ever loading the full dense array.</p></li>
<li><p>Important if you’re analyzing &gt;1M cells where even CSR won’t fit in RAM.</p></li>
<li><p>Zarr integrates with Scanpy; TileDB adds cloud-native storage (S3/GCS/Azure).</p></li>
</ul></li>
<li><p>Apache Arrow / Polars</p>
<ul>
<li><p>Arrow defines a columnar in-memory format with zero-copy sharing across languages.</p></li>
<li><p>Polars is a DataFrame library built on Arrow: multithreaded, faster than pandas.</p></li>
<li><p>Advanced: replace heavy .obs / .var pandas operations with Polars for speed.</p></li>
</ul></li>
</ul>
</section>
<section id="cpu-accelerators" class="level3">
<h3 class="anchored" data-anchor-id="cpu-accelerators">8.3 CPU accelerators</h3>
<ul>
<li><p>NumExpr / Bottleneck</p>
<ul>
<li><p>Vectorized kernels for elementwise arithmetic and reductions in dense arrays.</p></li>
<li><p>If you’re stuck with a dense step (e.g.&nbsp;PCA input after HVG selection), these can cut runtime 2–3× with minimal code change.</p></li>
</ul></li>
<li><p>Numba</p>
<ul>
<li><p>Already shown for custom kernels. Still one of the cleanest ways to JIT-compile a Python loop down to native code.</p></li>
<li><p>Best for bespoke elementwise transforms when you can’t express the logic as pure vectorization.</p></li>
</ul></li>
</ul>
</section>
<section id="clusterdistributed-execution" class="level3">
<h3 class="anchored" data-anchor-id="clusterdistributed-execution">8.4 Cluster/distributed execution</h3>
<ul>
<li><p>For datasets approaching multi-million cells, sometimes even a single GPU workstation isn’t enough. That’s where distributed runtimes shine:</p></li>
<li><p>Dask</p>
<ul>
<li><p>Python’s “batteries-included” distributed array/dataframe engine.</p></li>
<li><p>Integrates with RAPIDS: you can distribute CuPy/GPUs across nodes.</p></li>
<li><p>Lets you keep the Scanpy-like workflow, just at cluster scale.</p></li>
</ul></li>
<li><p>Apache Spark</p>
<ul>
<li><p>More heavyweight, but battle-tested in data engineering.</p></li>
<li><p>Spark DataFrames (with Arrow interop) can store and query single-cell metadata across clusters.</p></li>
<li><p>Most useful when analysis is tied into big data pipelines (ETL, multi-omics integration with other large datasets).</p></li>
</ul></li>
<li><p>Ray</p>
<ul>
<li><p>General-purpose distributed execution framework.</p></li>
<li><p>Used by some ML workflows for scalable hyperparameter tuning and batch inference on cell embeddings.</p></li>
</ul></li>
</ul>
<div id="K9HApnD3B-8s" class="cell" data-outputid="494a63c7-f301-45e9-a031-bfb8a8817ef5">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>env LC_ALL<span class="op">=</span>C.UTF<span class="op">-</span><span class="dv">8</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>env LANG<span class="op">=</span>C.UTF<span class="op">-</span><span class="dv">8</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>env: LC_ALL=C.UTF-8
env: LANG=C.UTF-8</code></pre>
</div>
</div>
<div id="QdZygtxb8Uhq" class="cell">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dask[complete] dask<span class="op">-</span>image tqdm ray[default] <span class="op">--</span>quiet</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="dask-lazy-parallel-compute-over-chunks-single-machine" class="level3">
<h3 class="anchored" data-anchor-id="dask-lazy-parallel-compute-over-chunks-single-machine">Dask lazy parallel compute over chunks (single machine)</h3>
<p>What this shows</p>
<p>We compute per-gene sums/nnz using Dask delayed on CSR column blocks in parallel, then materialize the result with a progress bar. This is the “hello world” for scaling chunked operations without changing your core logic.</p>
<div id="IBF4wQkjCGFr" class="cell" data-outputid="1f5d514f-2089-4259-af5e-64fd41970bcb">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dask <span class="im">import</span> delayed, compute</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dask.diagnostics <span class="im">import</span> ProgressBar</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> adata.X.tocsr().astype(np.float32, copy<span class="op">=</span><span class="va">False</span>)  <span class="co"># CPU CSR</span></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> col_block_stats(X, start, end):</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute per-gene (column) sum and nnz for a slice of columns</span></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>    Xc <span class="op">=</span> X[:, start:end]           <span class="co"># still sparse</span></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> np.asarray(Xc.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)).ravel().astype(np.float32, copy<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> Xc.getnnz(axis<span class="op">=</span><span class="dv">0</span>).astype(np.int32, copy<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> s, c</span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a lazy graph of tasks over column blocks</span></span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>block <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a>tasks <span class="op">=</span> []</span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j0 <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, X.shape[<span class="dv">1</span>], block):</span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>    j1 <span class="op">=</span> <span class="bu">min</span>(j0<span class="op">+</span>block, X.shape[<span class="dv">1</span>])</span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a>    tasks.append(delayed(col_block_stats)(X, j0, j1))</span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Trigger parallel execution</span></span>
<span id="cb83-22"><a href="#cb83-22" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> ProgressBar():</span>
<span id="cb83-23"><a href="#cb83-23" aria-hidden="true" tabindex="-1"></a>    parts <span class="op">=</span> compute(<span class="op">*</span>tasks)</span>
<span id="cb83-24"><a href="#cb83-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-25"><a href="#cb83-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Stitch results</span></span>
<span id="cb83-26"><a href="#cb83-26" aria-hidden="true" tabindex="-1"></a>sums <span class="op">=</span> np.concatenate([p[<span class="dv">0</span>] <span class="cf">for</span> p <span class="kw">in</span> parts])</span>
<span id="cb83-27"><a href="#cb83-27" aria-hidden="true" tabindex="-1"></a>nnzs <span class="op">=</span> np.concatenate([p[<span class="dv">1</span>] <span class="cf">for</span> p <span class="kw">in</span> parts])</span>
<span id="cb83-28"><a href="#cb83-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dask finished:"</span>, sums.shape, nnzs.shape, <span class="st">"sum(sums)="</span>, <span class="bu">float</span>(sums.<span class="bu">sum</span>()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[########################################] | 100% Completed | 6.11 s
Dask finished: (18129,) (18129,) sum(sums)= 2566197760.0</code></pre>
</div>
</div>
<p>Why this is useful: it’s the same logic you’d write with a for-loop, but Dask builds a DAG and runs blocks in parallel using your CPU cores.</p>
</section>
<section id="dask-array-out-of-core-dense-blocks-optional-if-you-have-zarr" class="level3">
<h3 class="anchored" data-anchor-id="dask-array-out-of-core-dense-blocks-optional-if-you-have-zarr">Dask Array — out-of-core dense blocks (optional if you have Zarr)</h3>
<p>What this shows</p>
<p>If you have a dense array (e.g., after HVG selection) or a Zarr store, Dask Array can make blocked operations trivial.</p>
<div id="WTTTFwMkCo1K" class="cell" data-outputid="b7bb1a44-1d40-4d93-89b2-481a700614f1">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> dask.array <span class="im">as</span> da</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: turn a dense (n_obs x 2000) view into a dask array with 10k-row chunks</span></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>X_hvg <span class="op">=</span> adata[:, adata.var_names[:<span class="dv">2000</span>]].X.toarray().astype(np.float32)  <span class="co"># demo</span></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>dX <span class="op">=</span> da.from_array(X_hvg, chunks<span class="op">=</span>(<span class="dv">10_000</span>, <span class="op">-</span><span class="dv">1</span>))   <span class="co"># chunk rows, keep columns whole</span></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute per-row sums and per-column means lazily, then .compute()</span></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>row_sums <span class="op">=</span> dX.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).compute()</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>col_means <span class="op">=</span> dX.mean(axis<span class="op">=</span><span class="dv">0</span>).compute()</span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(row_sums.shape, col_means.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(289331,) (2000,)</code></pre>
</div>
</div>
<p>When to use: dense or Zarr-backed workflows where you want simple NumPy-like code that scales to memory.</p>
</section>
<section id="ray-parallel-map-over-batches" class="level3">
<h3 class="anchored" data-anchor-id="ray-parallel-map-over-batches">Ray — parallel “map over batches”</h3>
<p>What this shows</p>
<p>Parallelize a function that processes a batch of cells (e.g., compute a few QC metrics) across CPU cores with Ray tasks. Good for wrapping bits of Scanpy/rsc you can’t easily vectorize.</p>
<div id="Bla_eA0KC57p" class="cell" data-outputid="395a0e87-2f07-45c4-9ada-7d41f3ee2f4f">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, ray</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>ray.shutdown()<span class="op">;</span> ray.init(ignore_reinit_error<span class="op">=</span><span class="va">True</span>, include_dashboard<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> qc_batch(X_csr, rows):</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    sub <span class="op">=</span> X_csr[rows, :]</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> np.asarray(sub.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)).ravel().astype(np.float32)</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>    genes  <span class="op">=</span> sub.getnnz(axis<span class="op">=</span><span class="dv">1</span>).astype(np.int32)</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> counts, genes</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a><span class="at">@ray.remote</span></span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> qc_batch_remote(X_csr, rows):</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> qc_batch(X_csr, rows)</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> adata.X.tocsr().astype(np.float32, copy<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> <span class="dv">10_000</span></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>row_idx <span class="op">=</span> np.arange(X.shape[<span class="dv">0</span>])</span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a>batches <span class="op">=</span> [row_idx[i:i<span class="op">+</span>batch] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(row_idx), batch)]</span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>futs <span class="op">=</span> [qc_batch_remote.remote(X, b) <span class="cf">for</span> b <span class="kw">in</span> batches]</span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>parts <span class="op">=</span> ray.get(futs)</span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.concatenate([p[<span class="dv">0</span>] <span class="cf">for</span> p <span class="kw">in</span> parts])</span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a>genes  <span class="op">=</span> np.concatenate([p[<span class="dv">1</span>] <span class="cf">for</span> p <span class="kw">in</span> parts])</span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Ray finished:"</span>, counts.shape, genes.shape, <span class="st">"sum(counts)="</span>, <span class="bu">float</span>(counts.<span class="bu">sum</span>()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2025-08-23 17:48:24,451 INFO worker.py:1781 -- Started a local Ray instance.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Ray finished: (289331,) (289331,) sum(counts)= 2566203648.0</code></pre>
</div>
</div>
<p>Why Ray: dead-simple task API; handy when you want to parallelize Python functions that don’t fit a simple array algebra mold.</p>
</section>
<section id="spark-local-mode-dataframesql-over-big-metadata-advanced" class="level3">
<h3 class="anchored" data-anchor-id="spark-local-mode-dataframesql-over-big-metadata-advanced">Spark (local mode) — DataFrame/SQL over big metadata (advanced)</h3>
<p>What this shows</p>
<p>Use Spark locally to handle large obs metadata joins/groupbys. This is not for sparse matrix arithmetic; it’s for data engineering tasks around your matrix.</p>
<div id="sECjCIt3DQoc" class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Spark is heavy and may clash with your pyarrow/RAPIDS</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="co"># If you try:  pip install -q pyspark==3.5.1 pyarrow==14.0.2  (pin Arrow!)</span></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql <span class="im">import</span> SparkSession, functions <span class="im">as</span> F</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>spark <span class="op">=</span> SparkSession.builder.master(<span class="st">"local[*]"</span>).appName(<span class="st">"singlecell-demo"</span>).getOrCreate()</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert .obs to Spark DataFrame (small demo sample)</span></span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>obs_df <span class="op">=</span> adata.obs[[<span class="st">'sample'</span>, <span class="st">'n_counts'</span>, <span class="st">'percent_mito'</span>]].reset_index(names<span class="op">=</span><span class="st">'cell'</span>)</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>sdf <span class="op">=</span> spark.createDataFrame(obs_df)</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Group-by example: per-sample summary</span></span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>agg <span class="op">=</span> (sdf.groupBy(<span class="st">"sample"</span>)</span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>         .agg(F.count(<span class="st">"*"</span>).alias(<span class="st">"cells"</span>),</span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a>              F.avg(<span class="st">"n_counts"</span>).alias(<span class="st">"mean_counts"</span>),</span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a>              F.avg(<span class="st">"percent_mito"</span>).alias(<span class="st">"mean_mito"</span>))</span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a>agg.show(<span class="dv">10</span>, truncate<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb90-20"><a href="#cb90-20" aria-hidden="true" tabindex="-1"></a>spark.stop()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When Spark makes sense: your single-cell pipeline sits inside a larger ETL/warehouse stack; you need SQL joins, window functions, or to integrate with Parquet/Lakehouse at scale.</p>
</section>
</section>
<section id="nvidias-multi-gpu-1m-single-cell-notebook" class="level2">
<h2 class="anchored" data-anchor-id="nvidias-multi-gpu-1m-single-cell-notebook">NVIDIA’s multi-GPU 1M single cell notebook</h2>
<p>https://github.com/NVIDIA-Genomics-Research/rapids-single-cell-examples/blob/master/notebooks/1M_brain_gpu_analysis_multigpu.ipynb</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>