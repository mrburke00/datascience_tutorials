{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Accelerating Dataframe workflows\n",
        "### Devin Burke"
      ],
      "metadata": {
        "id": "jZaGnAnJ3n3H"
      },
      "id": "jZaGnAnJ3n3H"
    },
    {
      "cell_type": "code",
      "source": [
        "%env LC_ALL=C.UTF-8\n",
        "%env LANG=C.UTF-8"
      ],
      "metadata": {
        "id": "S01BsA8Y0g9H"
      },
      "id": "S01BsA8Y0g9H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "- [1. Primer: Sparsity, Density, and CSR](#primer-sparsity-density-and-csr)\n",
        "  - [Sparsity vs Density](#primer-sparsity-density-and-csr)\n",
        "  - [Dense vs Sparse Memory Intuition](#dense-vs.-sparse-in-memory)\n",
        "  - [What CSR Actually Looks Like](#what-csr-actually-looks-like)\n",
        "  - [Why CSR is Fast for Rows but Slower for Columns](#why-csr-is-fast-for-row-operations)\n",
        "  - [Common Pitfalls & Safer Alternatives](#common-pitfalls)\n",
        "\n",
        "- [2. Tools to Find Bottlenecks](#tools-to-find-bottlenecks---measuring-before-optimizing)\n",
        "  - [Inspecting Matrix Memory Footprint](#tools-to-find-bottlenecks---measuring-before-optimizing)\n",
        "  - [Timing QC Metrics](#tools-to-find-bottlenecks---measuring-before-optimizing)\n",
        "  - [Row-wise Scaling Example (Mito Fractions)](#tools-to-find-bottlenecks---measuring-before-optimizing)\n",
        "  - [The Dangers of Densification](#tools-to-find-bottlenecks---measuring-before-optimizing)\n",
        "  - [Takeaways from Bottleneck Profiling](#tools-to-find-bottlenecks---measuring-before-optimizing)\n",
        "\n",
        "- [3. Vectorization](#vectorization)\n",
        "  - [Primer: What is Vectorization](#vectorization)\n",
        "  - [Loop vs Vectorized Reductions](#vectorization)\n",
        "  - [Elementwise Ops on Sparse `.data`](#vectorization)\n",
        "  - [Broadcasting Safely for Normalization](#vectorization)\n",
        "  - [Fuse Passes to Reuse Intermediates](#vectorization)\n",
        "  - [Scaling Curve: Vectorized vs Loop](#vectorization)\n",
        "\n",
        "- [4. Sparsity-Savvy Ops](#sparsity-savvy-ops)\n",
        "  - [Counting Genes per Cell: getnnz vs Boolean](#sparsity-savvy-ops)\n",
        "  - [Column Sums: CSR vs CSC](#sparsit--savvy-ops)\n",
        "  - [Batching Column Ops](#sparsity-savvy-ops)\n",
        "  - [Anti-Pattern: Densification via NumPy](#sparsity-savvy-ops)\n",
        "\n",
        "- [5. CPU Micro-Optimizations](#cpu-micro-optimizations)\n",
        "  - [Dtype Discipline](#cpu-micro-optimizations)\n",
        "  - [Row Normalization: Diag vs In-Place](#cpu-micro-optimizations)\n",
        "  - [Numba for Custom Transforms](#cpu-micro-optimizations)\n",
        "  - [Block/Tile Processing](#cpu-micro-optimizations)\n",
        "\n",
        "- [6. When (and How) to Use the GPU](#when-and-how-to-use-the-gpu)\n",
        "  - [CuPy Basics](#when-and-how-to-use-the-gpu)\n",
        "  - [CuPy Sparse CSR](#when-and-how-to-use-the-gpu)\n",
        "  - [RAPIDS cuDF / cuML](#when-and-how-to-use-the-gpu)\n",
        "  - [RAPIDS Single-Cell End-to-End Pipeline](#when-and-how-to-use-the-gpu)\n",
        "  - [CPU vs GPU Timing Comparison](#when-and-how-to-use-the-gpu)\n",
        "\n",
        "- [7. Hybrid Pipelines](#hybrid-pipelines)\n",
        "  - [CPU for Preprocessing](#hybrid-pipelines)\n",
        "  - [GPU for Linear Algebra & Graphs](#hybrid-pipelines)\n",
        "  - [Split-Pipeline Pattern](#hybrid-pipelines)\n",
        "\n",
        "- [8. Emerging Runtimes & Tools](#emerging-tools)\n",
        "  - [GPU-Native Frameworks](#emerging-tools)\n",
        "  - [Scalable Array & Storage Backends](#emerging-tools)\n",
        "  - [CPU Accelerators](#emerging-tools)\n",
        "  - [Cluster & Distributed Execution](#emerging-tools)\n",
        "  - [Where Things Are Heading](#emerging-tools)\n"
      ],
      "metadata": {
        "id": "eaVbj02X_sn5"
      },
      "id": "eaVbj02X_sn5"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Uninstall only the conflicting RAPIDS bits (keep it minimal)\n",
        "!pip uninstall -y cugraph cugraph-cu12 cuml cuml-cu12 rmm rmm-cu12 pylibraft-cu12 pylibcugraph-cu12\n",
        "# 2) Reinstall a coherent RAPIDS 24.10 set that your Colab already uses\n",
        "!pip install \\\n",
        "  \"rmm-cu12==24.10.*\" \\\n",
        "  \"pylibraft-cu12==24.10.*\" \\\n",
        "  \"pylibcugraph-cu12==24.10.*\" \\\n",
        "  \"cuml-cu12==24.10.*\" \\\n",
        "  \"cugraph-cu12==24.10.*\" \\\n",
        "  --extra-index-url=https://pypi.nvidia.com -U --quiet\n",
        "# 3) Arrow must be <20 for these RAPIDS wheels\n",
        "!pip install \"pyarrow<20\" --upgrade --force-reinstall --quiet\n",
        "# 4) Print what we actually have now (versions)\n",
        "import sys, subprocess\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "for pkg in [\n",
        "    \"cudf-cu12\",\"cuml-cu12\",\"cugraph-cu12\",\n",
        "    \"rmm-cu12\",\"pylibraft-cu12\",\"pylibcugraph-cu12\",\n",
        "    \"pyarrow\",\"cupy-cuda12x\",\"cuda-python\"\n",
        "]:\n",
        "    try:\n",
        "        out = subprocess.check_output([sys.executable,\"-m\",\"pip\",\"show\",pkg], text=True)\n",
        "        ver = next((l.split(\":\",1)[1].strip() for l in out.splitlines() if l.startswith(\"Version:\")), \"unknown\")\n",
        "        print(f\"{pkg}: {ver}\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"{pkg}: not installed\")\n"
      ],
      "metadata": {
        "id": "UAXNK3Dd0jpy"
      },
      "id": "UAXNK3Dd0jpy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import rmm, cudf, cuml, cugraph\n",
        "    import pyarrow as pa\n",
        "    print(\"Imports OK:\",\n",
        "          getattr(cudf, \"__version__\", \"?\"),\n",
        "          getattr(cuml, \"__version__\", \"?\"),\n",
        "          getattr(cugraph, \"__version__\", \"?\"),\n",
        "          pa.__version__)\n",
        "except Exception as e:\n",
        "    import traceback, sys\n",
        "    print(\"Import failed:\")\n",
        "    traceback.print_exc(limit=2, file=sys.stdout)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uTHinLX0os2",
        "outputId": "8de5e1c0-89d8-4597-8556-bc7f555b99ee",
        "collapsed": true
      },
      "id": "8uTHinLX0os2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports OK: 24.10.01 24.10.00 24.10.00 19.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"rapids-singlecell==0.13.1\" \"scanpy>=1.10,<1.12\" \"anndata>=0.10,<0.12\" \"igraph\" \"leidenalg\" \"pyarrow<20\" --upgrade-strategy only-if-needed -q"
      ],
      "metadata": {
        "id": "Pc31d8xHnjzv"
      },
      "id": "Pc31d8xHnjzv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scanpy as sc, anndata, pandas as pd, numpy as np, scipy as sp\n",
        "import rmm, cudf, cuml, cugraph, rapids_singlecell as rsc\n",
        "import cupy as cp\n",
        "\n",
        "print(\"RAPIDS:\", getattr(cudf, \"__version__\", \"?\"), getattr(cuml, \"__version__\", \"?\"), getattr(cugraph, \"__version__\", \"?\"))\n",
        "print(\"rsc:\", rsc.__version__)\n",
        "print(\"scanpy:\", sc.__version__)\n",
        "print(\"anndata:\", anndata.__version__)\n",
        "print(\"pandas/numpy/scipy:\", pd.__version__, np.__version__, sp.__version__)\n",
        "print(\"cupy:\", cp.__version__)\n",
        "\n",
        "import scanpy as sc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import time, warnings\n",
        "import rapids_singlecell as rsc\n"
      ],
      "metadata": {
        "id": "57QqqsM0tJ4D"
      },
      "id": "57QqqsM0tJ4D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cf.10xgenomics.com/samples/cell-exp/9.0.0/320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16/320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16_count_sample_filtered_feature_bc_matrix.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umwqXP8zcsy4",
        "outputId": "bc288635-87c2-4a42-ca6a-aa5db4303cf9",
        "collapsed": true
      },
      "id": "umwqXP8zcsy4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-20 19:56:49--  https://cf.10xgenomics.com/samples/cell-exp/9.0.0/320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16/320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16_count_sample_filtered_feature_bc_matrix.h5\n",
            "Resolving cf.10xgenomics.com (cf.10xgenomics.com)... 104.18.1.173, 104.18.0.173, 2606:4700::6812:1ad, ...\n",
            "Connecting to cf.10xgenomics.com (cf.10xgenomics.com)|104.18.1.173|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1187383220 (1.1G) [application/x-hdf5]\n",
            "Saving to: ‘320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16_count_sample_filtered_feature_bc_matrix.h5’\n",
            "\n",
            "320k_Human_PBMCs_Su 100%[===================>]   1.11G  20.2MB/s    in 49s     \n",
            "\n",
            "2025-08-20 19:57:39 (23.2 MB/s) - ‘320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16_count_sample_filtered_feature_bc_matrix.h5’ saved [1187383220/1187383220]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Primer: Sparsity, Density, and CSR\n",
        "\n",
        "\n",
        "## Sparsity vs. Density\n",
        "\n",
        "Density = fraction of non-zeros\n",
        "\n",
        "Sparsity = fraction of zeros\n",
        "\n",
        "\n",
        "```\n",
        "density = nnz / (n_rows × n_cols)\n",
        "sparsity = 1 – density\n",
        "```\n",
        "Where:\n",
        "* Dense algorithms (e.g. BLAS, numpy) treat every entry equally, doing work even on zeros.\n",
        "\n",
        "* Sparse algorithms (e.g. CSR, CSC) skip over zeros and only process the non-zeros.\n",
        "\n",
        "When a dataset is very sparse, this saves both memory and compute.\n",
        "But as density rises (20–30% or more), the cost of bookkeeping (extra index arrays) can outweigh the savings, and dense storage or dense kernels may actually win.\n",
        "\n",
        "## Sparsity vs. Density in memory\n",
        "\n",
        "###Dense storage\n",
        "\n",
        "If you store every entry as a float32 (4 bytes), the cost is:\n",
        "\n",
        "`bytes_dense = n_rows × n_cols × 4`\n",
        "\n",
        "No matter how many zeros, you pay for them all.\n",
        "CSR storage\n",
        "\n",
        "**Compressed Sparse Row (CSR**) avoids storing zeros. Instead, it uses three arrays:\n",
        "\n",
        "1. **data**\n",
        "\n",
        "  * Length = `nnz`\n",
        "\n",
        "  * Stores the actual non-zero values (floats, 4 bytes each)\n",
        "\n",
        "2. **indices**\n",
        "\n",
        "  * Length = `nnz`\n",
        "\n",
        "  * Stores the column index for each non-zero (ints, 4 bytes each)\n",
        "\n",
        "3. **indptr**\n",
        "\n",
        "  * Length = `n_rows + 1`\n",
        "\n",
        "  * A “row pointer” array: tells you where each row starts and ends inside `data/indices`.\n",
        "\n",
        "  * Think of it as the “table of contents” for rows.\n",
        "\n",
        "  * Each entry is also an int (4 bytes).\n",
        "\n",
        "  ```\n",
        "  bytes_CSR = 4 × nnz (data)\n",
        "          + 4 × nnz (indices)\n",
        "          + 4 × (n_rows + 1) (indptr)\n",
        "  ```\n",
        "\n",
        "### 320k PBMC dataset\n",
        "\n",
        "Matrix: 289,331 × 18,129 with nnz ≈ 935,809,081\n",
        "\n",
        "Dense: `289,331 × 18,129 × 4 ≈ 21.0 GB`\n",
        "CSR:\n",
        "```\n",
        "(4 × 935,809,081) [data]\n",
        "+ (4 × 935,809,081) [indices]\n",
        "+ (4 × 289,332)    [indptr]\n",
        "≈ 7.49 GB\n",
        "```\n",
        "\n",
        "### Rule of Thumb\n",
        "\n",
        "CSR is worth it when sparsity is high (lots of zeros).\n",
        "\n",
        "As density increases, the overhead of storing indices + row pointers eats into savings. Around 20–30% density, dense storage can be competitive or even faster.\n",
        "\n",
        "### What CSR Actually Looks Like\n",
        "```\n",
        "Row 0:  .   3   .   .   .   1\n",
        "Row 1:  2   .   .   .   .   .\n",
        "Row 2:  .   .   .   7   .   .\n",
        "Row 3:  .   4   5   .   .   .\n",
        "```\n",
        "Coordinates of non-zeros\n",
        "```\n",
        "(0,1,3), (0,5,1), (1,0,2), (2,3,7), (3,1,4), (3,2,5)\n",
        "```\n",
        "CSR Arrays\n",
        "```\n",
        "data    = [3, 1, 2, 7, 4, 5]       (the values, row by row)\n",
        "indices = [1, 5, 0, 3, 1, 2]       (the column index of each value)\n",
        "indptr  = [0, 2, 3, 4, 6]          (row boundaries in data/indices)\n",
        "```\n",
        "### Why CSR is Fast for Row Operations\n",
        "Many common tasks in single-cell analysis are per-row (per-cell):\n",
        "\n",
        "* total UMI counts per cell\n",
        "\n",
        "* mitochondrial fractions\n",
        "\n",
        "* number of detected genes per cell\n",
        "\n",
        "In CSR, each row’s values are stored contiguously in data, with start/end marked by indptr.\n",
        "That means row operations are just `O(nnz)`, with good cache locality.\n",
        "\n",
        "### Why CSR is Slower for Column Operations\n",
        "\n",
        "Column entries are scattered across rows, not contiguous.\n",
        "So column sums require “hunting” through many rows, which is slower.\n",
        "If you care about per-column operations, you often convert to CSC format.\n",
        "\n",
        "### CSC: The Mirror Image\n",
        "Compressed Sparse Column (CSC) flips the idea:\n",
        "\n",
        "* data/indices are grouped by column.\n",
        "\n",
        "* indptr marks column boundaries.\n",
        "\n",
        "CSC is ideal for per-column (per-gene) work.\n",
        "That’s why R’s Matrix package uses CSC (dgCMatrix) by default.\n",
        "In CSC:\n",
        "* per-gene ops are “free”\n",
        "\n",
        "* per-cell ops require either transposing or special functions\n",
        "\n",
        "## Common Pitfalls\n",
        "Sparse matrices save huge amounts of memory, but only if you stay within the sparse world. Many “convenient” operations in numpy/pandas land are secretly dense, and if you aren’t careful, you can blow up memory without realizing it.\n",
        "\n",
        "1. Boolean masks and densification\n",
        "\n",
        "Pretend we are counting detected genes with something like\n",
        "```\n",
        "genes_per_cell = (X > 0).sum(axis=1).A1\n",
        "```\n",
        "Here’s what happens\n",
        "\n",
        "  1. `(X > 0)` creates a new sparse matrix of the same `nnz `pattern, but with dtype `bool`.\n",
        "\n",
        "    * This doubles your memory use temporarily (since both `X` and (X > 0)` exist).\n",
        "\n",
        "  2. `.sum(axis=1)` computes row sums, but the result is still a matrix object.\n",
        "\n",
        "  3. `.A1` (or `.toarray()`) forces dense materialization.\n",
        "\n",
        "    * At this point, you’ve asked Python to allocate `n_rows × n_cols` floats in memory.\n",
        "\n",
        "    * For large single-cell datasets, this can be tens of gigabytes, even if the original sparse matrix fit comfortably in RAM.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2. Implicit Conversions to Dense\n",
        "Many of the default function calls in popular packages (numpy, sklearn, pandas) don't support sparse arrays. If you pass a sparse matrix to them, they will silently convert to dense behind the senses. Meaning:\n",
        "\n",
        "* a 21 GB sparse matrix might suddenly try to allocate 100GB+\n",
        "\n",
        "Examples:\n",
        "\n",
        "* `.mean(axis=0)` on a SciPy sparse matrix → densifies\n",
        "* `Using `np.log1p(X)` directly on sparse → densifies.\n",
        "* Converting sparse → pandas DataFrame (`pd.DataFrame(X)`) → densifies\n",
        "\n",
        "Safer alternatives:\n",
        "\n",
        "* Use sparse-aware functions: scipy.sparse has its own `.sum`, `.getnnz`, etc.\n",
        "\n",
        "* For elementwise transforms, use specialized functions like `sparsefuncs.log1p` from `sklearn.utils.extmath` or libraries like `scanpy/anndata` that wrap these safely.\n",
        "---\n",
        "\n",
        "\n",
        "3. Copy-on-write behavior\n",
        "Sparse matrices don’t always behave like numpy arrays with views. Many slicing operations return new copies of arrays (data, indices, indptr). If you repeatedly slice in a loop, you may be silently creating many copies of large structures.\n",
        "---\n",
        "\n",
        "4. Awareness of dtype\n",
        "\n",
        "Your values (data) are often float32 (4 bytes).\n",
        "\n",
        "But if you accidentally convert to float64, memory doubles instantly.\n",
        "\n",
        "Similarly, some sparse constructors default to int64 indices instead of int32, doubling index storage.\n",
        "\n",
        "**Packages like SciPy's functions will often solve all of these issues for you, but it's still useful to understand when you wander off path**"
      ],
      "metadata": {
        "id": "c5b5SRuJH5M_"
      },
      "id": "c5b5SRuJH5M_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Tools to find bottlenecks - Measuring before optimizing\n",
        "\n",
        "Our goal in this section is not to optimize, rather to measure and understand.\n",
        "*   What the matrix looks like in memory\n",
        "*   How common tasks scale\n",
        "*   What densification does to memory and runtime\n",
        "\n",
        "Once we can name bottlenecks with evidence, targeted optimizations are much more obvious.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xn7v9zNO_7rx"
      },
      "id": "xn7v9zNO_7rx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Inspect the matrix in memory (shape, density, bytes)\n",
        "\n",
        "how big is the matrix, how sparse is it, and what would it cost if someone accidentally made it dense?\n",
        "\n",
        "density tells you whether sparse is still paying off; the dense vs CSR bytes show your safety margin."
      ],
      "metadata": {
        "id": "6aAVb2dpXzMc"
      },
      "id": "6aAVb2dpXzMc"
    },
    {
      "cell_type": "code",
      "source": [
        "adata = sc.read_10x_h5(\"/content/320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16_count_sample_filtered_feature_bc_matrix.h5\")\n",
        "adata.var_names_make_unique()"
      ],
      "metadata": {
        "id": "h2Ov5iO6EIRX"
      },
      "id": "h2Ov5iO6EIRX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ——— Download (only once) ———\n",
        "# !wget -O bone_marrow.h5 \\\n",
        "#   https://cf.10xgenomics.com/samples/cell-vdj/6.1.0/10k_BMMNC_5pv2_nextgem_10k_BMMNC_5pv2_nextgem/10k_BMMNC_5pv2_nextgem_10k_BMMNC_5pv2_nextgem_count_sample_feature_bc_matrix.h5\n",
        "\n",
        "import scanpy as sc\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "import time, statistics as stats, psutil, os, gc\n",
        "\n",
        "# If not already loaded:\n",
        "#adata = sc.read_10x_h5(\"/content/320k_Human_PBMCs_Sub_Pool_16-plex_GEM-X_FLEX_PBMC_BC1-16_count_sample_filtered_feature_bc_matrix.h5\")\n",
        "#adata.var_names_make_unique()\n",
        "\n",
        "\n",
        "def rss_mb():\n",
        "    return psutil.Process(os.getpid()).memory_info().rss / 1e6\n",
        "\n",
        "class Stopwatch:\n",
        "    def __init__(self, label): self.label = label\n",
        "    def __enter__(self):\n",
        "        gc.collect()\n",
        "        self._rss0 = rss_mb()\n",
        "        self._t0 = time.perf_counter()\n",
        "    def __exit__(self, *exc):\n",
        "        dt = time.perf_counter() - self._t0\n",
        "        drss = rss_mb() - self._rss0\n",
        "        print(f\"{self.label}: {dt:.3f}s | RSS Δ {drss:+.1f} MB (now {rss_mb():.1f} MB)\")\n",
        "\n",
        "def bench(fn, *args, repeats=3, warmup=1, **kwargs):\n",
        "    for _ in range(warmup): fn(*args, **kwargs)\n",
        "    times = []\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        fn(*args, **kwargs)\n",
        "        times.append(time.perf_counter() - t0)\n",
        "    return dict(median_s=stats.median(times), mean_s=float(np.mean(times)),\n",
        "                min_s=min(times), max_s=max(times), n=repeats)\n",
        "\n",
        "def matrix_stats(X, name=\"X\"):\n",
        "    if sparse.issparse(X):\n",
        "        m, n = X.shape\n",
        "        nnz = X.nnz\n",
        "        density = nnz/(m*n)\n",
        "        dense_bytes = m*n*4  # float32\n",
        "        csr_bytes = nnz*(4+4) + (m+1)*4  # data(float32)+indices(int32)+indptr(int32)\n",
        "        print(f\"{name}: sparse {X.format} {X.shape}, nnz={nnz:,} (density={density:.4%}), dtype={X.dtype}\")\n",
        "        print(f\"  ~Dense(float32) if materialized: {dense_bytes/1e9:.2f} GB\")\n",
        "        print(f\"  ~CSR payload (data+idx+indptr): {csr_bytes/1e6:.1f} MB  (excl. Python overhead)\")\n",
        "    else:\n",
        "        print(f\"{name}: dense {X.shape}, dtype={getattr(X,'dtype',None)}, nbytes={getattr(X,'nbytes',0)/1e9:.2f} GB\")\n",
        "\n",
        "from scipy import sparse as sp\n",
        "def ensure_csr_float32(X):\n",
        "    if sp.isspmatrix_csr(X):\n",
        "        return X.astype(np.float32, copy=False)\n",
        "    return X.tocsr().astype(np.float32, copy=False)\n",
        "\n",
        "print(f\"AnnData: n_obs={adata.n_obs:,}, n_var={adata.n_vars:,}\")\n",
        "adata.X = ensure_csr_float32(adata.X)\n",
        "matrix_stats(adata.X, \"adata.X (CSR/float32)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93gw3OImv3IX",
        "outputId": "7358540c-8789-44f6-fa9b-2fe67eeca37a"
      },
      "id": "93gw3OImv3IX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnnData: n_obs=289,331, n_var=18,129\n",
            "adata.X (CSR/float32): sparse csr (289331, 18129), nnz=935,809,081 (density=17.8410%), dtype=float32\n",
            "  ~Dense(float32) if materialized: 20.98 GB\n",
            "  ~CSR payload (data+idx+indptr): 7487.6 MB  (excl. Python overhead)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What the matrix looks like\n",
        "Our dataset has 289,331 cells × 18,129 genes, with ~935.8M non-zero entries.\n",
        "\n",
        "* Density ≈ 17.84% → still sparse enough that CSR is beneficial.\n",
        "\n",
        "* Dense float32 footprint ≈ 21.0 GB.\n",
        "\n",
        "* CSR payload ≈ 7.5 GB (data + indices + indptr, excluding Python overhead).\n",
        "\n",
        "At this density, CSR saves ~2.8× memory, though we’re close to the “edge zone” where index overhead becomes more noticeable."
      ],
      "metadata": {
        "id": "A7IgsQ78W_ec"
      },
      "id": "A7IgsQ78W_ec"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Time Scanpy QC to spot fixed overheads vs row-wise work\n",
        "Now we measure a realistic multi-pass routine. Expect some fixed overhead and several row-wise passes; the full run vs a subset gives a sense of scaling and where time is spent."
      ],
      "metadata": {
        "id": "6hYSulUGX-Os"
      },
      "id": "6hYSulUGX-Os"
    },
    {
      "cell_type": "code",
      "source": [
        "def qc_metrics(adata_):\n",
        "    sc.pp.calculate_qc_metrics(adata_, percent_top=None, log1p=False, inplace=True)\n",
        "\n",
        "with Stopwatch(\"QC metrics (full copy)\"):\n",
        "    qc_metrics(adata.copy())\n",
        "\n",
        "n_demo = min(80_000, adata.n_obs)\n",
        "with Stopwatch(f\"QC metrics (subset {n_demo:,})\"):\n",
        "    qc_metrics(adata[:n_demo,:].copy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzYWTSs4A3cx",
        "outputId": "c3138548-8dd3-4e76-e084-24fd9e5f06b8"
      },
      "id": "xzYWTSs4A3cx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QC metrics (full copy): 9.783s | RSS Δ +0.1 MB (now 10625.0 MB)\n",
            "QC metrics (subset 80,000): 5.394s | RSS Δ +0.3 MB (now 10625.3 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QC Metrics: multi-pass overhead\n",
        "\n",
        "Running Scanpy’s calculate_qc_metrics:\n",
        "\n",
        "* Full copy (289k cells): ~9.78s\n",
        "\n",
        "* Subset (80k cells): ~5.39s\n",
        "\n",
        "This is not perfectly linear with n, because the function does several passes and bookkeeping tasks. Still, the cost is dominated by row-wise work, which matches what CSR is good at."
      ],
      "metadata": {
        "id": "I6-_RkjZXSQt"
      },
      "id": "I6-_RkjZXSQt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Measure a pure row-wise kernel across sizes\n",
        "\n",
        "We benchmark mitochondrial fraction, a straightforward row reduction. If CSR fits the workload, you should see smooth, near-linear scaling with n."
      ],
      "metadata": {
        "id": "QgYDgVWlYFps"
      },
      "id": "QgYDgVWlYFps"
    },
    {
      "cell_type": "code",
      "source": [
        "mito_mask = np.fromiter((g.startswith(\"MT-\") for g in adata.var_names), dtype=bool)\n",
        "\n",
        "def mito_fraction_vec(X, mito_mask):\n",
        "    if sp.issparse(X):\n",
        "        num = X[:, mito_mask].sum(axis=1).A1\n",
        "        den = X.sum(axis=1).A1\n",
        "    else:\n",
        "        num = X[:, mito_mask].sum(axis=1)\n",
        "        den = X.sum(axis=1)\n",
        "    out = np.divide(num, den, out=np.zeros_like(num, dtype=np.float32), where=den>0)\n",
        "    return out\n",
        "\n",
        "sizes = [20_000, 40_000, 80_000, 160_000, min(320_000, adata.n_obs)]\n",
        "cpu_curve = []\n",
        "for n in sizes:\n",
        "    subX = adata.X[:n, :]\n",
        "    stats_n = bench(lambda: mito_fraction_vec(subX, mito_mask), repeats=3, warmup=1)\n",
        "    cpu_curve.append((n, stats_n[\"median_s\"]))\n",
        "    print(f\"mito_fraction_vec n={n:>6}: {stats_n}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3PeyYjPA5v2",
        "outputId": "ac939de1-5ed4-4d33-90d2-576e07f14b50"
      },
      "id": "R3PeyYjPA5v2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mito_fraction_vec n= 20000: {'median_s': 0.15986045399949944, 'mean_s': 0.16111667266644267, 'min_s': 0.15954597699965234, 'max_s': 0.1639435870001762, 'n': 3}\n",
            "mito_fraction_vec n= 40000: {'median_s': 0.3227266129997588, 'mean_s': 0.3246616669991151, 'min_s': 0.3221335949983768, 'max_s': 0.3291247929992096, 'n': 3}\n",
            "mito_fraction_vec n= 80000: {'median_s': 0.6394602919990575, 'mean_s': 0.6403345810000852, 'min_s': 0.6314820660008991, 'max_s': 0.6500613850002992, 'n': 3}\n",
            "mito_fraction_vec n=160000: {'median_s': 1.297852285000772, 'mean_s': 1.2947686590002074, 'min_s': 1.2806189630009612, 'max_s': 1.3058347289988887, 'n': 3}\n",
            "mito_fraction_vec n=289331: {'median_s': 2.322633675999896, 'mean_s': 2.32081454766679, 'min_s': 2.313078230999963, 'max_s': 2.326731736000511, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling: row-wise mito fraction\n",
        "We measured per-cell mitochondrial fraction (a row-wise reduction) across increasing numbers of cells:\n",
        "\n",
        "* 20k → 0.16s\n",
        "\n",
        "* 40k → 0.32s\n",
        "\n",
        "* 80k → 0.64s\n",
        "\n",
        "* 160k → 1.30s\n",
        "\n",
        "* 289k → 2.32s\n",
        "\n",
        "This is almost perfectly linear scaling. Throughput stays stable at ~120–130k cells/sec. That’s exactly what we’d expect: CSR stores each row contiguously, so row reductions are `O(nnz)` with good cache locality.\n",
        "\n",
        "Takeaway: CSR delivers efficient row-wise ops — the backbone of per-cell QC and summary metrics."
      ],
      "metadata": {
        "id": "F_jeggcPXmtg"
      },
      "id": "F_jeggcPXmtg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Dangers of densification\n",
        "\n",
        "Accidental densification is the most common way pipelines blow up. Here we deliberately convert a modest slice to dense and hold it to capture the real memory impact."
      ],
      "metadata": {
        "id": "OuAjMoAEYMPl"
      },
      "id": "OuAjMoAEYMPl"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse as sp\n",
        "\n",
        "def force_dense_and_hold(X):\n",
        "    if sp.issparse(X):\n",
        "        D = X.toarray()  # allocate dense (n_rows x n_cols)\n",
        "    else:\n",
        "        D = np.asarray(X, order=\"C\")\n",
        "    # Do a tiny op to ensure it's touched\n",
        "    out = D.sum(axis=1)\n",
        "    return D, out\n",
        "\n",
        "n_safe = min(25_000, adata.n_obs)\n",
        "subX = adata.X[:n_safe, :]\n",
        "\n",
        "print(\"Before:\", rss_mb(), \"MB\")\n",
        "with Stopwatch(\"force_dense_and_hold\"):\n",
        "    D_held, _out = force_dense_and_hold(subX)\n",
        "print(\"After holding dense:\", rss_mb(), \"MB\")\n",
        "\n",
        "# Keep D_held alive for a moment (optional: inspect top)\n",
        "# time.sleep(2)\n",
        "\n",
        "# Cleanup\n",
        "del D_held, _out; gc.collect()\n",
        "print(\"After cleanup:\", rss_mb(), \"MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pn6VwYfA7JU",
        "outputId": "c453fb9b-31f7-4d51-b150-ad8e3536ea66"
      },
      "id": "9Pn6VwYfA7JU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: 11275.620352 MB\n",
            "force_dense_and_hold: 2.270s | RSS Δ +1812.9 MB (now 12439.1 MB)\n",
            "After holding dense: 12439.097344 MB\n",
            "After cleanup: 10626.224128 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Densification: the real hazard\n",
        "\n",
        "We  tested the “what if I accidentally densify” scenario with a 25k × 18,129 slice:\n",
        "\n",
        "CSR payload ≈ 650 MB\n",
        "\n",
        "Dense float32 equivalent ≈ 1.81 GB\n",
        "\n",
        "Observed RAM jump: +1.81 GB while the dense copy was held\n",
        "\n",
        "This validates the caution: densifying isn’t a rounding error — even a modest slice can balloon memory by gigabytes. In larger pipelines, those dense temporaries often stick around (via chained expressions or DataFrame conversions), which can silently push you into OOM territory."
      ],
      "metadata": {
        "id": "u3I3opuLYUni"
      },
      "id": "u3I3opuLYUni"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Takeaways\n",
        "\n",
        "* The matrix is sparse enough for CSR to pay off; row-wise ops scale linearly and fast.\n",
        "\n",
        "* QC has multi-pass overhead but remains row-dominated, aligning with CSR strengths.\n",
        "\n",
        "* Densification is a gigabyte-scale event even for modest slices; avoid it unless it’s a small, deliberate block."
      ],
      "metadata": {
        "id": "nH9rXFjFYfcx"
      },
      "id": "nH9rXFjFYfcx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Vectorization\n",
        "\n",
        "## 3.1 Primer - What is vectorization and why it matters\n",
        "\n",
        "Non-vectorized (Python loop): You write an explicit for loop in Python and operate on one element at a time. Every iteration goes through the Python interpreter → slow overhead, poor cache usage.\n",
        "\n",
        "Vectorized (array operation): You express the work as a whole-array operation (sum, multiply, log, etc.). The heavy lifting happens inside optimized C/CUDA kernels, touching millions of elements in a tight, compiled loop.\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine we want to multiply two arrays `a` and `b`.\n",
        "\n",
        "**Scalar**\n",
        "```\n",
        "a = [1, 2, 3, 4]\n",
        "b = [5, 6, 7, 8]\n",
        "```\n",
        "```\n",
        "take a[0], b[0] → 1*5 = 5 → store\n",
        "take a[1], b[1] → 2*6 = 12 → store\n",
        "take a[2], b[2] → 3*7 = 21 → store\n",
        "take a[3], b[3] → 4*8 = 32 → store\n",
        "```\n",
        "```\n",
        " [1]   [2]   [3]   [4]\n",
        "   ×     ×     ×     ×      (each step separate)\n",
        " [5]   [6]   [7]   [8]\n",
        "\n",
        " → [5] [12] [21] [32]\n",
        "```\n",
        "Each multiplication is its own  job — the “vector” is not visible as a whole, just a sequence of scalars.\n",
        "\n",
        "**Vectorized (treat arrays as one object)**\n",
        "\n",
        "Now, same math (dot product), but expressed as a whole operation:\n",
        "\n",
        "```\n",
        "c = a · b\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "     [1   2   3   4]\n",
        "   ⨉ [5   6   7   8]\n",
        "   -------------------\n",
        "     [5  12  21  32]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "## Why it’s faster\n",
        "\n",
        "* Interpreter overhead: Python loops pay a cost per iteration; vectorized ops pay one cost per whole array.\n",
        "\n",
        "* Cache and memory locality: compiled kernels access memory contiguously and predictably, while Python loops jump around object references.\n",
        "\n",
        "* Parallelism under the hood: libraries can use SIMD (vector CPU instructions), multithreading, or GPU CUDA kernels automatically.\n",
        "\n",
        "* Fewer passes: combining operations reduces memory bandwidth usage.\n"
      ],
      "metadata": {
        "id": "S_b4IAGcarmo"
      },
      "id": "S_b4IAGcarmo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 sparse CSR: per-cell totals, loop vs vectorized\n",
        "\n",
        "We compare a Python row loop (`getrow().sum()`) to the vectorized sparse reduction"
      ],
      "metadata": {
        "id": "NLjuhAh3dFHB"
      },
      "id": "NLjuhAh3dFHB"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse as sp\n",
        "import numpy as np, time, statistics as stats\n",
        "\n",
        "X = adata.X  # CSR float32 from Part 2\n",
        "\n",
        "def loop_counts_per_cell(X):\n",
        "    m = X.shape[0]\n",
        "    out = np.empty(m, dtype=np.float32)\n",
        "    for i in range(m):\n",
        "        out[i] = X.getrow(i).sum()\n",
        "    return out\n",
        "\n",
        "def vect_counts_per_cell(X):\n",
        "    return X.sum(axis=1).A1\n",
        "\n",
        "def bench(fn, *args, repeats=3, warmup=1, **kwargs):\n",
        "    for _ in range(warmup): fn(*args, **kwargs)\n",
        "    ts = []\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        fn(*args, **kwargs)\n",
        "        ts.append(time.perf_counter() - t0)\n",
        "    return dict(median_s=stats.median(ts), mean_s=float(np.mean(ts)),\n",
        "                min_s=min(ts), max_s=max(ts), n=repeats)\n",
        "\n",
        "n_demo = min(40_000, X.shape[0])\n",
        "print(\"loop_counts_per_cell:\", bench(loop_counts_per_cell, X[:n_demo,:]))\n",
        "print(\"vect_counts_per_cell:\", bench(vect_counts_per_cell, X[:n_demo,:]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4LuAVVYEq__",
        "outputId": "c027bd6a-edbf-4a3c-bd03-370114f7a285"
      },
      "id": "L4LuAVVYEq__",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loop_counts_per_cell: {'median_s': 3.9722902630001045, 'mean_s': 3.970004134333673, 'min_s': 3.962057611999626, 'max_s': 3.975664528001289, 'n': 3}\n",
            "vect_counts_per_cell: {'median_s': 0.05190052400030254, 'mean_s': 0.05413572200025859, 'min_s': 0.04996352800117165, 'max_s': 0.06054311399930157, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop vs Vectorizd on CSR\n",
        "\n",
        "Per-cell totals (40k rows)\n",
        "\n",
        "* Loop: 3.972 s\n",
        "\n",
        "* Vectorized: 0.0519 s\n",
        "\n",
        "* ≈ 76× faster\n",
        "\n",
        "Why: the vectorized reduction treats the whole matrix as one object and walks contiguous row segments; the loop does 40k tiny jobs and pays per-iteration overhead."
      ],
      "metadata": {
        "id": "93halL00fyQ0"
      },
      "id": "93halL00fyQ0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Elementwise transform on sparse: touch .data (vectorized)\n",
        "\n",
        "We scale-and-shift the non-zeros in one shot by operating on the CSR data buffer. Then we compare to a scalar loop over that buffer."
      ],
      "metadata": {
        "id": "56QYF__XdP-B"
      },
      "id": "56QYF__XdP-B"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import sparse as sp\n",
        "import time, statistics as stats\n",
        "\n",
        "def scale_shift_sparse(X, a=1.2, b=0.7):\n",
        "    Y = X.copy()\n",
        "    Y.data = Y.data * a + b\n",
        "    return Y\n",
        "\n",
        "def scale_shift_loop(X, a=1.2, b=0.7):\n",
        "    Y = X.copy()\n",
        "    d = Y.data\n",
        "    for i in range(d.size):\n",
        "        d[i] = d[i] * a + b\n",
        "    return Y\n",
        "\n",
        "def bench(fn, *args, repeats=3, warmup=1, **kwargs):\n",
        "    for _ in range(warmup): fn(*args, **kwargs)\n",
        "    ts = []\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        fn(*args, **kwargs)\n",
        "        ts.append(time.perf_counter() - t0)\n",
        "    return dict(median_s=stats.median(ts), mean_s=float(np.mean(ts)),\n",
        "                min_s=min(ts), max_s=max(ts), n=repeats)\n",
        "\n",
        "subX = X[:1000,:]\n",
        "print(\"scale_shift_loop:\", bench(scale_shift_loop, subX))\n",
        "print(\"scale_shift_sparse:\", bench(scale_shift_sparse, subX))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtzfcf11dNJR",
        "outputId": "c3b8c929-aed1-407a-a07a-38a89b9ee269"
      },
      "id": "dtzfcf11dNJR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scale_shift_loop: {'median_s': 5.757489456000258, 'mean_s': 5.747247039000285, 'min_s': 5.724614021000889, 'max_s': 5.75963763999971, 'n': 3}\n",
            "scale_shift_sparse: {'median_s': 0.0071631329992669635, 'mean_s': 0.007237320999289902, 'min_s': 0.007122330998754478, 'max_s': 0.007426498999848263, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elementwise on sparse: operate on .data\n",
        "\n",
        "Scale-and-shift nonzeros (50k rows slice)\n",
        "\n",
        "* Loop over nonzeros: 5.757 s\n",
        "\n",
        "* Vectorized on X.data: 0.00716 s\n",
        "\n",
        "* ≈ 800× faster\n",
        "\n",
        "Lesson: when you can express the operation as “apply function to all nonzeros,” touch CSR.data directly. You skip zeros entirely and do exactly one pass."
      ],
      "metadata": {
        "id": "9SgyhSASgHta"
      },
      "id": "9SgyhSASgHta"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Broadcast safely: per-cell normalization without densifying\n",
        "\n",
        "Row-normalize using a diagonal multiply (sparse-safe). This is the canonical way to avoid broadcasting traps that create dense temporaries.\n",
        "\n",
        "Sidenote: Broadcasting is NumPy/Scipy's way of automatically expanding arrays of different shapes so you can combine them without looping.\n",
        "\n",
        "With dense matrices, Numpy broadcasting is cheap but with Scipy sparse you can often experience unintended densification because sparse matrices don't fully support Numpy's broadcasting rules. So below we are showing one way to ensure Scipy doesn't silently densify."
      ],
      "metadata": {
        "id": "qgOsRxpYdWQM"
      },
      "id": "qgOsRxpYdWQM"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import sparse as sp\n",
        "import time, statistics as stats\n",
        "\n",
        "def row_normalize_csr(X):\n",
        "    counts = X.sum(axis=1).A1\n",
        "    scale = np.divide(1.0, counts, out=np.zeros_like(counts, dtype=np.float32), where=counts>0)\n",
        "    D = sp.diags(scale, offsets=0, dtype=np.float32, format=\"csr\")\n",
        "    return D @ X\n",
        "\n",
        "def bench(fn, *args, repeats=3, warmup=1, **kwargs):\n",
        "    for _ in range(warmup): fn(*args, **kwargs)\n",
        "    ts = []\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        fn(*args, **kwargs)\n",
        "        ts.append(time.perf_counter() - t0)\n",
        "    return dict(median_s=stats.median(ts), mean_s=float(np.mean(ts)),\n",
        "                min_s=min(ts), max_s=max(ts), n=repeats)\n",
        "\n",
        "print(\"row_normalize_csr:\", bench(row_normalize_csr, X[:80_000,:]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWJnQKrAdVbp",
        "outputId": "8a6de01f-4f7e-460f-95fb-a3a10ebd0728"
      },
      "id": "hWJnQKrAdVbp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "row_normalize_csr: {'median_s': 2.2612861880006676, 'mean_s': 2.2661139726666684, 'min_s': 2.260578056000668, 'max_s': 2.27647767399867, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Fuse passes: reuse intermediates (mito fraction example)\n",
        "\n",
        "We compute mitochondrial fraction two ways: naïve (two sums) vs fused (reuse precomputed row totals)."
      ],
      "metadata": {
        "id": "ipXQUOUbdj2S"
      },
      "id": "ipXQUOUbdj2S"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import sparse as sp\n",
        "import time, statistics as stats\n",
        "\n",
        "mito_mask = np.fromiter((g.startswith(\"MT-\") for g in adata.var_names), dtype=bool)\n",
        "\n",
        "def mito_fraction_naive(X, mito_mask):\n",
        "    num = X[:, mito_mask].sum(axis=1).A1\n",
        "    den = X.sum(axis=1).A1\n",
        "    return np.divide(num, den, out=np.zeros_like(num, dtype=np.float32), where=den>0)\n",
        "\n",
        "def mito_fraction_fused(X, mito_mask, row_counts=None):\n",
        "    if row_counts is None:\n",
        "        row_counts = X.sum(axis=1).A1\n",
        "    num = X[:, mito_mask].sum(axis=1).A1\n",
        "    return np.divide(num, row_counts, out=np.zeros_like(num, dtype=np.float32), where=row_counts>0)\n",
        "\n",
        "def bench(fn, *args, repeats=3, warmup=1, **kwargs):\n",
        "    for _ in range(warmup): fn(*args, **kwargs)\n",
        "    ts = []\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        fn(*args, **kwargs)\n",
        "        ts.append(time.perf_counter() - t0)\n",
        "    return dict(median_s=stats.median(ts), mean_s=float(np.mean(ts)),\n",
        "                min_s=min(ts), max_s=max(ts), n=repeats)\n",
        "\n",
        "subX = X[:160_000,:]\n",
        "row_counts = subX.sum(axis=1).A1\n",
        "print(\"naive:\", bench(mito_fraction_naive, subX, mito_mask))\n",
        "print(\"fused (cached counts):\", bench(mito_fraction_fused, subX, mito_mask, row_counts=row_counts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCFzEDlBdgnj",
        "outputId": "4b3693de-982b-4dc7-fe71-8f448702b8be"
      },
      "id": "wCFzEDlBdgnj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "naive: {'median_s': 1.2879675139993196, 'mean_s': 1.2942474799995882, 'min_s': 1.276605152999764, 'max_s': 1.3181697729996813, 'n': 3}\n",
            "fused (cached counts): {'median_s': 1.0758264319993032, 'mean_s': 1.0748436906663603, 'min_s': 1.0687177370000427, 'max_s': 1.0799869029997353, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Broadcast safely: row normalization without traps\n",
        "\n",
        "Row-normalize (80k rows slice)\n",
        "\n",
        "* Diagonal-multiply approach: 2.261 s\n",
        "\n",
        "This is already vectorized and sparse-safe (no densification), but the cost is dominated by building the diagonal + a sparse matmul. Keep this result in mind—we’ll beat it in Part 5 with a row-scaling kernel that multiplies each row’s data segment in place. Same math, fewer moving parts.\n",
        "\n",
        "Fuse passes when you can\n",
        "\n",
        "Mito fraction (160k rows)\n",
        "\n",
        "* Naïve (two sums): 1.288 s\n",
        "\n",
        "* Fused (reuse row counts): 1.076 s\n",
        "\n",
        "* ~20% faster\n",
        "\n",
        "Even vectorized code can burn bandwidth if it revisits the matrix. Cache shared intermediates (row totals, masks) to avoid extra passes."
      ],
      "metadata": {
        "id": "47-ihfm8gWkU"
      },
      "id": "47-ihfm8gWkU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Vectorized scaling curve (with a small loop baseline)\n",
        "\n",
        "We replicate the Part-2 style scaling for the vectorized mito calculation and overlay a tiny loop baseline so the structural benefit is visible."
      ],
      "metadata": {
        "id": "ZkJ0rmX6dv7K"
      },
      "id": "ZkJ0rmX6dv7K"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, time, statistics as stats\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse as sp\n",
        "\n",
        "def bench_vect_mito(X, mito_mask, n_list):\n",
        "    out = []\n",
        "    for n in n_list:\n",
        "        sub = X[:n,:]\n",
        "        t = bench(lambda: (sub[:, mito_mask].sum(axis=1).A1, sub.sum(axis=1).A1), repeats=3)['median_s']\n",
        "        out.append((n, t))\n",
        "        print(f\"n={n:>6} vectorized mito: {t:.3f}s\")\n",
        "    return out\n",
        "\n",
        "def bench_loop_counts(X, n_list):\n",
        "    out = []\n",
        "    for n in n_list:\n",
        "        sub = X[:n,:]\n",
        "        t = bench(lambda: [sub.getrow(i).sum() for i in range(n)], repeats=1, warmup=0)['median_s']\n",
        "        out.append((n, t))\n",
        "        print(f\"n={n:>6} loop row sums:   {t:.3f}s\")\n",
        "    return out\n",
        "\n",
        "n_list_small = [5_000, 10_000, 20_000, 40_000]\n",
        "rows_vect = bench_vect_mito(X, mito_mask, n_list_small)\n",
        "rows_loop = bench_loop_counts(X, n_list_small)\n",
        "\n",
        "ns_v = np.array([r[0] for r in rows_vect]); tv = np.array([r[1] for r in rows_vect])\n",
        "ns_l = np.array([r[0] for r in rows_loop]); tl = np.array([r[1] for r in rows_loop])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.plot(ns_v, tv, marker=\"o\", label=\"vectorized mito (CSR)\")\n",
        "plt.plot(ns_l, tl, marker=\"o\", label=\"loop row sums (baseline)\")\n",
        "plt.xlabel(\"cells (n)\")\n",
        "plt.ylabel(\"seconds\")\n",
        "plt.title(\"Vectorized vs Loop scaling\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "KhI-OLKHdvkX",
        "outputId": "b61a5ee3-2ff0-46fe-dd8e-7ab68a705b34"
      },
      "id": "KhI-OLKHdvkX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n=  5000 vectorized mito: 0.040s\n",
            "n= 10000 vectorized mito: 0.082s\n",
            "n= 20000 vectorized mito: 0.164s\n",
            "n= 40000 vectorized mito: 0.320s\n",
            "n=  5000 loop row sums:   0.504s\n",
            "n= 10000 loop row sums:   0.991s\n",
            "n= 20000 loop row sums:   1.985s\n",
            "n= 40000 loop row sums:   3.944s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x320 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 315,
              "height": 318
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Sparsity-Savvy Ops\n",
        "\n",
        "Not all sparse operations are equal\n",
        "\n",
        "### 4.1 Row counts: boolean temp vs getnnz\n",
        "\n",
        "Counting nonzeros per row is a classic pitfall. Boolean masks look vectorized but create new sparse structures; getnnz is cheaper and avoids densification. Let’s measure both."
      ],
      "metadata": {
        "id": "I17m1wgJyCZK"
      },
      "id": "I17m1wgJyCZK"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse as sp\n",
        "import numpy as np, statistics as stats, time\n",
        "\n",
        "def bench(fn, *args, repeats=3, warmup=1, **kwargs):\n",
        "    for _ in range(warmup): fn(*args, **kwargs)\n",
        "    ts = []\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        fn(*args, **kwargs)\n",
        "        ts.append(time.perf_counter() - t0)\n",
        "    return dict(median_s=stats.median(ts), mean_s=float(np.mean(ts)),\n",
        "                min_s=min(ts), max_s=max(ts), n=repeats)\n",
        "\n",
        "def genes_per_cell_getnnz(X):\n",
        "    return X.getnnz(axis=1)\n",
        "\n",
        "def genes_per_cell_boolean(X):\n",
        "    return np.asarray((X > 0).sum(axis=1)).ravel()\n",
        "\n",
        "subX = adata.X[:40_000,:]\n",
        "print(\"getnnz:\", bench(genes_per_cell_getnnz, subX))\n",
        "print(\"boolean:\", bench(genes_per_cell_boolean, subX))\n"
      ],
      "metadata": {
        "id": "8Tv07231d1qr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ba5a97-eb21-462b-b5a2-70d0ebeb897e"
      },
      "id": "8Tv07231d1qr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getnnz: {'median_s': 1.3445009244605899e-05, 'mean_s': 1.557865956177314e-05, 'min_s': 1.2559990864247084e-05, 'max_s': 2.073097857646644e-05, 'n': 3}\n",
            "boolean: {'median_s': 0.7257259640027769, 'mean_s': 0.7260426096715188, 'min_s': 0.7230394780053757, 'max_s': 0.7293623870064039, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Counting genes per cell: `getnnz` vs boolean temp\n",
        "\n",
        "* `getnnz`: 0.000013 s\n",
        "\n",
        "* Boolean mask: 0.726 s\n",
        "\n",
        "That’s a ~50,000× speedup. Why?\n",
        "\n",
        "* `getnnz` directly counts nonzeros row by row using the CSR index structure.\n",
        "\n",
        "* `(X > 0)` builds a whole new sparse boolean matrix and then sums it.\n",
        "\n",
        "Takeaway: for detection counts, always prefer `getnnz`. It avoids memory waste and runs in microseconds.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 4.2 Column sums: CSR vs CSC\n",
        "\n",
        "CSR is row-friendly, CSC is column-friendly. We’ll compare column sums done directly in CSR vs after a one-time CSC conversion."
      ],
      "metadata": {
        "id": "K0Kp480byQzf"
      },
      "id": "K0Kp480byQzf"
    },
    {
      "cell_type": "code",
      "source": [
        "def col_sums_csr(X):\n",
        "    return np.asarray(X.sum(axis=0)).ravel()\n",
        "\n",
        "def col_sums_csc(X):\n",
        "    Xc = X.tocsc()\n",
        "    return np.asarray(Xc.sum(axis=0)).ravel()\n",
        "\n",
        "subX = adata.X[:40_000,:]\n",
        "print(\"col_sums_csr:\", bench(col_sums_csr, subX))\n",
        "print(\"col_sums_csc:\", bench(col_sums_csc, subX))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tffMVYRgxT-G",
        "outputId": "f25a7cb5-bd3b-4418-a764-3736c233682d"
      },
      "id": "tffMVYRgxT-G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "col_sums_csr: {'median_s': 0.14801824200549163, 'mean_s': 0.14785454766630815, 'min_s': 0.14687072299420834, 'max_s': 0.1486746779992245, 'n': 3}\n",
            "col_sums_csc: {'median_s': 1.8990398200112395, 'mean_s': 1.8993665896705352, 'min_s': 1.8879537030006759, 'max_s': 1.9111062459996901, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Column sums: CSR vs CSC\n",
        "\n",
        "* CSR: 0.148 s\n",
        "\n",
        "* CSC: 1.899 s\n",
        "\n",
        "This shows the classic tradeoff:\n",
        "\n",
        "CSR is row-optimized. Doing column work directly in CSR is cheaper than converting for one op.\n",
        "\n",
        "CSC pays the one-time conversion, but shines if you batch multiple column ops.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 4.3 Batch multiple column ops after one conversion\n",
        "\n",
        "The best pattern is: convert once, do many column operations, then (optionally) convert back. This avoids paying the conversion cost repeatedly."
      ],
      "metadata": {
        "id": "d__82GElyWX8"
      },
      "id": "d__82GElyWX8"
    },
    {
      "cell_type": "code",
      "source": [
        "def col_ops_batched(X):\n",
        "    Xc = X.tocsc()\n",
        "    s1 = np.asarray(Xc.sum(axis=0)).ravel()\n",
        "    s2 = Xc.getnnz(axis=0)\n",
        "    s3 = np.asarray(Xc.power(2).sum(axis=0)).ravel()\n",
        "    return s1, s2, s3\n",
        "\n",
        "subX = adata.X[:40_000,:]\n",
        "print(\"batched column ops:\", bench(col_ops_batched, subX))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH-bO_wxxWxq",
        "outputId": "e1f86a9c-ca71-4bf4-bbce-cab1240611f4"
      },
      "id": "dH-bO_wxxWxq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batched column ops: {'median_s': 2.405151458020555, 'mean_s': 2.4064764560122662, 'min_s': 2.404300422000233, 'max_s': 2.409977488016011, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Batched column ops (CSC)\n",
        "\n",
        "* Doing three column ops after one conversion: 2.405 s\n",
        "\n",
        "* That’s only slightly slower than the conversion itself, because the ops become “free” once in CSC.\n",
        "\n",
        "Pattern:\n",
        "\n",
        "* If you only need one column op → stay in CSR.\n",
        "\n",
        "* If you need many → convert once to CSC, batch them, and optionally convert back.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6K7Uc9V70ZVh"
      },
      "id": "6K7Uc9V70ZVh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Anti-pattern: densification through NumPy APIs\n",
        "\n",
        "Some NumPy functions don’t understand sparse matrices. Let’s show how np.mean on a SciPy sparse matrix forces a dense conversion."
      ],
      "metadata": {
        "id": "nnPtpqKXycZE"
      },
      "id": "nnPtpqKXycZE"
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_mean(X):\n",
        "    return np.mean(X)\n",
        "\n",
        "subX = adata.X[:5000,:]  # keep safe size\n",
        "print(\"np.mean on sparse (forces dense!):\")\n",
        "with Stopwatch(\"np.mean(sparse)\"):\n",
        "    _ = dense_mean(subX)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FwF1OpXxYCB",
        "outputId": "a447239b-073b-4343-fad7-fe3062d7a05d"
      },
      "id": "_FwF1OpXxYCB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "np.mean on sparse (forces dense!):\n",
            "np.mean(sparse): 0.200s | RSS Δ +0.0 MB (now 12744.8 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Anti-pattern: NumPy APIs on sparse\n",
        "\n",
        "`np.mean(X)` on a sparse slice ran in 0.200 s and (here) didn’t spike RSS much, but only because you used a small slice. Under the hood, this forces a dense array. On a real slice with 25k × 18k cells, you’d see multi-GB blowups.\n",
        "\n",
        "Rule: if you see plain NumPy functions on a sparse matrix, assume it’s dangerous unless explicitly sparse-aware. Or at least test it using the above tools\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xi03sN-J0iTW"
      },
      "id": "xi03sN-J0iTW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. CPU Micro-Optimizations\n",
        "\n",
        "Vectorization gets us 10×–100× wins. Micro-optimizations give the last 2×–3× by reducing overhead, improving cache locality, and avoiding waste. This section covers:\n",
        "\n",
        "* Dtype discipline: `float32` / `int32` only.\n",
        "\n",
        "* Row-scaling in place: avoid `D @ X`.\n",
        "\n",
        "* Block processing: keep data in cache.\n",
        "\n",
        "* Numba kernels: for custom transforms on `.data`.\n",
        "\n",
        "### 5.1 Dtype inflation doubles memory\n",
        "\n",
        "Show how `float64` silently doubles payload vs `float32`."
      ],
      "metadata": {
        "id": "pHEBpJlxyiJe"
      },
      "id": "pHEBpJlxyiJe"
    },
    {
      "cell_type": "code",
      "source": [
        "def mem_bytes_csr(X):\n",
        "    return X.data.nbytes + X.indices.nbytes + X.indptr.nbytes\n",
        "\n",
        "X32 = adata.X.astype(np.float32, copy=True)\n",
        "X64 = adata.X.astype(np.float64, copy=True)\n",
        "\n",
        "print(\"float32 bytes:\", mem_bytes_csr(X32))\n",
        "print(\"float64 bytes:\", mem_bytes_csr(X64))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tzXI7RvxZw2",
        "outputId": "8cde61bd-2334-42f8-c71b-86b755e37108"
      },
      "id": "6tzXI7RvxZw2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float32 bytes: 7487629976\n",
            "float64 bytes: 11230866300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dtype discipline\n",
        "\n",
        "* CSR `float32`: ~7.49 GB\n",
        "\n",
        "* CSR `float64`: ~11.23 GB\n",
        "\n",
        "That’s 50% more memory for zero extra information. Same for `int64` indices. Always store values as `float32` and indices as `int32`.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mUqOX-fe0v9e"
      },
      "id": "mUqOX-fe0v9e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Row-normalization: diag multiply vs in-place row scale\n",
        "\n",
        "Compare the “safe but heavy” diagonal approach from Part 3 to a lean in-place row scaling kernel that touches each row’s data segment once."
      ],
      "metadata": {
        "id": "OD8ewja8y4Y8"
      },
      "id": "OD8ewja8y4Y8"
    },
    {
      "cell_type": "code",
      "source": [
        "def row_normalize_diag(X):\n",
        "    counts = X.sum(axis=1).A1\n",
        "    scale = np.divide(1.0, counts, out=np.zeros_like(counts, dtype=np.float32), where=counts>0)\n",
        "    D = sp.diags(scale, offsets=0, dtype=np.float32, format=\"csr\")\n",
        "    return D @ X\n",
        "\n",
        "def row_normalize_inplace(X):\n",
        "    X = X.copy()\n",
        "    counts = X.sum(axis=1).A1\n",
        "    scale = np.divide(1.0, counts, out=np.zeros_like(counts, dtype=np.float32), where=counts>0)\n",
        "    for i in range(X.shape[0]):\n",
        "        start, end = X.indptr[i], X.indptr[i+1]\n",
        "        if scale[i] != 0:\n",
        "            X.data[start:end] *= scale[i]\n",
        "    return X\n",
        "\n",
        "subX = adata.X[:80_000,:]\n",
        "print(\"row_normalize_diag:\", bench(row_normalize_diag, subX))\n",
        "print(\"row_normalize_inplace:\", bench(row_normalize_inplace, subX))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUFZzFKQxbMF",
        "outputId": "4ed0c639-9006-45a0-88ed-96fc0ac5c5c9"
      },
      "id": "gUFZzFKQxbMF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "row_normalize_diag: {'median_s': 2.2259483500092756, 'mean_s': 2.225888760003727, 'min_s': 2.2219033279980067, 'max_s': 2.2298146020038985, 'n': 3}\n",
            "row_normalize_inplace: {'median_s': 1.2840500500169583, 'mean_s': 1.286525678326143, 'min_s': 1.28124573698733, 'max_s': 1.2942812479741406, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Advanced: 5.3 Numba for custom elementwise transforms\n",
        "\n",
        "Numba is a just in time (JIT) compiler for python focused on numerical arrays. You decorate a Python function with `@numba.njit`, and Numba compiles it to fast machine code the first time you call it.\n",
        "\n",
        "Caveats:\n",
        "\n",
        "* Compilation has overhead on the first call; use warmup runs before timing.\n",
        "\n",
        "* Works best on numeric arrays with simple loops; struggles with Python objects.\n",
        "\n",
        "* Debugging JIT’d code is trickier (can’t step through like normal Python)."
      ],
      "metadata": {
        "id": "4VVQotq6y87T"
      },
      "id": "4VVQotq6y87T"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import sparse as sp\n",
        "import time, statistics as stats\n",
        "import numba as nb\n",
        "\n",
        "def bench(fn, *args, repeats=3, warmup=1, **kwargs):\n",
        "    for _ in range(warmup): fn(*args, **kwargs)\n",
        "    ts = []\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        fn(*args, **kwargs)\n",
        "        ts.append(time.perf_counter() - t0)\n",
        "    return dict(median_s=stats.median(ts), mean_s=float(np.mean(ts)),\n",
        "                min_s=min(ts), max_s=max(ts), n=repeats)\n",
        "\n",
        "# Piecewise parameters\n",
        "t1 = 1.0\n",
        "t2 = 5.0\n",
        "alpha = 0.5\n",
        "\n",
        "# --- 1) Python loop baseline\n",
        "def piecewise_python(X, t1=t1, t2=t2, alpha=alpha):\n",
        "    Y = X.copy()\n",
        "    d = Y.data\n",
        "    for i in range(d.size):\n",
        "        x = d[i]\n",
        "        if x < t1:\n",
        "            d[i] = alpha * x\n",
        "        elif x < t2:\n",
        "            d[i] = np.log1p(x)\n",
        "        else:\n",
        "            d[i] = np.sqrt(x)\n",
        "    return Y\n",
        "\n",
        "# --- 2) NumPy masking (vectorized with multiple passes + masks)\n",
        "def piecewise_numpy_masks(X, t1=t1, t2=t2, alpha=alpha):\n",
        "    Y = X.copy()\n",
        "    d = Y.data\n",
        "    m1 = d < t1\n",
        "    m2 = (~m1) & (d < t2)\n",
        "    m3 = ~(m1 | m2)\n",
        "    d[m1] = alpha * d[m1]\n",
        "    d[m2] = np.log1p(d[m2])\n",
        "    d[m3] = np.sqrt(d[m3])\n",
        "    return Y\n",
        "\n",
        "# Optional: np.select variant (also allocates full-size temporaries)\n",
        "def piecewise_numpy_select(X, t1=t1, t2=t2, alpha=alpha):\n",
        "    Y = X.copy()\n",
        "    d = Y.data\n",
        "    Y.data = np.select(\n",
        "        [d < t1, d < t2],\n",
        "        [alpha * d, np.log1p(d)],\n",
        "        default=np.sqrt(d)\n",
        "    ).astype(d.dtype, copy=False)\n",
        "    return Y\n",
        "\n",
        "# --- 3) Numba single-pass, branchy kernel (no masks)\n",
        "@nb.njit(cache=True, fastmath=True)\n",
        "def _piecewise_kernel(d, t1, t2, alpha):\n",
        "    for i in range(d.size):\n",
        "        x = d[i]\n",
        "        if x < t1:\n",
        "            d[i] = alpha * x\n",
        "        elif x < t2:\n",
        "            d[i] = np.log1p(x)\n",
        "        else:\n",
        "            d[i] = np.sqrt(x)\n",
        "\n",
        "def piecewise_numba(X, t1=t1, t2=t2, alpha=alpha):\n",
        "    Y = X.copy()\n",
        "    _piecewise_kernel(Y.data, t1, t2, alpha)\n",
        "    return Y\n",
        "\n",
        "# ---- Benchmark on a realistic slice\n",
        "subX = adata.X[:100, :]  # increase/decrease as your machine allows\n",
        "\n",
        "print(\"Warm up Numba…\")\n",
        "_ = piecewise_numba(subX)\n",
        "\n",
        "print(\"python loop:\", bench(piecewise_python, subX))\n",
        "print(\"numpy masks:\", bench(piecewise_numpy_masks, subX))\n",
        "print(\"numpy select:\", bench(piecewise_numpy_select, subX))\n",
        "print(\"numba single-pass:\", bench(piecewise_numba, subX))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuyRRk--xc-q",
        "outputId": "802ae34c-6b0e-4e67-82c3-e7a5fcc26f2e"
      },
      "id": "MuyRRk--xc-q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warm up Numba…\n",
            "python loop: {'median_s': 1.30168483700254, 'mean_s': 1.2997610680079863, 'min_s': 1.293608893000055, 'max_s': 1.303989474021364, 'n': 3}\n",
            "numpy masks: {'median_s': 0.004838541004573926, 'mean_s': 0.004852356670501952, 'min_s': 0.004779270006110892, 'max_s': 0.004939259000821039, 'n': 3}\n",
            "numpy select: {'median_s': 0.002205534983659163, 'mean_s': 0.002208151990392556, 'min_s': 0.0021943429892417043, 'max_s': 0.0022245779982768, 'n': 3}\n",
            "numba single-pass: {'median_s': 0.004716848983662203, 'mean_s': 0.004716882326950629, 'min_s': 0.00470307600335218, 'max_s': 0.004730721993837506, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Block/tile demo (optional)\n",
        "\n",
        "Operating on all genes at once can overflow caches. Here we tile the columns into smaller blocks to keep working sets in L2/L3."
      ],
      "metadata": {
        "id": "_kWEff9mzAMH"
      },
      "id": "_kWEff9mzAMH"
    },
    {
      "cell_type": "code",
      "source": [
        "def hvg_blocked(X, block_size=2000):\n",
        "    n = X.shape[1]\n",
        "    vars = []\n",
        "    for start in range(0, n, block_size):\n",
        "        end = min(start+block_size, n)\n",
        "        A = X[:, start:end].toarray().astype(np.float32, copy=False)\n",
        "        vars.append(A.var(axis=0))\n",
        "    return np.concatenate(vars)\n",
        "\n",
        "subX = adata.X[:40_000,:]\n",
        "print(\"hvg_blocked:\", bench(hvg_blocked, subX))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROHS9hxdxffx",
        "outputId": "1fe6e445-9e78-4d9d-dac4-e2ff9cea3dcf"
      },
      "id": "ROHS9hxdxffx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hvg_blocked: {'median_s': 8.301487449003616, 'mean_s': 8.295305196671203, 'min_s': 8.278520805004518, 'max_s': 8.305907336005475, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Block/tile processing\n",
        "\n",
        "Blocked HVG proxy: 8.30 s for 40k cells.\n",
        "\n",
        "This method keeps working sets cache-friendly by slicing columns into blocks. It’s slower than lightweight row ops, but essential for expensive column-wise statistics (variance, correlations) when matrices get very wide."
      ],
      "metadata": {
        "id": "w5pcqlmS2DVP"
      },
      "id": "w5pcqlmS2DVP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. When and How to Use the GPU\n",
        "\n",
        "GPU acceleration can feel like a silver bullet, but it only wins under the right conditions. The key is to know when transfer costs dominate vs when workloads are big enough to amortize them.\n",
        "\n",
        "Heuristics\n",
        "\n",
        "* Yes: large matrix operations (PCA, KNN graph, UMAP, batched elementwise ops).\n",
        "\n",
        "* No: tiny slices, heavy Python control flow, or memory-bound row sums on small datasets.\n",
        "\n",
        "* Critical rule: once you move data to GPU, keep it there for as much of the pipeline as possible. Avoid CPU↔GPU ping-pong.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 6.1 CuPy basics: a drop-in NumPy\n",
        "\n",
        "We’ll start with a dense demo. CuPy arrays behave like NumPy arrays, but live on the GPU. Expect timings to look worse on tiny arrays (transfer cost dominates), but better on big ones."
      ],
      "metadata": {
        "id": "YqWiucUw4HCU"
      },
      "id": "YqWiucUw4HCU"
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import numpy as np, time\n",
        "\n",
        "def bench(fn, *args, repeats=3, warmup=1, **kwargs):\n",
        "    for _ in range(warmup): fn(*args, **kwargs)\n",
        "    ts = []\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        fn(*args, **kwargs)\n",
        "        ts.append(time.perf_counter() - t0)\n",
        "    return dict(median_s=np.median(ts), mean_s=float(np.mean(ts)),\n",
        "                min_s=min(ts), max_s=max(ts), n=repeats)\n",
        "\n",
        "n = 5_000_000\n",
        "x_cpu = np.random.rand(n).astype(np.float32)\n",
        "x_gpu = cp.asarray(x_cpu)\n",
        "\n",
        "print(\"CPU sum:\", bench(lambda: x_cpu.sum()))\n",
        "print(\"GPU sum (resident):\", bench(lambda: x_gpu.sum()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt28mfSE2Kae",
        "outputId": "0ee3f612-91e2-4500-fad6-6217525911b2"
      },
      "id": "Kt28mfSE2Kae",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU sum: {'median_s': 0.0019603009859565645, 'mean_s': 0.001969521321977178, 'min_s': 0.0019528029952198267, 'max_s': 0.0019954599847551435, 'n': 3}\n",
            "GPU sum (resident): {'median_s': 0.0003212369920220226, 'mean_s': 0.0003279416705481708, 'min_s': 0.000268882024101913, 'max_s': 0.00039370599552057683, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dense “hello world”: summing 5M floats\n",
        "\n",
        "* CPU sum: 0.00196 s\n",
        "\n",
        "* GPU sum (already on device): 0.000321 s\n",
        "\n",
        "~6× faster on GPU when the array is already resident.\n",
        "\n",
        "Moral: once data is on the GPU, even simple reductions get a consistent speedup. But the transfer has to be amortized\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QHJDY1177bDT"
      },
      "id": "QHJDY1177bDT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Sparse matrices with CuPy\n",
        "\n",
        "CuPy has `cupyx.scipy.sparse` which mirrors SciPy’s sparse formats. Let’s move our AnnData matrix to GPU as CSR and benchmark a row sum."
      ],
      "metadata": {
        "id": "PlHRbnEz5rLT"
      },
      "id": "PlHRbnEz5rLT"
    },
    {
      "cell_type": "code",
      "source": [
        "from cupyx.scipy import sparse as cpx_sp\n",
        "\n",
        "def to_cupy_csr(X):\n",
        "    from scipy import sparse as sp\n",
        "    if sp.isspmatrix_csr(X):\n",
        "        return cpx_sp.csr_matrix((cp.asarray(X.data),\n",
        "                                  cp.asarray(X.indices),\n",
        "                                  cp.asarray(X.indptr)),\n",
        "                                 shape=X.shape)\n",
        "    return cpx_sp.csr_matrix(X.tocsr())\n",
        "\n",
        "Xg = to_cupy_csr(adata.X)\n",
        "\n",
        "def gpu_row_sums(Xg):\n",
        "    return Xg.sum(axis=1).get().ravel()\n",
        "\n",
        "subX = adata.X[:40_000,:]\n",
        "subXg = to_cupy_csr(subX)\n",
        "\n",
        "print(\"CPU row sums:\", bench(lambda: subX.sum(axis=1).A1))\n",
        "print(\"GPU row sums:\", bench(lambda: gpu_row_sums(subXg)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djuJIP_d5iV1",
        "outputId": "dfbd46b7-6a12-4bc8-a770-dd4ad2830c1b"
      },
      "id": "djuJIP_d5iV1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU row sums: {'median_s': 0.05175546099781059, 'mean_s': 0.051829414325766265, 'min_s': 0.05166531799477525, 'max_s': 0.05206746398471296, 'n': 3}\n",
            "GPU row sums: {'median_s': 0.004674978990806267, 'mean_s': 0.004725466996508961, 'min_s': 0.004659167985664681, 'max_s': 0.0048422540130559355, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sparse CSR row sums (40k × 18k slice)\n",
        "\n",
        "* CPU row sums: 0.0518 s\n",
        "\n",
        "* GPU row sums (resident): 0.00467 s\n",
        "\n",
        "* ~11× faster on GPU for the same operation.\n",
        "\n",
        "Transfer cost (host → device)\n",
        "\n",
        "* 40k-row CSR to GPU: ~0.248 s\n",
        "\n",
        "Put together:\n",
        "\n",
        "* If you transfer once and run many ops (PCA → KNN → UMAP → clustering), the ~0.25 s “toll” fades into the noise.\n",
        "\n",
        "* If you bounce back and forth, you’ll erase the speedup. Keep the pipeline GPU-resident after the first hop.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 6.3 RAPIDS cuDF: GPU DataFrames\n",
        "\n",
        "For obs/var DataFrame operations (groupbys, joins), cuDF provides a GPU-accelerated replacement for pandas. Let’s demo a simple groupby."
      ],
      "metadata": {
        "id": "8h3IeEms54IE"
      },
      "id": "8h3IeEms54IE"
    },
    {
      "cell_type": "code",
      "source": [
        "import cudf\n",
        "\n",
        "counts = adata.X.sum(axis=1).A1.astype(np.float32)\n",
        "gdf = cudf.DataFrame({\"counts\": cp.asarray(counts)})\n",
        "gdf[\"bin\"] = (gdf[\"counts\"] > 1000).astype(\"int8\")\n",
        "\n",
        "print(\"GPU groupby mean:\")\n",
        "print(gdf.groupby(\"bin\").counts.mean().to_pandas())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmzfbe_n520i",
        "outputId": "f2a07b15-0205-42e8-ec26-26679057f60a"
      },
      "id": "pmzfbe_n520i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU groupby mean:\n",
            "bin\n",
            "0     733.258743\n",
            "1    9076.485380\n",
            "Name: counts, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Hybrid Pipelines\n",
        "\n",
        "The sweet spot is often split workflows: use the CPU for cheap preprocessing, then move to GPU for expensive linear algebra.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 7.1 CPU preprocessing (QC)\n",
        "\n",
        "We’ll run QC metrics on CPU, as before."
      ],
      "metadata": {
        "id": "0WjIsCqA6DfO"
      },
      "id": "0WjIsCqA6DfO"
    },
    {
      "cell_type": "code",
      "source": [
        "import scanpy as sc\n",
        "\n",
        "def qc_metrics(adata_):\n",
        "    sc.pp.calculate_qc_metrics(adata_, percent_top=None, log1p=False, inplace=True)\n",
        "\n",
        "qc_metrics(adata[:40_000,:].copy())  # CPU slice for demo\n"
      ],
      "metadata": {
        "id": "3MEghcKK5_FP"
      },
      "id": "3MEghcKK5_FP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Transfer once to GPU\n",
        "\n",
        "Now move the processed matrix to GPU. Measure the transfer cost separately so you can see when it’s worth it."
      ],
      "metadata": {
        "id": "QSGJXW1H6I5V"
      },
      "id": "QSGJXW1H6I5V"
    },
    {
      "cell_type": "code",
      "source": [
        "subX = adata.X[:40_000,:]\n",
        "\n",
        "def transfer_to_gpu(X):\n",
        "    return to_cupy_csr(X)\n",
        "\n",
        "print(\"Transfer 40k slice to GPU:\")\n",
        "print(bench(transfer_to_gpu, subX))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESI6Y5N36Gw_",
        "outputId": "40b6211e-5b8c-4cdb-a1e8-7e0806880d23"
      },
      "id": "ESI6Y5N36Gw_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transfer 40k slice to GPU:\n",
            "{'median_s': 0.24823900000774302, 'mean_s': 0.24861315032467246, 'min_s': 0.24495537197799422, 'max_s': 0.25264507898828015, 'n': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 GPU linear algebra (PCA → neighbors → clustering)\n",
        "\n",
        "RAPIDS cuml provides GPU PCA and KNN. The pattern is: keep everything on GPU until you have small results to bring back."
      ],
      "metadata": {
        "id": "rMXFqoNK6NJe"
      },
      "id": "rMXFqoNK6NJe"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scanpy as sc\n",
        "import cupy as cp\n",
        "from cuml.decomposition import PCA as cuPCA\n",
        "from cuml.neighbors import NearestNeighbors\n",
        "\n",
        "# 1) HVGs on CPU\n",
        "adata_tmp = adata[:40_000, :].copy()          # demo subset; scale up as you like\n",
        "sc.pp.highly_variable_genes(adata_tmp, n_top_genes=2000, flavor=\"seurat_v3\", inplace=True)\n",
        "hvg_mask = adata_tmp.var[\"highly_variable\"].values\n",
        "assert hvg_mask.sum() > 0\n",
        "\n",
        "# 2) Slice to HVGs (still CSR on CPU)\n",
        "X_hvg = adata_tmp.X[:, hvg_mask]\n",
        "\n",
        "# 3) Move once to GPU as DENSE (width=~2k → safe)\n",
        "Xg = cp.asarray(X_hvg.toarray(), dtype=cp.float32, order=\"C\")  # deliberate densify after HVG\n",
        "\n",
        "# 4) PCA on GPU\n",
        "pca = cuPCA(n_components=50, svd_solver=\"jacobi\", output_type=\"cupy\", random_state=0)\n",
        "Yg = pca.fit_transform(Xg)                           # (n_cells, 50) on GPU\n",
        "\n",
        "# 5) Neighbors on GPU\n",
        "knn = NearestNeighbors(n_neighbors=15, algorithm=\"brute\")\n",
        "knn.fit(Yg)\n",
        "distances, indices = knn.kneighbors(Yg)\n",
        "\n",
        "print(\"GPU pipeline OK:\", Xg.shape, Yg.shape, indices.shape, type(Yg), type(indices))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn9vbxDC6LaP",
        "outputId": "81ad485f-98c9-48e6-fc8b-5f96780f3e75"
      },
      "id": "xn9vbxDC6LaP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracting highly variable genes\n",
            "--> added\n",
            "    'highly_variable', boolean vector (adata.var)\n",
            "    'highly_variable_rank', float vector (adata.var)\n",
            "    'means', float vector (adata.var)\n",
            "    'variances', float vector (adata.var)\n",
            "    'variances_norm', float vector (adata.var)\n",
            "GPU pipeline OK: (40000, 2000) (40000, 50) (40000, 15) <class 'cupy.ndarray'> <class 'cupy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 RAPIDS Single-Cell\n",
        "\n",
        "What is RAPIDS Single-Cell (rsc)?\n",
        "\n",
        "RSC is a GPU-accelerated, Scanpy-compatible toolkit. It aims to be a near drop-in replacement: you keep using `AnnData`, but call `rsc.pp.*` and `rsc.tl.*` for GPU versions of familiar steps (normalize, log1p, HVGs, PCA/TSVD, neighbors, UMAP, clustering). Under the hood it leans on CuPy and RAPIDS cuML/cuGraph. In many workflows you’ll see order-of-magnitude speedups while keeping the Scanpy mental model.\n",
        "\n",
        "Tips:\n",
        "\n",
        "It works with a standard `AnnData`. For best performance, keep data `float32`, and try to keep matrices GPU-resident once you move them there to avoid PCIe ping-pong.\n",
        "\n",
        "You can start with CPU-backed AnnData; rsc will move data as needed. If you want full control, you can put a CuPy dense or CuPy-sparse array into `adata.X` before calling rsc"
      ],
      "metadata": {
        "id": "eAE2uAW-86Mn"
      },
      "id": "eAE2uAW-86Mn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll run the standard preprocessing and analysis steps on GPU via rsc: total-count normalization → log1p → HVGs (2k) → PCA (50 PCs) → neighbors (k=15) → UMAP. This mirrors the manual pipeline you just ran (HVG → GPU PCA → KNN), but with fewer lines and tight Scanpy compatibility."
      ],
      "metadata": {
        "id": "gHN1Eygq9cdn"
      },
      "id": "gHN1Eygq9cdn"
    },
    {
      "cell_type": "code",
      "source": [
        "import time, statistics as stats\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import rapids_singlecell as rsc\n",
        "import scanpy as sc\n",
        "from cupyx.scipy import sparse as cpx_sp\n",
        "from scipy import sparse as sp\n",
        "\n",
        "def gpu_sync():\n",
        "    # ensure all queued GPU work finishes before we stop timers\n",
        "    try:\n",
        "        cp.cuda.Stream.null.synchronize()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def bench_once(fn, *args, **kwargs):\n",
        "    t0 = time.perf_counter()\n",
        "    out = fn(*args, **kwargs)\n",
        "    gpu_sync()\n",
        "    return out, (time.perf_counter() - t0)\n",
        "\n",
        "def bench_step(label, fn, *args, **kwargs):\n",
        "    # one warmup + 3 measured runs (use median)\n",
        "    _, _ = bench_once(fn, *args, **kwargs)\n",
        "    ts = []\n",
        "    for _ in range(3):\n",
        "        _, dt = bench_once(fn, *args, **kwargs)\n",
        "        ts.append(dt)\n",
        "    return float(np.median(ts))\n",
        "\n",
        "def to_gpu_csr(X):\n",
        "    X = X.tocsr().astype('float32', copy=False)\n",
        "    return cpx_sp.csr_matrix(\n",
        "        (cp.asarray(X.data), cp.asarray(X.indices), cp.asarray(X.indptr)),\n",
        "        shape=X.shape)\n",
        "\n",
        "def gpu_rsc_pipeline(adata_src, n_top_genes=2000, n_pcs=50, k=15):\n",
        "    A = adata_src.copy()\n",
        "    A.X = to_gpu_csr(A.X)  # ensure GPU CSR\n",
        "\n",
        "    times = {}\n",
        "    times['normalize_total'] = bench_step(\"norm\", rsc.pp.normalize_total, A, target_sum=1e4)\n",
        "    times['log1p']           = bench_step(\"log1p\", rsc.pp.log1p, A)\n",
        "    times['hvg']             = bench_step(\"hvg\", rsc.pp.highly_variable_genes, A, n_top_genes=n_top_genes)\n",
        "    times['pca']             = bench_step(\"pca\", rsc.pp.pca, A, n_comps=n_pcs)\n",
        "    times['neighbors']       = bench_step(\"neighbors\", rsc.pp.neighbors, A, n_neighbors=k)\n",
        "    times['umap']            = bench_step(\"umap\", rsc.tl.umap, A)\n",
        "\n",
        "    total = sum(times.values())\n",
        "    return A, times, total\n",
        "\n",
        "def cpu_scanpy_pipeline(adata_src, n_top_genes=2000, n_pcs=50, k=15):\n",
        "    A = adata_src.copy()\n",
        "\n",
        "    times = {}\n",
        "    # normalize_total in Scanpy 1.11 still supports target_sum; mutates in place\n",
        "    times['normalize_total'] = bench_step(\"norm\", sc.pp.normalize_total, A, target_sum=1e4)\n",
        "\n",
        "    # log1p in Scanpy 1.11 mutates in place by default; DO NOT pass inplace\n",
        "    times['log1p']           = bench_step(\"log1p\", sc.pp.log1p, A)\n",
        "\n",
        "    # HVGs (CPU), inplace by default in this version\n",
        "    times['hvg']             = bench_step(\n",
        "        \"hvg\", sc.pp.highly_variable_genes, A,\n",
        "        n_top_genes=n_top_genes, flavor=\"seurat_v3\"\n",
        "    )\n",
        "\n",
        "    # subset to HVGs before PCA to keep the dense step bounded\n",
        "    hvg_mask = A.var['highly_variable'].values\n",
        "    A = A[:, hvg_mask].copy()\n",
        "\n",
        "    # PCA (CPU; densifies X[:, HVG])\n",
        "    times['pca']             = bench_step(\"pca\", sc.pp.pca, A, n_comps=n_pcs, svd_solver=\"arpack\")\n",
        "\n",
        "    # neighbors + umap (CPU)\n",
        "    times['neighbors']       = bench_step(\"neighbors\", sc.pp.neighbors, A, n_neighbors=k)\n",
        "    times['umap']            = bench_step(\"umap\", sc.tl.umap, A)\n",
        "\n",
        "    total = sum(times.values())\n",
        "    return A, times, total\n",
        "# use the same 40k slice you benchmarked earlier\n",
        "base = adata[:40_000, :]\n",
        "\n",
        "gpu_A, gpu_times, gpu_total = gpu_rsc_pipeline(base, n_top_genes=2000, n_pcs=50, k=15)\n",
        "cpu_A, cpu_times, cpu_total = cpu_scanpy_pipeline(base, n_top_genes=2000, n_pcs=50, k=15)\n",
        "\n",
        "def pretty(d):\n",
        "    return \", \".join(f\"{k}={v:.3f}s\" for k,v in d.items())\n",
        "\n",
        "print(\"\\nGPU (rsc) times:\")\n",
        "print(pretty(gpu_times), f\"| TOTAL={gpu_total:.3f}s\")\n",
        "print(\"GPU shapes:\", gpu_A.obsm['X_pca'].shape, gpu_A.obsm['X_umap'].shape)\n",
        "\n",
        "print(\"\\nCPU (scanpy) times:\")\n",
        "print(pretty(cpu_times), f\"| TOTAL={cpu_total:.3f}s\")\n",
        "print(\"CPU shapes:\", cpu_A.obsm['X_pca'].shape, cpu_A.obsm['X_umap'].shape)\n",
        "\n",
        "print(\"\\nSpeedups (CPU/GPU):\")\n",
        "for k in gpu_times:\n",
        "    if k in cpu_times:\n",
        "        spd = cpu_times[k] / max(1e-9, gpu_times[k])\n",
        "        print(f\"{k:>12}: ×{spd:.1f}\")\n",
        "print(f\"{'TOTAL':>12}: ×{cpu_total/max(1e-9,gpu_total):.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wMLbotpd6Qnl",
        "outputId": "5ee0b367-51ef-44e5-cb87-99c540489188"
      },
      "id": "wMLbotpd6Qnl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalizing counts per cell\n",
            "    finished (0:00:00)\n",
            "normalizing counts per cell\n",
            "    finished (0:00:00)\n",
            "normalizing counts per cell\n",
            "    finished (0:00:00)\n",
            "normalizing counts per cell\n",
            "    finished (0:00:00)\n",
            "WARNING: adata.X seems to be already log-transformed.\n",
            "WARNING: adata.X seems to be already log-transformed.\n",
            "WARNING: adata.X seems to be already log-transformed.\n",
            "extracting highly variable genes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/legacy_api_wrap/__init__.py:82: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.\n",
            "  return fn(*args_all, **kw)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> added\n",
            "    'highly_variable', boolean vector (adata.var)\n",
            "    'highly_variable_rank', float vector (adata.var)\n",
            "    'means', float vector (adata.var)\n",
            "    'variances', float vector (adata.var)\n",
            "    'variances_norm', float vector (adata.var)\n",
            "extracting highly variable genes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/legacy_api_wrap/__init__.py:82: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.\n",
            "  return fn(*args_all, **kw)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> added\n",
            "    'highly_variable', boolean vector (adata.var)\n",
            "    'highly_variable_rank', float vector (adata.var)\n",
            "    'means', float vector (adata.var)\n",
            "    'variances', float vector (adata.var)\n",
            "    'variances_norm', float vector (adata.var)\n",
            "extracting highly variable genes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/legacy_api_wrap/__init__.py:82: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.\n",
            "  return fn(*args_all, **kw)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> added\n",
            "    'highly_variable', boolean vector (adata.var)\n",
            "    'highly_variable_rank', float vector (adata.var)\n",
            "    'means', float vector (adata.var)\n",
            "    'variances', float vector (adata.var)\n",
            "    'variances_norm', float vector (adata.var)\n",
            "extracting highly variable genes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/legacy_api_wrap/__init__.py:82: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.\n",
            "  return fn(*args_all, **kw)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> added\n",
            "    'highly_variable', boolean vector (adata.var)\n",
            "    'highly_variable_rank', float vector (adata.var)\n",
            "    'means', float vector (adata.var)\n",
            "    'variances', float vector (adata.var)\n",
            "    'variances_norm', float vector (adata.var)\n",
            "computing PCA\n",
            "    with n_comps=50\n",
            "    finished (0:00:07)\n",
            "computing PCA\n",
            "    with n_comps=50\n",
            "    finished (0:00:07)\n",
            "computing PCA\n",
            "    with n_comps=50\n",
            "    finished (0:00:07)\n",
            "computing PCA\n",
            "    with n_comps=50\n",
            "    finished (0:00:07)\n",
            "computing neighbors\n",
            "    using 'X_pca' with n_pcs = 50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    finished: added to `.uns['neighbors']`\n",
            "    `.obsp['distances']`, distances for each pair of neighbors\n",
            "    `.obsp['connectivities']`, weighted adjacency matrix (0:01:08)\n",
            "computing neighbors\n",
            "    using 'X_pca' with n_pcs = 50\n",
            "    finished: added to `.uns['neighbors']`\n",
            "    `.obsp['distances']`, distances for each pair of neighbors\n",
            "    `.obsp['connectivities']`, weighted adjacency matrix (0:00:06)\n",
            "computing neighbors\n",
            "    using 'X_pca' with n_pcs = 50\n",
            "    finished: added to `.uns['neighbors']`\n",
            "    `.obsp['distances']`, distances for each pair of neighbors\n",
            "    `.obsp['connectivities']`, weighted adjacency matrix (0:00:06)\n",
            "computing neighbors\n",
            "    using 'X_pca' with n_pcs = 50\n",
            "    finished: added to `.uns['neighbors']`\n",
            "    `.obsp['distances']`, distances for each pair of neighbors\n",
            "    `.obsp['connectivities']`, weighted adjacency matrix (0:00:06)\n",
            "computing UMAP\n",
            "    finished: added\n",
            "    'X_umap', UMAP coordinates (adata.obsm)\n",
            "    'umap', UMAP parameters (adata.uns) (0:00:31)\n",
            "computing UMAP\n",
            "    finished: added\n",
            "    'X_umap', UMAP coordinates (adata.obsm)\n",
            "    'umap', UMAP parameters (adata.uns) (0:00:29)\n",
            "computing UMAP\n",
            "    finished: added\n",
            "    'X_umap', UMAP coordinates (adata.obsm)\n",
            "    'umap', UMAP parameters (adata.uns) (0:00:29)\n",
            "computing UMAP\n",
            "    finished: added\n",
            "    'X_umap', UMAP coordinates (adata.obsm)\n",
            "    'umap', UMAP parameters (adata.uns) (0:00:29)\n",
            "\n",
            "GPU (rsc) times:\n",
            "normalize_total=0.016s, log1p=0.026s, hvg=0.049s, pca=0.183s, neighbors=0.118s, umap=0.107s | TOTAL=0.499s\n",
            "GPU shapes: (40000, 50) (40000, 2)\n",
            "\n",
            "CPU (scanpy) times:\n",
            "normalize_total=0.337s, log1p=0.174s, hvg=2.033s, pca=7.307s, neighbors=6.432s, umap=29.808s | TOTAL=46.090s\n",
            "CPU shapes: (40000, 50) (40000, 2)\n",
            "\n",
            "Speedups (CPU/GPU):\n",
            "normalize_total: ×21.1\n",
            "       log1p: ×6.6\n",
            "         hvg: ×41.2\n",
            "         pca: ×39.9\n",
            "   neighbors: ×54.6\n",
            "        umap: ×279.0\n",
            "       TOTAL: ×92.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Emerging Tools\n",
        "\n",
        "Single-cell analysis has historically revolved around Scanpy (Python/CPU) and Seurat (R/CPU). Over the last 2–3 years, the performance landscape has expanded dramatically. Here’s a guided sweep of the tools you should be aware of, even if you don’t use them immediately.\n",
        "\n",
        "### 8.1 GPU-native runtimes\n",
        "\n",
        "* RAPIDS Single-Cell (rapids_singlecell)\n",
        "\n",
        "  * Already demoed. Scanpy-like API, accelerated PCA/UMAP/KNN.\n",
        "\n",
        "  * Good entry point if you’re comfortable in Python and want fast wins on 100k–1M cell data.\n",
        "\n",
        "* PyTorch Sparse / JAX Sparse\n",
        "\n",
        "  * Experimental support for sparse tensors in deep-learning frameworks.\n",
        "\n",
        "  * Useful if you’re moving from QC into deep generative models (scVI, VAEs, diffusion models).\n",
        "\n",
        "  * These frameworks can directly interop with GPUs, TPUs, or clusters.\n",
        "\n",
        "* scvi-tools / PyTorch Geometric\n",
        "\n",
        "  * scvi-tools brings state-of-the-art generative models to single-cell (e.g. batch correction, multimodal integration).\n",
        "\n",
        "  * PyTorch Geometric supports graph neural networks — potentially useful for cell-cell interaction models.\n",
        "\n",
        "### 8.2 Scalable array & storage backends\n",
        "\n",
        "* Zarr / TileDB\n",
        "\n",
        "  * Both store n-dimensional arrays on disk in chunked formats.\n",
        "\n",
        "  * Let you stream slices (cells × genes) into memory without ever loading the full dense array.\n",
        "\n",
        "  * Important if you’re analyzing >1M cells where even CSR won’t fit in RAM.\n",
        "\n",
        "  * Zarr integrates with Scanpy; TileDB adds cloud-native storage (S3/GCS/Azure).\n",
        "\n",
        "* Apache Arrow / Polars\n",
        "\n",
        "  * Arrow defines a columnar in-memory format with zero-copy sharing across languages.\n",
        "\n",
        "  * Polars is a DataFrame library built on Arrow: multithreaded, faster than pandas.\n",
        "\n",
        "  * Advanced: replace heavy .obs / .var pandas operations with Polars for speed.\n",
        "\n",
        "### 8.3 CPU accelerators\n",
        "\n",
        "* NumExpr / Bottleneck\n",
        "\n",
        "  * Vectorized kernels for elementwise arithmetic and reductions in dense arrays.\n",
        "\n",
        "  * If you’re stuck with a dense step (e.g. PCA input after HVG selection), these can cut runtime 2–3× with minimal code change.\n",
        "\n",
        "* Numba\n",
        "\n",
        "  * Already shown for custom kernels. Still one of the cleanest ways to JIT-compile a Python loop down to native code.\n",
        "\n",
        "  * Best for bespoke elementwise transforms when you can’t express the logic as pure vectorization.\n",
        "\n",
        "### 8.4 Cluster/distributed execution\n",
        "* For datasets approaching multi-million cells, sometimes even a single GPU workstation isn’t enough. That’s where distributed runtimes shine:\n",
        "\n",
        "* Dask\n",
        "\n",
        "  * Python’s “batteries-included” distributed array/dataframe engine.\n",
        "\n",
        "  * Integrates with RAPIDS: you can distribute CuPy/GPUs across nodes.\n",
        "\n",
        "  * Lets you keep the Scanpy-like workflow, just at cluster scale.\n",
        "\n",
        "* Apache Spark\n",
        "\n",
        "  * More heavyweight, but battle-tested in data engineering.\n",
        "\n",
        "  * Spark DataFrames (with Arrow interop) can store and query single-cell metadata across clusters.\n",
        "\n",
        "  * Most useful when analysis is tied into big data pipelines (ETL, multi-omics integration with other large datasets).\n",
        "\n",
        "* Ray\n",
        "\n",
        "  * General-purpose distributed execution framework.\n",
        "\n",
        "  * Used by some ML workflows for scalable hyperparameter tuning and batch inference on cell embeddings."
      ],
      "metadata": {
        "id": "yttzGEqiAZkV"
      },
      "id": "yttzGEqiAZkV"
    },
    {
      "cell_type": "code",
      "source": [
        "%env LC_ALL=C.UTF-8\n",
        "%env LANG=C.UTF-8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9HApnD3B-8s",
        "outputId": "494a63c7-f301-45e9-a031-bfb8a8817ef5"
      },
      "id": "K9HApnD3B-8s",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LC_ALL=C.UTF-8\n",
            "env: LANG=C.UTF-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask[complete] dask-image tqdm ray[default] --quiet"
      ],
      "metadata": {
        "id": "QdZygtxb8Uhq"
      },
      "id": "QdZygtxb8Uhq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dask lazy parallel compute over chunks (single machine)\n",
        "What this shows\n",
        "\n",
        "We compute per-gene sums/nnz using Dask delayed on CSR column blocks in parallel, then materialize the result with a progress bar. This is the “hello world” for scaling chunked operations without changing your core logic."
      ],
      "metadata": {
        "id": "sLtXBy6zC_3q"
      },
      "id": "sLtXBy6zC_3q"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from dask import delayed, compute\n",
        "from dask.diagnostics import ProgressBar\n",
        "\n",
        "X = adata.X.tocsr().astype(np.float32, copy=False)  # CPU CSR\n",
        "\n",
        "def col_block_stats(X, start, end):\n",
        "    # compute per-gene (column) sum and nnz for a slice of columns\n",
        "    Xc = X[:, start:end]           # still sparse\n",
        "    s = np.asarray(Xc.sum(axis=0)).ravel().astype(np.float32, copy=False)\n",
        "    c = Xc.getnnz(axis=0).astype(np.int32, copy=False)\n",
        "    return s, c\n",
        "\n",
        "# Build a lazy graph of tasks over column blocks\n",
        "block = 2000\n",
        "tasks = []\n",
        "for j0 in range(0, X.shape[1], block):\n",
        "    j1 = min(j0+block, X.shape[1])\n",
        "    tasks.append(delayed(col_block_stats)(X, j0, j1))\n",
        "\n",
        "# Trigger parallel execution\n",
        "with ProgressBar():\n",
        "    parts = compute(*tasks)\n",
        "\n",
        "# Stitch results\n",
        "sums = np.concatenate([p[0] for p in parts])\n",
        "nnzs = np.concatenate([p[1] for p in parts])\n",
        "print(\"Dask finished:\", sums.shape, nnzs.shape, \"sum(sums)=\", float(sums.sum()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBF4wQkjCGFr",
        "outputId": "1f5d514f-2089-4259-af5e-64fd41970bcb"
      },
      "id": "IBF4wQkjCGFr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[########################################] | 100% Completed | 6.11 s\n",
            "Dask finished: (18129,) (18129,) sum(sums)= 2566197760.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why this is useful: it’s the same logic you’d write with a for-loop, but Dask builds a DAG and runs blocks in parallel using your CPU cores."
      ],
      "metadata": {
        "id": "7UH5DBuHDFP8"
      },
      "id": "7UH5DBuHDFP8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dask Array — out-of-core dense blocks (optional if you have Zarr)\n",
        "What this shows\n",
        "\n",
        "If you have a dense array (e.g., after HVG selection) or a Zarr store, Dask Array can make blocked operations trivial."
      ],
      "metadata": {
        "id": "thkEbs-hDGCc"
      },
      "id": "thkEbs-hDGCc"
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.array as da\n",
        "\n",
        "# Example: turn a dense (n_obs x 2000) view into a dask array with 10k-row chunks\n",
        "X_hvg = adata[:, adata.var_names[:2000]].X.toarray().astype(np.float32)  # demo\n",
        "dX = da.from_array(X_hvg, chunks=(10_000, -1))   # chunk rows, keep columns whole\n",
        "\n",
        "# Compute per-row sums and per-column means lazily, then .compute()\n",
        "row_sums = dX.sum(axis=1).compute()\n",
        "col_means = dX.mean(axis=0).compute()\n",
        "print(row_sums.shape, col_means.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTTTFwMkCo1K",
        "outputId": "b7bb1a44-1d40-4d93-89b2-481a700614f1"
      },
      "id": "WTTTFwMkCo1K",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(289331,) (2000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to use: dense or Zarr-backed workflows where you want simple NumPy-like code that scales to memory."
      ],
      "metadata": {
        "id": "Gdq_wQw1DKmp"
      },
      "id": "Gdq_wQw1DKmp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ray —  parallel “map over batches”\n",
        "What this shows\n",
        "\n",
        "Parallelize a function that processes a batch of cells (e.g., compute a few QC metrics) across CPU cores with Ray tasks. Good for wrapping bits of Scanpy/rsc you can’t easily vectorize."
      ],
      "metadata": {
        "id": "KCZ0Z-S4DMwK"
      },
      "id": "KCZ0Z-S4DMwK"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, ray\n",
        "ray.shutdown(); ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
        "\n",
        "def qc_batch(X_csr, rows):\n",
        "    sub = X_csr[rows, :]\n",
        "    counts = np.asarray(sub.sum(axis=1)).ravel().astype(np.float32)\n",
        "    genes  = sub.getnnz(axis=1).astype(np.int32)\n",
        "    return counts, genes\n",
        "\n",
        "@ray.remote\n",
        "def qc_batch_remote(X_csr, rows):\n",
        "    return qc_batch(X_csr, rows)\n",
        "\n",
        "X = adata.X.tocsr().astype(np.float32, copy=False)\n",
        "batch = 10_000\n",
        "row_idx = np.arange(X.shape[0])\n",
        "batches = [row_idx[i:i+batch] for i in range(0, len(row_idx), batch)]\n",
        "\n",
        "futs = [qc_batch_remote.remote(X, b) for b in batches]\n",
        "parts = ray.get(futs)\n",
        "\n",
        "counts = np.concatenate([p[0] for p in parts])\n",
        "genes  = np.concatenate([p[1] for p in parts])\n",
        "print(\"Ray finished:\", counts.shape, genes.shape, \"sum(counts)=\", float(counts.sum()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bla_eA0KC57p",
        "outputId": "395a0e87-2f07-45c4-9ada-7d41f3ee2f4f"
      },
      "id": "Bla_eA0KC57p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-23 17:48:24,451\tINFO worker.py:1781 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ray finished: (289331,) (289331,) sum(counts)= 2566203648.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Ray: dead-simple task API; handy when you want to parallelize Python functions that don’t fit a simple array algebra mold."
      ],
      "metadata": {
        "id": "jax6fXJbB3tF"
      },
      "id": "jax6fXJbB3tF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark (local mode) — DataFrame/SQL over big metadata (advanced)\n",
        "What this shows\n",
        "\n",
        "Use Spark locally to handle large obs metadata joins/groupbys. This is not for sparse matrix arithmetic; it’s for data engineering tasks around your matrix."
      ],
      "metadata": {
        "id": "eMSqA7ksDUI7"
      },
      "id": "eMSqA7ksDUI7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark is heavy and may clash with your pyarrow/RAPIDS\n",
        "# If you try:  pip install -q pyspark==3.5.1 pyarrow==14.0.2  (pin Arrow!)\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"singlecell-demo\").getOrCreate()\n",
        "\n",
        "# Convert .obs to Spark DataFrame (small demo sample)\n",
        "obs_df = adata.obs[['sample', 'n_counts', 'percent_mito']].reset_index(names='cell')\n",
        "sdf = spark.createDataFrame(obs_df)\n",
        "\n",
        "# Group-by example: per-sample summary\n",
        "agg = (sdf.groupBy(\"sample\")\n",
        "         .agg(F.count(\"*\").alias(\"cells\"),\n",
        "              F.avg(\"n_counts\").alias(\"mean_counts\"),\n",
        "              F.avg(\"percent_mito\").alias(\"mean_mito\"))\n",
        "      )\n",
        "agg.show(10, truncate=False)\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "sECjCIt3DQoc"
      },
      "id": "sECjCIt3DQoc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When Spark makes sense: your single-cell pipeline sits inside a larger ETL/warehouse stack; you need SQL joins, window functions, or to integrate with Parquet/Lakehouse at scale."
      ],
      "metadata": {
        "id": "wEE-e5ITDZji"
      },
      "id": "wEE-e5ITDZji"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NVIDIA's multi-GPU 1M single cell notebook\n",
        "\n",
        "https://github.com/NVIDIA-Genomics-Research/rapids-single-cell-examples/blob/master/notebooks/1M_brain_gpu_analysis_multigpu.ipynb"
      ],
      "metadata": {
        "id": "PAU2hDMuNwFm"
      },
      "id": "PAU2hDMuNwFm"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DliVJQAyDcmu"
      },
      "id": "DliVJQAyDcmu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "dataframe_acceleration",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}